{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ed34ffa",
      "metadata": {
        "id": "2ed34ffa"
      },
      "source": [
        "# Optimization!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe06d737",
      "metadata": {
        "id": "fe06d737"
      },
      "outputs": [],
      "source": [
        "!pip install torch tiktoken transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1447ccba",
      "metadata": {
        "id": "1447ccba",
        "outputId": "50b9e374-e1b1-4094-d748-dbe203aefe9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import tiktoken\n",
        "import math\n",
        "import inspect\n",
        "import os\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3269586",
      "metadata": {
        "id": "f3269586"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B=B\n",
        "    self.T=T\n",
        "\n",
        "    # at init, load tokens from disk and store them in memory\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"Loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "    self.current_size = 0 # state\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_size:self.current_size + B*T + 1] # +1 coz we need it in 'y'\n",
        "    # buf = buf.to(device) dont do this here to save space on gpu\n",
        "    x = buf[:-1].view(B, T) # inputs\n",
        "    y = buf[1:].view(B, T) # targets\n",
        "    self.current_size += B*T # advance position in tensor\n",
        "    # if loading next batch would be out of bounds, reset\n",
        "    if self.current_size + B*T + 1 > len(self.tokens):\n",
        "      self.current_size = 0\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c6e49b",
      "metadata": {
        "id": "11c6e49b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # make sure hidden dim is a multiple of no. of heads\n",
        "    assert config.n_embed % config.n_head == 0\n",
        "\n",
        "    # a single linear layer to compute Q, K, V simultaneously\n",
        "    self.c_attn=nn.Linear(config.n_embed, 3 * config.n_embed)\n",
        "\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1 # flag for weight initialization of c_proj, use std = 0.02/sqroot(num layers)\n",
        "\n",
        "    # regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embed = config.n_embed\n",
        "\n",
        "    # not really a bias, more of a mask, but following OpenAI naming convention\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                         .view(1, 1,config.block_size, config.block_size ))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()  # Batch size, sequence length, n_embed\n",
        "    qkv= self.c_attn(x)\n",
        "    q,k,v = qkv.split(self.n_embed, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # explanation : C = n_head * head_size\n",
        "    # k.shape = (B, T, n_head, head_size)\n",
        "    # k = k.transpose(1, 2)\n",
        "    # Before transpose: (B, T, n_head, head_size)\n",
        "    # After transpose:  (B, n_head, T, head_size)\n",
        "\n",
        "    # similar for q and v\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # Attention\n",
        "    # att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    # att = att.masked_fill(self.bias[:, :, :T, :T] == 0.0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, hs), basically a weighted sum of values\n",
        "\n",
        "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.c_proj(y)\n",
        "\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281721e9",
      "metadata": {
        "id": "281721e9"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)         # ffn. increasing hidden dim size increases capacity of model to learn, 4*embed dim is just design choice\n",
        "    self.gelu = nn.GELU(approximate='tanh')                            # activation\n",
        "    self.c_proj = nn.Linear( 4 * config.n_embed, config.n_embed) # projection\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44aa49d4",
      "metadata": {
        "id": "44aa49d4"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embed)  # layer norm 1\n",
        "    self.attn = CausalSelfAttention(config) # causal attention\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embed) # layer norm 2\n",
        "    self.mlp = MLP(config) # fnn\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size : int = 1024    # max sequence length\n",
        "  vocab_size : int = 50257   # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "  n_layer : int = 12\n",
        "  n_head : int = 12\n",
        "  n_embed : int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config=config\n",
        "\n",
        "    self.transformer=nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embed),  # weights for token embeddings\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embed),  # weights for positional embeddings\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # block for each layer\n",
        "        ln_f = nn.LayerNorm(config.n_embed),  # final layer normalisation\n",
        "        ))\n",
        "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size,bias=False) # last second, linear layer\n",
        "\n",
        "    # weight-sharing scheme\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    # initialize parameters\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'): # will be true only for output projection, `c_proj` layer\n",
        "        std *= (2 * self.config.n_layer) ** -0.05 # scale std by 1/sqrt(no_of_layers) acc to GPT paper\n",
        "        # we are doing 2 * no of layers bcoz every layer has 2 blocks that add to residual stream - attention and then mlp\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std = std) # inititalise weights according to gpt2 official code, i.e., mean 0, std 0.02 for weights\n",
        "      # sqroot n thing is done to control the growth of activations in residual stream in forward pass as each residual stream adds its data so we scale down every contribution to residual stream\n",
        "        torch.nn.init.zeros_(module.bias) # and normal initialisation for bias\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx (B, T) Batch size, B sequences, each of length T stacked up, T<=block_size\n",
        "    B, T = idx.size()\n",
        "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "    # forward the token and posisition embeddings\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T), arange iterate from 0 to T\n",
        "    pos_emb = self.transformer.wpe(pos) # shape (T, n_embd) # identical for every single row (batch)\n",
        "    tok_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
        "    x = tok_emb + pos_emb # internal broadcasting\n",
        "    # forward the blocks of transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "    # forward the final layernorm\n",
        "    x = self.transformer.ln_f(x)\n",
        "    # forward the final classifier\n",
        "    logits=self.lm_head(x) # (B, T, vocab_size)\n",
        "    loss=None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # logits - (B*T, vocab_size)\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "      # Loads pretrained GPT-2 model weights from huggingface\n",
        "      assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "      print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "      # n_layer, n_head and n_embed are determined from model_type\n",
        "      config_args = {\n",
        "          'gpt2':         dict(n_layer=12, n_head=12, n_embed=768),  # 124M params\n",
        "          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embed=1024), # 350M params\n",
        "          'gpt2-large':   dict(n_layer=36, n_head=20, n_embed=1280), # 774M params\n",
        "          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params\n",
        "      }[model_type]\n",
        "      config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "      config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "      # create a from-scratch initialized minGPT model\n",
        "      config = GPTConfig(**config_args)\n",
        "      model = GPT(config)\n",
        "      sd = model.state_dict()\n",
        "      sd_keys = sd.keys()\n",
        "      sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "      # init a huggingface/transformers model\n",
        "      model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "      sd_hf = model_hf.state_dict()\n",
        "\n",
        "      # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "      sd_keys_hf = sd_hf.keys()\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "      transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "      # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "      # this means that we have to transpose these weights when we import them\n",
        "      assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "      for k in sd_keys_hf:\n",
        "          if any(k.endswith(w) for w in transposed):\n",
        "              # special treatment for the Conv1D weights we need to transpose\n",
        "              assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k].t())\n",
        "          else:\n",
        "              # vanilla copy over the other parameters\n",
        "              assert sd_hf[k].shape == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k])\n",
        "\n",
        "      return model\n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "      # start with all of the candidate parameters (that require grad)\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters()}  # (name, tesnor)\n",
        "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "      # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "      # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "      # as parameters with dim = 1 include biases, layer norm gamma, beta\n",
        "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "      num_decay_params = sum(p.numel() for p in decay_params)\n",
        "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "      # if master_process:\n",
        "      print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "      print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "      # Create AdamW optimizer and use the fused version if it is available\n",
        "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "      use_fused = fused_available and device_type == \"cuda\"\n",
        "      # if master_process:\n",
        "      print(f\"using fused AdamW: {use_fused}\")\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "      return optimizer\n",
        "      # fused means to fuse all weight updates into single kernel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e2b34b",
      "metadata": {
        "id": "29e2b34b"
      },
      "source": [
        "By default, all numerical values—such as weights, biases, and logits—are stored in `float32` (32-bit floating point). However, deep learning workloads can often tolerate much lower numerical precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01186ed",
      "metadata": {
        "id": "d01186ed",
        "outputId": "016567a9-3395-4574-b73b-339f3dce9c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 0, loss : 10.942951202392578, dt : 858.21ms, tok/sec : 9545.42972068293\n",
            "step : 1, loss : 9.644050598144531, dt : 455.96ms, tok/sec : 17966.477501992224\n",
            "step : 2, loss : 9.0220947265625, dt : 455.22ms, tok/sec : 17995.591344033703\n",
            "step : 3, loss : 8.714418411254883, dt : 455.65ms, tok/sec : 17978.7269478367\n",
            "step : 4, loss : 8.611956596374512, dt : 455.68ms, tok/sec : 17977.598134836153\n",
            "step : 5, loss : 8.491798400878906, dt : 454.40ms, tok/sec : 18028.109777123434\n",
            "step : 6, loss : 8.387723922729492, dt : 453.86ms, tok/sec : 18049.560166271545\n",
            "step : 7, loss : 8.011007308959961, dt : 454.82ms, tok/sec : 18011.458209534194\n",
            "step : 8, loss : 7.754319190979004, dt : 455.32ms, tok/sec : 17991.624284269827\n",
            "step : 9, loss : 7.754043102264404, dt : 454.47ms, tok/sec : 18025.253576749554\n",
            "step : 10, loss : 7.741122722625732, dt : 453.89ms, tok/sec : 18048.460363209644\n",
            "step : 11, loss : 7.565569877624512, dt : 454.51ms, tok/sec : 18023.891998506042\n",
            "step : 12, loss : 7.525517463684082, dt : 455.94ms, tok/sec : 17967.323051372503\n",
            "step : 13, loss : 7.293506622314453, dt : 454.22ms, tok/sec : 18035.17850711046\n",
            "step : 14, loss : 7.21181583404541, dt : 454.39ms, tok/sec : 18028.715180860454\n",
            "step : 15, loss : 7.048220634460449, dt : 454.97ms, tok/sec : 18005.728950927725\n",
            "step : 16, loss : 6.958008766174316, dt : 455.39ms, tok/sec : 17988.949155701404\n",
            "step : 17, loss : 6.948432922363281, dt : 454.50ms, tok/sec : 18024.109459005067\n",
            "step : 18, loss : 6.827005863189697, dt : 454.80ms, tok/sec : 18012.392981404002\n",
            "step : 19, loss : 6.689802169799805, dt : 454.98ms, tok/sec : 18005.342097816494\n",
            "step : 20, loss : 6.708784103393555, dt : 455.71ms, tok/sec : 17976.375414032667\n",
            "step : 21, loss : 6.592628479003906, dt : 455.13ms, tok/sec : 17999.088706952592\n",
            "step : 22, loss : 6.662064552307129, dt : 454.58ms, tok/sec : 18021.19781457221\n",
            "step : 23, loss : 6.548648834228516, dt : 454.92ms, tok/sec : 18007.729529809152\n",
            "step : 24, loss : 6.53104305267334, dt : 456.12ms, tok/sec : 17960.05392657021\n",
            "step : 25, loss : 6.595467567443848, dt : 456.48ms, tok/sec : 17946.067565683054\n",
            "step : 26, loss : 6.679272651672363, dt : 455.57ms, tok/sec : 17981.841325265515\n",
            "step : 27, loss : 6.582622528076172, dt : 455.79ms, tok/sec : 17973.103102894707\n",
            "step : 28, loss : 6.798788070678711, dt : 456.11ms, tok/sec : 17960.654768077882\n",
            "step : 29, loss : 6.6332197189331055, dt : 455.72ms, tok/sec : 17975.92398993215\n",
            "step : 30, loss : 6.620692253112793, dt : 457.43ms, tok/sec : 17908.8302088198\n",
            "step : 31, loss : 6.65189266204834, dt : 456.03ms, tok/sec : 17963.772278697816\n",
            "step : 32, loss : 6.5267333984375, dt : 456.20ms, tok/sec : 17956.937706302535\n",
            "step : 33, loss : 6.805206298828125, dt : 456.48ms, tok/sec : 17946.161298359293\n",
            "step : 34, loss : 6.705226898193359, dt : 456.20ms, tok/sec : 17956.937706302535\n",
            "step : 35, loss : 6.629811763763428, dt : 457.30ms, tok/sec : 17913.75076861316\n",
            "step : 36, loss : 6.686397075653076, dt : 457.32ms, tok/sec : 17913.181077781126\n",
            "step : 37, loss : 6.7159504890441895, dt : 456.56ms, tok/sec : 17942.693841316657\n",
            "step : 38, loss : 6.5101423263549805, dt : 456.44ms, tok/sec : 17947.614280058042\n",
            "step : 39, loss : 6.562961101531982, dt : 456.40ms, tok/sec : 17949.039368038557\n",
            "step : 40, loss : 6.741015434265137, dt : 456.84ms, tok/sec : 17931.766066953423\n",
            "step : 41, loss : 6.490441799163818, dt : 457.24ms, tok/sec : 17916.30081260116\n",
            "step : 42, loss : 6.499359607696533, dt : 457.41ms, tok/sec : 17909.52097817169\n",
            "step : 43, loss : 6.268959999084473, dt : 457.58ms, tok/sec : 17903.0541029621\n",
            "step : 44, loss : 6.214916706085205, dt : 458.33ms, tok/sec : 17873.764468183832\n",
            "step : 45, loss : 6.275051116943359, dt : 457.39ms, tok/sec : 17910.426526815543\n",
            "step : 46, loss : 6.392098903656006, dt : 456.73ms, tok/sec : 17936.025098098173\n",
            "step : 47, loss : 6.37393045425415, dt : 457.14ms, tok/sec : 17920.2253751489\n",
            "step : 48, loss : 6.264632225036621, dt : 457.02ms, tok/sec : 17924.768801065897\n",
            "step : 49, loss : 6.146198272705078, dt : 457.09ms, tok/sec : 17921.889160929983\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n",
        "\n",
        "  # first iteration is usually slower since the model performs various one-time initializations, memory allocations etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgoHMQ1u7IRi"
      },
      "id": "GgoHMQ1u7IRi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0b2e6ffb",
      "metadata": {
        "id": "0b2e6ffb"
      },
      "source": [
        "## TF32 (TensorFloat-32)\n",
        "FP32 - sign : 1, exponent : 8, mantissa : 23\n",
        "\n",
        "TF32 - sign : 1, exponent : 8, mantissa : 10\n",
        "\n",
        "13 mantissa bits are truncated which makes matrix multiplication faster.\n",
        "Speedup is achieved at the cost of precision.\n",
        "Inputs are fp32, outputs are fp32, but internally, numbers get truncated to perform operations faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KDxumaV4bYn",
        "outputId": "2e7643c9-c584-4669-d50d-0180fba29685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/khushi/miniconda3/lib/python3.13/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step : 0, loss : 11.05107307434082, dt : 385.26ms, tok/sec : 21263.516990211654\n",
            "step : 1, loss : 9.797891616821289, dt : 363.87ms, tok/sec : 22513.835937361786\n",
            "step : 2, loss : 9.275678634643555, dt : 363.43ms, tok/sec : 22540.805248190194\n",
            "step : 3, loss : 8.857524871826172, dt : 363.49ms, tok/sec : 22537.049898694007\n",
            "step : 4, loss : 8.780006408691406, dt : 363.57ms, tok/sec : 22531.80337638628\n",
            "step : 5, loss : 8.659692764282227, dt : 363.63ms, tok/sec : 22528.582778747437\n",
            "step : 6, loss : 8.526681900024414, dt : 363.31ms, tok/sec : 22548.12734145317\n",
            "step : 7, loss : 8.185690879821777, dt : 363.52ms, tok/sec : 22535.21703297604\n",
            "step : 8, loss : 7.856198310852051, dt : 363.48ms, tok/sec : 22537.81860853492\n",
            "step : 9, loss : 7.79470682144165, dt : 363.48ms, tok/sec : 22537.774258464848\n",
            "step : 10, loss : 7.773467063903809, dt : 363.66ms, tok/sec : 22526.780827701594\n",
            "step : 11, loss : 7.627633571624756, dt : 363.79ms, tok/sec : 22518.778922367295\n",
            "step : 12, loss : 7.5837082862854, dt : 364.34ms, tok/sec : 22484.67965500543\n",
            "step : 13, loss : 7.346126556396484, dt : 364.36ms, tok/sec : 22483.193663826583\n",
            "step : 14, loss : 7.227218151092529, dt : 363.61ms, tok/sec : 22529.882725690688\n",
            "step : 15, loss : 7.0419721603393555, dt : 363.71ms, tok/sec : 22523.724078345924\n",
            "step : 16, loss : 6.951973915100098, dt : 363.82ms, tok/sec : 22516.816223764858\n",
            "step : 17, loss : 6.954168319702148, dt : 363.63ms, tok/sec : 22528.553236242136\n",
            "step : 18, loss : 6.823177337646484, dt : 363.36ms, tok/sec : 22545.405038240868\n",
            "step : 19, loss : 6.690393447875977, dt : 363.36ms, tok/sec : 22544.887283235097\n",
            "step : 20, loss : 6.72336483001709, dt : 363.50ms, tok/sec : 22536.26645992961\n",
            "step : 21, loss : 6.603011131286621, dt : 363.57ms, tok/sec : 22532.409187994253\n",
            "step : 22, loss : 6.661812782287598, dt : 363.43ms, tok/sec : 22540.82003550403\n",
            "step : 23, loss : 6.558131217956543, dt : 363.41ms, tok/sec : 22542.328443439055\n",
            "step : 24, loss : 6.540836334228516, dt : 363.40ms, tok/sec : 22542.59465403152\n",
            "step : 25, loss : 6.5976152420043945, dt : 363.34ms, tok/sec : 22546.60336246172\n",
            "step : 26, loss : 6.681509017944336, dt : 363.58ms, tok/sec : 22531.640847106963\n",
            "step : 27, loss : 6.590243816375732, dt : 363.56ms, tok/sec : 22532.911593061435\n",
            "step : 28, loss : 6.7941436767578125, dt : 363.95ms, tok/sec : 22508.46751150489\n",
            "step : 29, loss : 6.624818801879883, dt : 364.73ms, tok/sec : 22460.530917130076\n",
            "step : 30, loss : 6.620328903198242, dt : 364.95ms, tok/sec : 22446.650169951703\n",
            "step : 31, loss : 6.646167755126953, dt : 365.13ms, tok/sec : 22435.55498907597\n",
            "step : 32, loss : 6.524969577789307, dt : 364.90ms, tok/sec : 22449.686033915226\n",
            "step : 33, loss : 6.813068866729736, dt : 365.03ms, tok/sec : 22441.72413842652\n",
            "step : 34, loss : 6.708600044250488, dt : 364.94ms, tok/sec : 22447.77935677084\n",
            "step : 35, loss : 6.634930610656738, dt : 364.94ms, tok/sec : 22447.354065282056\n",
            "step : 36, loss : 6.685081481933594, dt : 364.91ms, tok/sec : 22449.436680803297\n",
            "step : 37, loss : 6.720354080200195, dt : 364.85ms, tok/sec : 22453.17755919816\n",
            "step : 38, loss : 6.507519721984863, dt : 365.10ms, tok/sec : 22437.840549065193\n",
            "step : 39, loss : 6.561004161834717, dt : 364.80ms, tok/sec : 22456.34727868304\n",
            "step : 40, loss : 6.718398094177246, dt : 364.63ms, tok/sec : 22466.44938994887\n",
            "step : 41, loss : 6.487578868865967, dt : 364.79ms, tok/sec : 22456.538077389374\n",
            "step : 42, loss : 6.497045040130615, dt : 365.02ms, tok/sec : 22442.398407857774\n",
            "step : 43, loss : 6.259949684143066, dt : 365.08ms, tok/sec : 22439.071428104955\n",
            "step : 44, loss : 6.212067604064941, dt : 364.70ms, tok/sec : 22462.38102267371\n",
            "step : 45, loss : 6.27888298034668, dt : 364.84ms, tok/sec : 22453.823170221665\n",
            "step : 46, loss : 6.392480850219727, dt : 364.98ms, tok/sec : 22445.095890804692\n",
            "step : 47, loss : 6.366108417510986, dt : 364.93ms, tok/sec : 22448.189998327478\n",
            "step : 48, loss : 6.271721363067627, dt : 364.64ms, tok/sec : 22465.83243081373\n",
            "step : 49, loss : 6.1292572021484375, dt : 364.83ms, tok/sec : 22454.190011207575\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "torch.set_float32_matmul_precision('high') # utilises tf-32 precision for all matmul in linear layers\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n",
        "\n"
      ],
      "id": "4KDxumaV4bYn"
    },
    {
      "cell_type": "markdown",
      "id": "6f6c5175",
      "metadata": {
        "id": "6f6c5175"
      },
      "source": [
        "## BF16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214fcecf",
      "metadata": {
        "id": "214fcecf"
      },
      "source": [
        "**FP16**, which is commonly used after FP32, is more aggressive in reducing numerical precision. <br>\n",
        "sign : 1, exponent : 5, mantissa : 7 <br>\n",
        "The exponent determines the numeric range the format can represent, while the mantissa controls how precisely numbers inside that range can be expressed. Because FP16 shortens the exponent compared to FP32, its numeric range becomes much smaller, even though the internal precision is also reduced.\n",
        "\n",
        "\n",
        "FP32 : exponent = 8 bits -> range = -126 to +127  <br>\n",
        "max value = (1.11111...binary)* 2^127 ~ 3.4 * 10^38 <br>\n",
        "min value approx = 2^-126 ~ 1.18 * 10^-38 <br>\n",
        "\n",
        "FP16 : exponent = 5 bits -> range = -14 to +15 <br>\n",
        "max value = (1.111111.... binary) * 2^15  ~ 6.55 * 10^4 <br>\n",
        "min value = 2^-14 = 6.1*10^-5 <br>\n",
        "\n",
        "Gradients are often extremely small. During backpropagation, it is common to see values like 1e−6, 1e−7, or 1e−8. FP16 simply cannot represent many numbers smaller than about 1e−5, so these tiny gradients underflow to zero.\n",
        "\n",
        "\n",
        "So why GRADIENT SCALING? <br>\n",
        "The idea is to multiply the loss (and therefore all gradients) by a large constant, such as 2^10: <br>\n",
        "\n",
        "scaled_loss = loss * 2^10\n",
        "\n",
        "This makes tiny gradients large enough to fit into FP16 without being rounded to zero.\n",
        "\n",
        "Then after backpropagation:\n",
        "scaled_gradients / 2^10\n",
        "\n",
        "HOW BF16 AVOIDS THE PROBLEM ?\n",
        "BF16 keeps FP32’s exponent size (8 bits), so it has the same numeric range, just lower precision. <br>\n",
        "\n",
        "**BF16** - sign : 1, exponent : 8, mantissa : 7 <br>\n",
        "\n",
        "\n",
        "When using `torch.autocast` with BF16, activations are converted to the BF16 dtype, while model weights remain in FP32. This is known as **Mixed-Precision training**. Only certain operations, such as matrix multiplications, run in BF16 for speed, while operations that are more numerically sensitive, such as layer normalization, continue using FP32 internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebfbf7ba",
      "metadata": {
        "id": "ebfbf7ba",
        "outputId": "630090f4-2c6e-4a03-abda-ac38423d26a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 0, loss : 10.88330078125, dt : 367.87ms, tok/sec : 22268.58632156576\n",
            "step : 1, loss : 9.643840789794922, dt : 271.62ms, tok/sec : 30160.2811423074\n",
            "step : 2, loss : 8.933135986328125, dt : 271.85ms, tok/sec : 30134.25312614561\n",
            "step : 3, loss : 8.750102996826172, dt : 271.79ms, tok/sec : 30141.099639198816\n",
            "step : 4, loss : 8.590248107910156, dt : 271.76ms, tok/sec : 30143.87613895766\n",
            "step : 5, loss : 8.452657699584961, dt : 271.74ms, tok/sec : 30145.91256390482\n",
            "step : 6, loss : 8.377922058105469, dt : 271.71ms, tok/sec : 30150.11852000049\n",
            "step : 7, loss : 8.070959091186523, dt : 271.73ms, tok/sec : 30148.002168988034\n",
            "step : 8, loss : 7.802304267883301, dt : 271.73ms, tok/sec : 30147.790550227688\n",
            "step : 9, loss : 7.761746406555176, dt : 271.78ms, tok/sec : 30142.183735378385\n",
            "step : 10, loss : 7.7245330810546875, dt : 271.68ms, tok/sec : 30153.187756799878\n",
            "step : 11, loss : 7.534664154052734, dt : 271.70ms, tok/sec : 30151.38847323649\n",
            "step : 12, loss : 7.496511459350586, dt : 271.82ms, tok/sec : 30137.662776633133\n",
            "step : 13, loss : 7.308710098266602, dt : 271.77ms, tok/sec : 30142.897694963132\n",
            "step : 14, loss : 7.203392028808594, dt : 271.72ms, tok/sec : 30148.293149634417\n",
            "step : 15, loss : 7.022220611572266, dt : 271.65ms, tok/sec : 30155.992950675794\n",
            "step : 16, loss : 6.93412971496582, dt : 271.80ms, tok/sec : 30139.354672556576\n",
            "step : 17, loss : 6.917430877685547, dt : 271.65ms, tok/sec : 30155.96648414386\n",
            "step : 18, loss : 6.801237106323242, dt : 271.69ms, tok/sec : 30151.600142510393\n",
            "step : 19, loss : 6.651893615722656, dt : 271.73ms, tok/sec : 30147.84345463926\n",
            "step : 20, loss : 6.707292556762695, dt : 271.66ms, tok/sec : 30155.887084826805\n",
            "step : 21, loss : 6.5794477462768555, dt : 271.75ms, tok/sec : 30145.145566366326\n",
            "step : 22, loss : 6.650368690490723, dt : 271.78ms, tok/sec : 30142.474603719063\n",
            "step : 23, loss : 6.55278205871582, dt : 271.99ms, tok/sec : 30118.721231405754\n",
            "step : 24, loss : 6.541208267211914, dt : 271.70ms, tok/sec : 30151.335556382375\n",
            "step : 25, loss : 6.60235595703125, dt : 271.69ms, tok/sec : 30152.26162814677\n",
            "step : 26, loss : 6.693887710571289, dt : 271.69ms, tok/sec : 30152.28808817593\n",
            "step : 27, loss : 6.602880477905273, dt : 271.70ms, tok/sec : 30151.335556382375\n",
            "step : 28, loss : 6.799461364746094, dt : 271.72ms, tok/sec : 30148.63704309463\n",
            "step : 29, loss : 6.636133193969727, dt : 271.65ms, tok/sec : 30156.839904227607\n",
            "step : 30, loss : 6.62226676940918, dt : 271.72ms, tok/sec : 30148.87512777833\n",
            "step : 31, loss : 6.656736373901367, dt : 271.71ms, tok/sec : 30149.615859731268\n",
            "step : 32, loss : 6.530088424682617, dt : 271.77ms, tok/sec : 30143.400131242346\n",
            "step : 33, loss : 6.806828498840332, dt : 271.75ms, tok/sec : 30145.595180879336\n",
            "step : 34, loss : 6.701720237731934, dt : 271.84ms, tok/sec : 30135.231009002924\n",
            "step : 35, loss : 6.632973670959473, dt : 271.81ms, tok/sec : 30138.402957726095\n",
            "step : 36, loss : 6.689846992492676, dt : 271.73ms, tok/sec : 30147.34087022594\n",
            "step : 37, loss : 6.726809501647949, dt : 271.80ms, tok/sec : 30139.46042235916\n",
            "step : 38, loss : 6.517913818359375, dt : 271.77ms, tok/sec : 30143.056357241803\n",
            "step : 39, loss : 6.567768096923828, dt : 271.67ms, tok/sec : 30154.510896427768\n",
            "step : 40, loss : 6.7259521484375, dt : 271.62ms, tok/sec : 30159.778143123105\n",
            "step : 41, loss : 6.4917449951171875, dt : 271.72ms, tok/sec : 30148.663496762678\n",
            "step : 42, loss : 6.5099334716796875, dt : 271.75ms, tok/sec : 30144.98688209325\n",
            "step : 43, loss : 6.279380798339844, dt : 271.65ms, tok/sec : 30156.52229107058\n",
            "step : 44, loss : 6.227662086486816, dt : 271.77ms, tok/sec : 30143.267909544942\n",
            "step : 45, loss : 6.299577713012695, dt : 271.69ms, tok/sec : 30151.467848865934\n",
            "step : 46, loss : 6.408534049987793, dt : 271.69ms, tok/sec : 30152.34100837357\n",
            "step : 47, loss : 6.39091682434082, dt : 271.80ms, tok/sec : 30139.72480011228\n",
            "step : 48, loss : 6.280254364013672, dt : 271.68ms, tok/sec : 30152.579151561742\n",
            "step : 49, loss : 6.1348557472229, dt : 271.99ms, tok/sec : 30118.906040744983\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "torch.set_float32_matmul_precision('high') # utilises tf-32 precision for all matmul in linear layers\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76485c7",
      "metadata": {
        "id": "c76485c7"
      },
      "source": [
        "## torch.compile() - TC\n",
        "\n",
        "It costs compilation time but execution will be much faster. SThe speedup comes from reducing Python overhead and minimizing unnecessary GPU read–write operations. TC will know what kind of operations will come and will try to optimize them. It takes out python interpreter from forward pass entirely and compiles entire neural net as a single object without the interpreter.\n",
        "\n",
        "Read/writes : between GPU and HBM.\n",
        "GPU has HBM (High Bandwidth Memory), CPU has RAM\n",
        "Without `torch.compile()`, Python often cannot detect when a variable is reused repeatedly, so tensors may perform multiple round trips between the GPU and HBM.With TC, the compiler understands the entire operation graph ahead of time and avoids these unnecessary transfers.\n",
        "\n",
        "If certain operations cause frequent switches between GPU and HBM, TC keeps the relevant tensors on the GPU until all dependent computations are complete, reducing memory traffic and improving performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385ea165",
      "metadata": {
        "id": "385ea165",
        "outputId": "65e809a3-c556-4ba3-ef17-3aecd1d307ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 0, loss : 10.927841186523438, dt : 35776.88ms, tok/sec : 228.97470031541704\n",
            "step : 1, loss : 9.605266571044922, dt : 166.52ms, tok/sec : 49195.32722157395\n",
            "step : 2, loss : 8.896303176879883, dt : 165.55ms, tok/sec : 49484.82444419161\n",
            "step : 3, loss : 8.641592979431152, dt : 165.81ms, tok/sec : 49404.91977822256\n",
            "step : 4, loss : 8.505861282348633, dt : 165.40ms, tok/sec : 49528.83587321905\n",
            "step : 5, loss : 8.390480995178223, dt : 165.72ms, tok/sec : 49431.288932112024\n",
            "step : 6, loss : 8.291648864746094, dt : 165.38ms, tok/sec : 49533.119929880406\n",
            "step : 7, loss : 7.981864929199219, dt : 165.75ms, tok/sec : 49424.818421385746\n",
            "step : 8, loss : 7.707769870758057, dt : 165.38ms, tok/sec : 49533.97683014734\n",
            "step : 9, loss : 7.687943458557129, dt : 165.70ms, tok/sec : 49437.6900049064\n",
            "step : 10, loss : 7.670999526977539, dt : 165.43ms, tok/sec : 49518.55716006894\n",
            "step : 11, loss : 7.490860939025879, dt : 165.72ms, tok/sec : 49434.13364870134\n",
            "step : 12, loss : 7.430739402770996, dt : 165.40ms, tok/sec : 49527.12245804\n",
            "step : 13, loss : 7.248168468475342, dt : 165.74ms, tok/sec : 49427.662393260754\n",
            "step : 14, loss : 7.13953971862793, dt : 165.42ms, tok/sec : 49523.76736335819\n",
            "step : 15, loss : 6.969720840454102, dt : 165.75ms, tok/sec : 49424.39185383157\n",
            "step : 16, loss : 6.885315895080566, dt : 165.36ms, tok/sec : 49539.0474301063\n",
            "step : 17, loss : 6.875174522399902, dt : 166.33ms, tok/sec : 49250.89388759645\n",
            "step : 18, loss : 6.748569965362549, dt : 166.00ms, tok/sec : 49350.422796736755\n",
            "step : 19, loss : 6.6308698654174805, dt : 166.40ms, tok/sec : 49230.35909779952\n",
            "step : 20, loss : 6.694441795349121, dt : 166.05ms, tok/sec : 49334.97550175316\n",
            "step : 21, loss : 6.563112258911133, dt : 166.37ms, tok/sec : 49240.73020145\n",
            "step : 22, loss : 6.639683246612549, dt : 166.00ms, tok/sec : 49347.942074611325\n",
            "step : 23, loss : 6.528955459594727, dt : 166.33ms, tok/sec : 49250.682101340215\n",
            "step : 24, loss : 6.523393630981445, dt : 165.96ms, tok/sec : 49361.2700251549\n",
            "step : 25, loss : 6.582095146179199, dt : 166.30ms, tok/sec : 49261.69740241493\n",
            "step : 26, loss : 6.691847801208496, dt : 166.40ms, tok/sec : 49230.00641599672\n",
            "step : 27, loss : 6.59812068939209, dt : 166.30ms, tok/sec : 49260.355531724854\n",
            "step : 28, loss : 6.809506416320801, dt : 166.00ms, tok/sec : 49349.9975123735\n",
            "step : 29, loss : 6.654585361480713, dt : 166.34ms, tok/sec : 49247.15259853805\n",
            "step : 30, loss : 6.6293110847473145, dt : 166.02ms, tok/sec : 49342.62708120916\n",
            "step : 31, loss : 6.671643257141113, dt : 166.30ms, tok/sec : 49259.29621176497\n",
            "step : 32, loss : 6.5371809005737305, dt : 166.01ms, tok/sec : 49345.17813515167\n",
            "step : 33, loss : 6.807474136352539, dt : 166.44ms, tok/sec : 49218.22974602856\n",
            "step : 34, loss : 6.699991703033447, dt : 165.98ms, tok/sec : 49354.038009793316\n",
            "step : 35, loss : 6.617452621459961, dt : 166.45ms, tok/sec : 49214.63440094763\n",
            "step : 36, loss : 6.656157493591309, dt : 166.31ms, tok/sec : 49258.16632069088\n",
            "step : 37, loss : 6.690360069274902, dt : 166.45ms, tok/sec : 49216.749246201645\n",
            "step : 38, loss : 6.4737396240234375, dt : 166.18ms, tok/sec : 49296.25808710124\n",
            "step : 39, loss : 6.520458221435547, dt : 166.35ms, tok/sec : 49245.105718695944\n",
            "step : 40, loss : 6.710758209228516, dt : 166.00ms, tok/sec : 49349.57223534008\n",
            "step : 41, loss : 6.455700874328613, dt : 166.29ms, tok/sec : 49264.73453762343\n",
            "step : 42, loss : 6.468833923339844, dt : 166.02ms, tok/sec : 49342.34364705308\n",
            "step : 43, loss : 6.2221784591674805, dt : 166.24ms, tok/sec : 49278.44161908737\n",
            "step : 44, loss : 6.166703224182129, dt : 165.98ms, tok/sec : 49354.39247095595\n",
            "step : 45, loss : 6.244176864624023, dt : 166.36ms, tok/sec : 49244.0470545071\n",
            "step : 46, loss : 6.3533406257629395, dt : 166.13ms, tok/sec : 49311.398063407825\n",
            "step : 47, loss : 6.31315803527832, dt : 166.32ms, tok/sec : 49253.64727468722\n",
            "step : 48, loss : 6.219126224517822, dt : 166.02ms, tok/sec : 49344.1860272745\n",
            "step : 49, loss : 6.082715034484863, dt : 166.30ms, tok/sec : 49259.013734136926\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "902ed14d",
      "metadata": {
        "id": "902ed14d"
      },
      "source": [
        "## Flash Attention\n",
        "Flash Attention replaces the attention implementation inside the `CausalSelfAttention` class. The four lines of PyTorch attention code are fused into a single highly optimized kernel. This fusion goes far beyond what `torch.compile()` can automatically detect, because it requires an algorithmic re-write of how attention is computed. Flash Attention is very mindful about how pytorch does that computation such that fewer read/writes to HBM are ensured.\n",
        "\n",
        "The key idea is that Flash Attention **never materializes the full attention matrix in HBM**. Instead, it uses the “online softmax” trick introduced in the paper *Online Normalizer Calculation for Softmax* (2018). This allows the softmax to be computed correctly without storing all logits at once.\n",
        "\n",
        "The trick is to maintain two running values for each query row:\n",
        "\n",
        "- `m`: the running maximum of logits seen so far  \n",
        "- `s`: the running sum of `exp(logits − m)`\n",
        "\n",
        "Whenever the running maximum changes from `m` to `m'`, the running sum can be updated using:\n",
        "s = s * exp(m − m') + Σ exp(logits − m')   [sum over all logits]\n",
        "\n",
        "`(logits - m)` keeps the exponentials numerically stable and prevents overflow.  \n",
        "By the end of the streaming pass over the logits, Flash Attention has:\n",
        "- the correct maximum\n",
        "- the correct sum\n",
        "So we can compute the correct softmax, even though we never saw the whole row at once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbfef03",
      "metadata": {
        "id": "3fbfef03",
        "outputId": "9320e7c9-abcb-4083-da8e-886b675228d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 0, loss : 10.986957550048828, dt : 1649.81ms, tok/sec : 4965.431513590949\n",
            "step : 1, loss : 9.674814224243164, dt : 126.05ms, tok/sec : 64991.31494546816\n",
            "step : 2, loss : 9.05832576751709, dt : 125.59ms, tok/sec : 65226.46887481586\n",
            "step : 3, loss : 8.783764839172363, dt : 125.67ms, tok/sec : 65185.13852516552\n",
            "step : 4, loss : 8.659647941589355, dt : 125.44ms, tok/sec : 65308.045662760065\n",
            "step : 5, loss : 8.559056282043457, dt : 125.65ms, tok/sec : 65195.15694202786\n",
            "step : 6, loss : 8.460760116577148, dt : 125.52ms, tok/sec : 65265.868061649744\n",
            "step : 7, loss : 8.097240447998047, dt : 125.69ms, tok/sec : 65176.23585023768\n",
            "step : 8, loss : 7.821047782897949, dt : 125.62ms, tok/sec : 65214.58412669491\n",
            "step : 9, loss : 7.800475120544434, dt : 125.70ms, tok/sec : 65169.06601927016\n",
            "step : 10, loss : 7.793295860290527, dt : 125.67ms, tok/sec : 65184.272880593875\n",
            "step : 11, loss : 7.582406997680664, dt : 125.79ms, tok/sec : 65126.327021566154\n",
            "step : 12, loss : 7.522665500640869, dt : 125.42ms, tok/sec : 65314.74947534901\n",
            "step : 13, loss : 7.318831443786621, dt : 125.84ms, tok/sec : 65100.53783760741\n",
            "step : 14, loss : 7.217833042144775, dt : 125.43ms, tok/sec : 65313.75623108624\n",
            "step : 15, loss : 7.049642562866211, dt : 125.72ms, tok/sec : 65160.16773371363\n",
            "step : 16, loss : 6.959827423095703, dt : 125.44ms, tok/sec : 65306.4319874708\n",
            "step : 17, loss : 6.92303466796875, dt : 125.73ms, tok/sec : 65156.33727573894\n",
            "step : 18, loss : 6.784767150878906, dt : 125.41ms, tok/sec : 65321.578847510216\n",
            "step : 19, loss : 6.653738021850586, dt : 125.67ms, tok/sec : 65186.49887023121\n",
            "step : 20, loss : 6.701663017272949, dt : 125.37ms, tok/sec : 65342.9453766091\n",
            "step : 21, loss : 6.577948093414307, dt : 125.71ms, tok/sec : 65164.98733673069\n",
            "step : 22, loss : 6.653797626495361, dt : 125.39ms, tok/sec : 65332.38458934975\n",
            "step : 23, loss : 6.541135787963867, dt : 125.61ms, tok/sec : 65215.5743569498\n",
            "step : 24, loss : 6.518791198730469, dt : 125.40ms, tok/sec : 65328.65806765636\n",
            "step : 25, loss : 6.578098297119141, dt : 125.64ms, tok/sec : 65203.69357826021\n",
            "step : 26, loss : 6.695233345031738, dt : 125.40ms, tok/sec : 65327.16757800914\n",
            "step : 27, loss : 6.606977462768555, dt : 125.63ms, tok/sec : 65207.52960645552\n",
            "step : 28, loss : 6.802163600921631, dt : 125.38ms, tok/sec : 65337.72667857054\n",
            "step : 29, loss : 6.634075164794922, dt : 125.63ms, tok/sec : 65206.29212869042\n",
            "step : 30, loss : 6.631628036499023, dt : 125.36ms, tok/sec : 65348.289193106044\n",
            "step : 31, loss : 6.652105331420898, dt : 125.64ms, tok/sec : 65201.09523493257\n",
            "step : 32, loss : 6.520171165466309, dt : 125.37ms, tok/sec : 65344.18804663891\n",
            "step : 33, loss : 6.807209491729736, dt : 125.69ms, tok/sec : 65174.99955993262\n",
            "step : 34, loss : 6.693218231201172, dt : 125.35ms, tok/sec : 65354.00680173771\n",
            "step : 35, loss : 6.605805397033691, dt : 125.71ms, tok/sec : 65166.594029511056\n",
            "step : 36, loss : 6.645748615264893, dt : 125.35ms, tok/sec : 65353.38527407357\n",
            "step : 37, loss : 6.667791366577148, dt : 125.92ms, tok/sec : 65056.04095759586\n",
            "step : 38, loss : 6.446996688842773, dt : 125.39ms, tok/sec : 65331.01814869955\n",
            "step : 39, loss : 6.483942031860352, dt : 125.70ms, tok/sec : 65171.909039351915\n",
            "step : 40, loss : 6.667932987213135, dt : 125.41ms, tok/sec : 65321.70303113261\n",
            "step : 41, loss : 6.426165580749512, dt : 125.67ms, tok/sec : 65189.09605029996\n",
            "step : 42, loss : 6.455839157104492, dt : 125.36ms, tok/sec : 65346.052140308704\n",
            "step : 43, loss : 6.198325157165527, dt : 125.63ms, tok/sec : 65205.549664576\n",
            "step : 44, loss : 6.156863689422607, dt : 125.40ms, tok/sec : 65324.435190290234\n",
            "step : 45, loss : 6.235267639160156, dt : 125.69ms, tok/sec : 65174.38143236779\n",
            "step : 46, loss : 6.338676929473877, dt : 125.38ms, tok/sec : 65337.478189932706\n",
            "step : 47, loss : 6.3154449462890625, dt : 125.74ms, tok/sec : 65150.03653435572\n",
            "step : 48, loss : 6.204071521759033, dt : 125.34ms, tok/sec : 65356.61734702612\n",
            "step : 49, loss : 6.061471462249756, dt : 125.76ms, tok/sec : 65142.50195372117\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "model=torch.compile(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227b75a7",
      "metadata": {
        "id": "227b75a7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Numbers\n",
        "\n",
        "CUDA kernels and GPU hardware are often optimized around powers of two, and many low-level GPU blocks are designed with power-of-two dimensions in mind. Because of this, changing certain model parameters (such as vocabulary size) to the nearest power of two can lead to measurable performance improvements.\n",
        "\n",
        "For example, increasing the vocabulary size to a power of two can speed up indexing and matrix operations. However, GPT-2 has a fixed vocabulary size of 50,257. If we expand this to the next power of two, such as 50,304, the tokens from index 50,258 onward will never actually be produced because the tokenizer will never generate them. This means we are wasting a small amount of embedding space.\n",
        "\n",
        "The model must also implicitly learn to assign extremely low probabilities (effectively negative infinity logits) to these unused token IDs, since they should never be predicted during inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31e44ea",
      "metadata": {
        "id": "c31e44ea",
        "outputId": "89bcac12-6146-47f0-f72e-c12b93641f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 0, loss : 10.941413879394531, dt : 1396.59ms, tok/sec : 5865.723813466514\n",
            "step : 1, loss : 9.538830757141113, dt : 130.61ms, tok/sec : 62723.373155798305\n",
            "step : 2, loss : 8.87597942352295, dt : 123.07ms, tok/sec : 66564.51706067762\n",
            "step : 3, loss : 8.73884105682373, dt : 123.35ms, tok/sec : 66410.51591854406\n",
            "step : 4, loss : 8.543574333190918, dt : 123.03ms, tok/sec : 66586.83362854496\n",
            "step : 5, loss : 8.380556106567383, dt : 123.33ms, tok/sec : 66421.42816436915\n",
            "step : 6, loss : 8.306436538696289, dt : 123.03ms, tok/sec : 66587.86597055073\n",
            "step : 7, loss : 8.031896591186523, dt : 123.30ms, tok/sec : 66437.35363851694\n",
            "step : 8, loss : 7.7480974197387695, dt : 123.02ms, tok/sec : 66592.77003225006\n",
            "step : 9, loss : 7.698884010314941, dt : 123.32ms, tok/sec : 66428.4909404459\n",
            "step : 10, loss : 7.647953510284424, dt : 123.25ms, tok/sec : 66466.01303022332\n",
            "step : 11, loss : 7.520112037658691, dt : 123.32ms, tok/sec : 66431.44491039614\n",
            "step : 12, loss : 7.48830509185791, dt : 123.07ms, tok/sec : 66566.32246940464\n",
            "step : 13, loss : 7.273999214172363, dt : 123.34ms, tok/sec : 66418.7318642233\n",
            "step : 14, loss : 7.185067176818848, dt : 123.05ms, tok/sec : 66572.25521625491\n",
            "step : 15, loss : 7.009355068206787, dt : 123.31ms, tok/sec : 66433.11466179951\n",
            "step : 16, loss : 6.9168901443481445, dt : 123.03ms, tok/sec : 66587.3497955466\n",
            "step : 17, loss : 6.8921942710876465, dt : 123.28ms, tok/sec : 66451.87302103036\n",
            "step : 18, loss : 6.768439292907715, dt : 123.01ms, tok/sec : 66596.64217018324\n",
            "step : 19, loss : 6.6415205001831055, dt : 123.29ms, tok/sec : 66447.63219137683\n",
            "step : 20, loss : 6.689542293548584, dt : 123.07ms, tok/sec : 66563.09859395039\n",
            "step : 21, loss : 6.566616058349609, dt : 123.23ms, tok/sec : 66479.00142981246\n",
            "step : 22, loss : 6.65175724029541, dt : 123.03ms, tok/sec : 66585.28517555317\n",
            "step : 23, loss : 6.538750648498535, dt : 123.27ms, tok/sec : 66455.08613116662\n",
            "step : 24, loss : 6.528154373168945, dt : 123.05ms, tok/sec : 66575.09299076739\n",
            "step : 25, loss : 6.58723783493042, dt : 123.41ms, tok/sec : 66382.6738510938\n",
            "step : 26, loss : 6.692770004272461, dt : 123.04ms, tok/sec : 66580.6402486523\n",
            "step : 27, loss : 6.596310615539551, dt : 123.28ms, tok/sec : 66450.33083852602\n",
            "step : 28, loss : 6.801966190338135, dt : 123.08ms, tok/sec : 66556.78070381038\n",
            "step : 29, loss : 6.630880355834961, dt : 123.24ms, tok/sec : 66470.12761742629\n",
            "step : 30, loss : 6.616978168487549, dt : 123.08ms, tok/sec : 66558.45676185605\n",
            "step : 31, loss : 6.645723819732666, dt : 123.29ms, tok/sec : 66446.98968864823\n",
            "step : 32, loss : 6.5131449699401855, dt : 123.06ms, tok/sec : 66570.8364197172\n",
            "step : 33, loss : 6.782260894775391, dt : 123.31ms, tok/sec : 66434.3991430748\n",
            "step : 34, loss : 6.665173053741455, dt : 123.06ms, tok/sec : 66566.96728197021\n",
            "step : 35, loss : 6.576693534851074, dt : 123.36ms, tok/sec : 66407.94885234749\n",
            "step : 36, loss : 6.616922378540039, dt : 123.03ms, tok/sec : 66585.80131854843\n",
            "step : 37, loss : 6.625378131866455, dt : 123.40ms, tok/sec : 66386.00853596097\n",
            "step : 38, loss : 6.477027893066406, dt : 123.11ms, tok/sec : 66542.60205245596\n",
            "step : 39, loss : 6.471493721008301, dt : 123.44ms, tok/sec : 66363.31364823313\n",
            "step : 40, loss : 6.674147605895996, dt : 123.07ms, tok/sec : 66562.32491030678\n",
            "step : 41, loss : 6.436033725738525, dt : 123.30ms, tok/sec : 66437.99595490845\n",
            "step : 42, loss : 6.471558570861816, dt : 123.06ms, tok/sec : 66570.96539872166\n",
            "step : 43, loss : 6.233310222625732, dt : 123.34ms, tok/sec : 66416.80606956808\n",
            "step : 44, loss : 6.171761512756348, dt : 123.03ms, tok/sec : 66583.86582357467\n",
            "step : 45, loss : 6.241548538208008, dt : 123.63ms, tok/sec : 66260.67558634203\n",
            "step : 46, loss : 6.374603271484375, dt : 123.50ms, tok/sec : 66329.62050905862\n",
            "step : 47, loss : 6.35837459564209, dt : 123.76ms, tok/sec : 66191.61881351197\n",
            "step : 48, loss : 6.254428863525391, dt : 123.47ms, tok/sec : 66350.3704101357\n",
            "step : 49, loss : 6.1181488037109375, dt : 123.73ms, tok/sec : 66206.03097602614\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
        "model.to(device)\n",
        "model=torch.compile(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c05523f",
      "metadata": {
        "id": "0c05523f"
      },
      "source": [
        "# Hyperparameters, AdamW, Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea0823d",
      "metadata": {
        "id": "6ea0823d"
      },
      "source": [
        "gpt-2 - gpt3 change : gpt3 context window 2048 from 1024 and was trained for longer time and on larger dataset\n",
        "\n",
        "AdamW beta1 =0.9 , beta2=0.95, change in code Gradient Clipping\n",
        "\n",
        "Norms are high at beginning and then stablise during training bcoz at start model is completely random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65babc74",
      "metadata": {
        "id": "65babc74",
        "outputId": "20d1b6d1-5dec-4875-ea07-d81a692d1d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 0 | loss : 10.999530792236328 | dt : 197.48ms | tok/sec : 41483.290495110945 | norm : 14.0023\n",
            "step : 1 | loss : 9.577607154846191 | dt : 125.98ms | tok/sec : 65024.8923996374 | norm : 4.3634\n",
            "step : 2 | loss : 8.831929206848145 | dt : 125.31ms | tok/sec : 65374.52384875462 | norm : 2.9763\n",
            "step : 3 | loss : 8.84640884399414 | dt : 125.59ms | tok/sec : 65229.19307304738 | norm : 4.2062\n",
            "step : 4 | loss : 8.569517135620117 | dt : 124.85ms | tok/sec : 65615.09534503568 | norm : 3.7140\n",
            "step : 5 | loss : 8.39919376373291 | dt : 125.07ms | tok/sec : 65497.77231153997 | norm : 2.7486\n",
            "step : 6 | loss : 8.354333877563477 | dt : 124.81ms | tok/sec : 65636.40358672438 | norm : 2.1312\n",
            "step : 7 | loss : 8.035881042480469 | dt : 125.17ms | tok/sec : 65448.24267082674 | norm : 2.6961\n",
            "step : 8 | loss : 7.711540222167969 | dt : 125.41ms | tok/sec : 65323.81422495822 | norm : 1.7410\n",
            "step : 9 | loss : 7.650970458984375 | dt : 125.64ms | tok/sec : 65202.57997719031 | norm : 1.6895\n",
            "step : 10 | loss : 7.63751745223999 | dt : 125.39ms | tok/sec : 65329.651765206945 | norm : 2.1036\n",
            "step : 11 | loss : 7.474471092224121 | dt : 125.71ms | tok/sec : 65165.11092567024 | norm : 1.9229\n",
            "step : 12 | loss : 7.407618522644043 | dt : 125.37ms | tok/sec : 65344.93367134443 | norm : 1.4565\n",
            "step : 13 | loss : 7.176350116729736 | dt : 125.67ms | tok/sec : 65185.50952273447 | norm : 1.1589\n",
            "step : 14 | loss : 7.05731725692749 | dt : 125.36ms | tok/sec : 65347.04636708736 | norm : 1.2873\n",
            "step : 15 | loss : 6.866204261779785 | dt : 125.75ms | tok/sec : 65146.33078509889 | norm : 1.1259\n",
            "step : 16 | loss : 6.789122104644775 | dt : 125.38ms | tok/sec : 65335.11764213729 | norm : 1.0567\n",
            "step : 17 | loss : 6.767908573150635 | dt : 125.68ms | tok/sec : 65182.912628431804 | norm : 1.0052\n",
            "step : 18 | loss : 6.632871627807617 | dt : 125.39ms | tok/sec : 65329.7759795263 | norm : 0.8641\n",
            "step : 19 | loss : 6.455571174621582 | dt : 125.69ms | tok/sec : 65173.88693875771 | norm : 0.9609\n",
            "step : 20 | loss : 6.509748458862305 | dt : 125.40ms | tok/sec : 65325.304561588964 | norm : 1.0722\n",
            "step : 21 | loss : 6.358070373535156 | dt : 125.66ms | tok/sec : 65193.91993381894 | norm : 0.8955\n",
            "step : 22 | loss : 6.476310729980469 | dt : 125.40ms | tok/sec : 65326.546560728864 | norm : 1.3081\n",
            "step : 23 | loss : 6.337381362915039 | dt : 125.65ms | tok/sec : 65196.641413844336 | norm : 1.4532\n",
            "step : 24 | loss : 6.364072799682617 | dt : 125.38ms | tok/sec : 65339.96316136774 | norm : 1.9589\n",
            "step : 25 | loss : 6.392240524291992 | dt : 125.66ms | tok/sec : 65190.3328754544 | norm : 0.9704\n",
            "step : 26 | loss : 6.637075424194336 | dt : 125.38ms | tok/sec : 65337.72667857054 | norm : 1.6077\n",
            "step : 27 | loss : 6.558969497680664 | dt : 125.64ms | tok/sec : 65201.46641415771 | norm : 2.1720\n",
            "step : 28 | loss : 6.7819647789001465 | dt : 125.41ms | tok/sec : 65319.46779816967 | norm : 1.7513\n",
            "step : 29 | loss : 6.5443243980407715 | dt : 125.88ms | tok/sec : 65077.97364284118 | norm : 1.5989\n",
            "step : 30 | loss : 6.506575107574463 | dt : 125.51ms | tok/sec : 65267.727689417465 | norm : 0.8585\n",
            "step : 31 | loss : 6.477158069610596 | dt : 125.72ms | tok/sec : 65162.0213428106 | norm : 1.2825\n",
            "step : 32 | loss : 6.3334503173828125 | dt : 125.47ms | tok/sec : 65289.30711860285 | norm : 1.3090\n",
            "step : 33 | loss : 6.630626678466797 | dt : 125.67ms | tok/sec : 65188.23030061565 | norm : 1.0215\n",
            "step : 34 | loss : 6.499607563018799 | dt : 125.39ms | tok/sec : 65333.25417225217 | norm : 1.3521\n",
            "step : 35 | loss : 6.389703750610352 | dt : 125.68ms | tok/sec : 65182.788971959424 | norm : 0.9485\n",
            "step : 36 | loss : 6.411311626434326 | dt : 125.74ms | tok/sec : 65147.81303421234 | norm : 1.2357\n",
            "step : 37 | loss : 6.403480529785156 | dt : 125.70ms | tok/sec : 65171.29097040715 | norm : 1.0657\n",
            "step : 38 | loss : 6.175378322601318 | dt : 125.40ms | tok/sec : 65326.04975540569 | norm : 1.1101\n",
            "step : 39 | loss : 6.204874038696289 | dt : 125.61ms | tok/sec : 65215.32679656687 | norm : 1.1653\n",
            "step : 40 | loss : 6.507342338562012 | dt : 125.38ms | tok/sec : 65337.478189932706 | norm : 1.1966\n",
            "step : 41 | loss : 6.379093170166016 | dt : 125.62ms | tok/sec : 65210.128462653825 | norm : 1.1559\n",
            "step : 42 | loss : 6.398321151733398 | dt : 125.40ms | tok/sec : 65327.78860709675 | norm : 1.2267\n",
            "step : 43 | loss : 6.115249156951904 | dt : 125.60ms | tok/sec : 65222.50681083051 | norm : 1.4088\n",
            "step : 44 | loss : 6.046464920043945 | dt : 125.37ms | tok/sec : 65342.9453766091 | norm : 1.2099\n",
            "step : 45 | loss : 6.098727703094482 | dt : 125.69ms | tok/sec : 65175.61769922248 | norm : 1.1191\n",
            "step : 46 | loss : 6.2075886726379395 | dt : 125.40ms | tok/sec : 65325.180364272404 | norm : 0.9739\n",
            "step : 47 | loss : 6.178984642028809 | dt : 125.60ms | tok/sec : 65223.744904118845 | norm : 1.3810\n",
            "step : 48 | loss : 6.050238609313965 | dt : 125.41ms | tok/sec : 65320.4612161561 | norm : 1.0254\n",
            "step : 49 | loss : 5.9089813232421875 | dt : 125.64ms | tok/sec : 65200.229166508536 | norm : 0.9319\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
        "model.to(device)\n",
        "model=torch.compile(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i} | loss : {loss.item()} | dt : {dt:.2f}ms | tok/sec : {tokens_per_sec} | norm : {norm:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e28eebcf",
      "metadata": {
        "id": "e28eebcf"
      },
      "source": [
        "# Learning Rate :  Cosine decay LR schedule with Warm-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3103ecb7",
      "metadata": {
        "id": "3103ecb7"
      },
      "outputs": [],
      "source": [
        "max_lr = 6e-4  # as per GPT-paper\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  #  linear warmup for warm-iter steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1)/ warmup_steps\n",
        "\n",
        "  # if it > lr_decay iters, return min lr\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "\n",
        "  # in between, use cosine decay down to min lr\n",
        "  decay_ratio = (it-warmup_steps) / (max_steps-warmup_steps)\n",
        "  assert 0<=decay_ratio<=1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "  return min_lr + coeff * (max_lr-min_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5ef2bd",
      "metadata": {
        "id": "ff5ef2bd",
        "outputId": "aef6841f-536f-430a-e3b3-36a4cf1d7079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step : 49 | loss : 10.987662315368652 | dt : 135.39ms | tok/sec : 60506.558510076284 | lr : 6.000000e-05\n",
            "step : 49 | loss : 9.78203010559082 | dt : 125.44ms | tok/sec : 65304.94251182663 | lr : 1.200000e-04\n",
            "step : 49 | loss : 9.17325496673584 | dt : 124.80ms | tok/sec : 65643.30039298399 | lr : 1.800000e-04\n",
            "step : 49 | loss : 9.601394653320312 | dt : 125.23ms | tok/sec : 65416.84204834715 | lr : 2.400000e-04\n",
            "step : 49 | loss : 9.002758026123047 | dt : 124.93ms | tok/sec : 65573.64661337018 | lr : 3.000000e-04\n",
            "step : 49 | loss : 8.708261489868164 | dt : 125.20ms | tok/sec : 65431.79096706867 | lr : 3.600000e-04\n",
            "step : 49 | loss : 8.647960662841797 | dt : 125.23ms | tok/sec : 65414.35122595728 | lr : 4.200000e-04\n",
            "step : 49 | loss : 8.216434478759766 | dt : 125.61ms | tok/sec : 65219.16419372289 | lr : 4.800000e-04\n",
            "step : 49 | loss : 7.806668758392334 | dt : 125.28ms | tok/sec : 65390.32453335769 | lr : 5.400000e-04\n",
            "step : 49 | loss : 7.660083770751953 | dt : 125.52ms | tok/sec : 65266.73587475377 | lr : 6.000000e-04\n",
            "step : 49 | loss : 7.451149940490723 | dt : 125.28ms | tok/sec : 65390.94676382765 | lr : 6.000000e-04\n",
            "step : 49 | loss : 7.208303451538086 | dt : 125.48ms | tok/sec : 65283.600824601235 | lr : 5.991677e-04\n",
            "step : 49 | loss : 7.116086006164551 | dt : 125.35ms | tok/sec : 65355.49851636563 | lr : 5.966759e-04\n",
            "step : 49 | loss : 6.896973609924316 | dt : 125.52ms | tok/sec : 65265.992033537404 | lr : 5.925399e-04\n",
            "step : 49 | loss : 6.7825093269348145 | dt : 125.28ms | tok/sec : 65391.56900613954 | lr : 5.867853e-04\n",
            "step : 49 | loss : 6.603608131408691 | dt : 125.58ms | tok/sec : 65231.29829117757 | lr : 5.794475e-04\n",
            "step : 49 | loss : 6.717985153198242 | dt : 125.28ms | tok/sec : 65391.444556729795 | lr : 5.705718e-04\n",
            "step : 49 | loss : 6.749535083770752 | dt : 125.56ms | tok/sec : 65245.04746822223 | lr : 5.602128e-04\n",
            "step : 49 | loss : 6.659117698669434 | dt : 125.26ms | tok/sec : 65399.659233965445 | lr : 5.484346e-04\n",
            "step : 49 | loss : 6.490269660949707 | dt : 125.52ms | tok/sec : 65262.892877629245 | lr : 5.353096e-04\n",
            "step : 49 | loss : 6.596914291381836 | dt : 125.24ms | tok/sec : 65408.12499976205 | lr : 5.209188e-04\n",
            "step : 49 | loss : 6.430337905883789 | dt : 125.56ms | tok/sec : 65243.8085660684 | lr : 5.053510e-04\n",
            "step : 49 | loss : 6.533102989196777 | dt : 125.26ms | tok/sec : 65402.52240948077 | lr : 4.887020e-04\n",
            "step : 49 | loss : 10.545022964477539 | dt : 125.54ms | tok/sec : 65254.09287953919 | lr : 4.710746e-04\n",
            "step : 49 | loss : 6.4387125968933105 | dt : 125.29ms | tok/sec : 65382.85869126725 | lr : 4.525774e-04\n",
            "step : 49 | loss : 6.469264984130859 | dt : 125.52ms | tok/sec : 65261.90120989952 | lr : 4.333245e-04\n",
            "step : 49 | loss : 6.747941970825195 | dt : 125.24ms | tok/sec : 65409.24563300489 | lr : 4.134346e-04\n",
            "step : 49 | loss : 6.58652925491333 | dt : 125.56ms | tok/sec : 65245.04746822223 | lr : 3.930302e-04\n",
            "step : 49 | loss : 6.828632354736328 | dt : 125.25ms | tok/sec : 65403.02037863993 | lr : 3.722373e-04\n",
            "step : 49 | loss : 6.597226142883301 | dt : 125.53ms | tok/sec : 65257.563017900386 | lr : 3.511840e-04\n",
            "step : 49 | loss : 6.568767547607422 | dt : 125.27ms | tok/sec : 65396.547370985485 | lr : 3.300000e-04\n",
            "step : 49 | loss : 6.550079345703125 | dt : 125.56ms | tok/sec : 65245.666936943264 | lr : 3.088160e-04\n",
            "step : 49 | loss : 6.4009294509887695 | dt : 125.40ms | tok/sec : 65327.291782882065 | lr : 2.877627e-04\n",
            "step : 49 | loss : 6.685126781463623 | dt : 125.63ms | tok/sec : 65209.88094361466 | lr : 2.669698e-04\n",
            "step : 49 | loss : 6.555436611175537 | dt : 125.27ms | tok/sec : 65394.555934087264 | lr : 2.465654e-04\n",
            "step : 49 | loss : 6.4669599533081055 | dt : 125.50ms | tok/sec : 65277.15143236563 | lr : 2.266755e-04\n",
            "step : 49 | loss : 6.488414764404297 | dt : 125.25ms | tok/sec : 65406.257363018914 | lr : 2.074226e-04\n",
            "step : 49 | loss : 6.510680198669434 | dt : 125.54ms | tok/sec : 65255.9518363347 | lr : 1.889254e-04\n",
            "step : 49 | loss : 6.281467914581299 | dt : 125.29ms | tok/sec : 65385.5959687456 | lr : 1.712980e-04\n",
            "step : 49 | loss : 6.342024803161621 | dt : 125.51ms | tok/sec : 65268.84351694609 | lr : 1.546490e-04\n",
            "step : 49 | loss : 6.576178550720215 | dt : 125.23ms | tok/sec : 65413.72854999734 | lr : 1.390812e-04\n",
            "step : 49 | loss : 6.385190010070801 | dt : 125.58ms | tok/sec : 65231.793656368594 | lr : 1.246904e-04\n",
            "step : 49 | loss : 6.4398393630981445 | dt : 125.28ms | tok/sec : 65390.697870218646 | lr : 1.115654e-04\n",
            "step : 49 | loss : 6.174651622772217 | dt : 125.54ms | tok/sec : 65254.58859099533 | lr : 9.978716e-05\n",
            "step : 49 | loss : 6.11740779876709 | dt : 125.25ms | tok/sec : 65406.257363018914 | lr : 8.942824e-05\n",
            "step : 49 | loss : 6.195971488952637 | dt : 125.48ms | tok/sec : 65286.82599864333 | lr : 8.055253e-05\n",
            "step : 49 | loss : 6.270228385925293 | dt : 125.24ms | tok/sec : 65408.87208432402 | lr : 7.321474e-05\n",
            "step : 49 | loss : 6.170895576477051 | dt : 125.53ms | tok/sec : 65261.28143287211 | lr : 6.746012e-05\n",
            "step : 49 | loss : 6.0566182136535645 | dt : 125.42ms | tok/sec : 65314.12869414468 | lr : 6.332415e-05\n",
            "step : 49 | loss : 5.949124813079834 | dt : 125.51ms | tok/sec : 65271.44726289054 | lr : 6.083232e-05\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=8, T=1024)\n",
        "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
        "model.to(device)\n",
        "model=torch.compile(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i} | loss : {loss.item()} | dt : {dt:.2f}ms | tok/sec : {tokens_per_sec} | lr : {lr:.6e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a53cecd0",
      "metadata": {
        "id": "a53cecd0"
      },
      "source": [
        "All models use weight decay of 0.1 to provide a small amount of regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d7de61",
      "metadata": {
        "id": "80d7de61",
        "outputId": "4c27f251-564a-4b05-f3a5-5b2616b363fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step : 49 | loss : 6.1445417404174805 | dt : 136.97ms | tok/sec : 59810.67647504243 | lr : 0.0001\n",
            "step : 49 | loss : 6.166140556335449 | dt : 124.93ms | tok/sec : 65571.64437608182 | lr : 0.0001\n",
            "step : 49 | loss : 6.219522953033447 | dt : 122.36ms | tok/sec : 66950.77126083136 | lr : 0.0002\n",
            "step : 49 | loss : 6.533832550048828 | dt : 121.79ms | tok/sec : 67263.09617756339 | lr : 0.0002\n",
            "step : 49 | loss : 6.195084571838379 | dt : 121.84ms | tok/sec : 67233.3507509999 | lr : 0.0003\n",
            "step : 49 | loss : 6.20240592956543 | dt : 121.53ms | tok/sec : 67407.59021523118 | lr : 0.0004\n",
            "step : 49 | loss : 6.032421588897705 | dt : 121.75ms | tok/sec : 67284.43432080586 | lr : 0.0004\n",
            "step : 49 | loss : 6.130808353424072 | dt : 121.64ms | tok/sec : 67345.75796502933 | lr : 0.0005\n",
            "step : 49 | loss : 6.22491455078125 | dt : 121.91ms | tok/sec : 67199.16287511343 | lr : 0.0005\n",
            "step : 49 | loss : 6.185684680938721 | dt : 121.57ms | tok/sec : 67386.70253329647 | lr : 0.0006\n",
            "step : 49 | loss : 6.016080856323242 | dt : 122.29ms | tok/sec : 66989.53888386312 | lr : 0.0006\n",
            "step : 49 | loss : 6.137221813201904 | dt : 122.05ms | tok/sec : 67120.4003188048 | lr : 0.0006\n",
            "step : 49 | loss : 6.074485778808594 | dt : 122.31ms | tok/sec : 66977.65581877688 | lr : 0.0006\n",
            "step : 49 | loss : 6.243298530578613 | dt : 122.01ms | tok/sec : 67143.61606690963 | lr : 0.0006\n",
            "step : 49 | loss : 6.10523796081543 | dt : 122.21ms | tok/sec : 67032.01284464881 | lr : 0.0006\n",
            "step : 49 | loss : 6.073895454406738 | dt : 121.99ms | tok/sec : 67152.67062497801 | lr : 0.0006\n",
            "step : 49 | loss : 6.134830474853516 | dt : 122.22ms | tok/sec : 67026.25937175083 | lr : 0.0006\n",
            "step : 49 | loss : 6.494504928588867 | dt : 121.94ms | tok/sec : 67179.32356397237 | lr : 0.0006\n",
            "step : 49 | loss : 6.304076194763184 | dt : 122.28ms | tok/sec : 66991.75926456682 | lr : 0.0005\n",
            "step : 49 | loss : 6.5929856300354 | dt : 121.96ms | tok/sec : 67168.02958856337 | lr : 0.0005\n",
            "step : 49 | loss : 6.326352119445801 | dt : 122.21ms | tok/sec : 67034.36682280825 | lr : 0.0005\n",
            "step : 49 | loss : 6.248771667480469 | dt : 121.97ms | tok/sec : 67166.45398691457 | lr : 0.0005\n",
            "step : 49 | loss : 6.286187648773193 | dt : 122.24ms | tok/sec : 67015.01679874082 | lr : 0.0005\n",
            "step : 49 | loss : 6.171934127807617 | dt : 121.99ms | tok/sec : 67155.0330852461 | lr : 0.0005\n",
            "step : 49 | loss : 6.447182655334473 | dt : 122.35ms | tok/sec : 66955.33750345887 | lr : 0.0005\n",
            "step : 49 | loss : 6.31357479095459 | dt : 121.96ms | tok/sec : 67168.292196025 | lr : 0.0004\n",
            "step : 49 | loss : 6.2101898193359375 | dt : 122.19ms | tok/sec : 67043.78438898179 | lr : 0.0004\n",
            "step : 49 | loss : 6.266719818115234 | dt : 122.04ms | tok/sec : 67125.90768749878 | lr : 0.0004\n",
            "step : 49 | loss : 6.292474269866943 | dt : 122.19ms | tok/sec : 67044.8309485667 | lr : 0.0004\n",
            "step : 49 | loss : 6.045777320861816 | dt : 121.99ms | tok/sec : 67152.93311235498 | lr : 0.0004\n",
            "step : 49 | loss : 6.083186149597168 | dt : 122.22ms | tok/sec : 67028.0899150828 | lr : 0.0003\n",
            "step : 49 | loss : 6.349085807800293 | dt : 121.98ms | tok/sec : 67159.49596375393 | lr : 0.0003\n",
            "step : 49 | loss : 6.249893665313721 | dt : 122.15ms | tok/sec : 67063.28192556622 | lr : 0.0003\n",
            "step : 49 | loss : 6.328880310058594 | dt : 121.96ms | tok/sec : 67170.91838358484 | lr : 0.0003\n",
            "step : 49 | loss : 6.029127597808838 | dt : 122.19ms | tok/sec : 67043.39193756097 | lr : 0.0002\n",
            "step : 49 | loss : 5.948203086853027 | dt : 121.94ms | tok/sec : 67181.6878999697 | lr : 0.0002\n",
            "step : 49 | loss : 6.0153656005859375 | dt : 122.21ms | tok/sec : 67032.01284464881 | lr : 0.0002\n",
            "step : 49 | loss : 6.0948686599731445 | dt : 121.97ms | tok/sec : 67165.92880279143 | lr : 0.0002\n",
            "step : 49 | loss : 5.987940788269043 | dt : 122.20ms | tok/sec : 67040.12168772255 | lr : 0.0002\n",
            "step : 49 | loss : 5.892402648925781 | dt : 121.96ms | tok/sec : 67172.10023498684 | lr : 0.0002\n",
            "step : 49 | loss : 5.748286247253418 | dt : 122.27ms | tok/sec : 67000.77290932987 | lr : 0.0001\n",
            "step : 49 | loss : 5.917886734008789 | dt : 121.92ms | tok/sec : 67189.96438670113 | lr : 0.0001\n",
            "step : 49 | loss : 6.007781982421875 | dt : 122.18ms | tok/sec : 67050.9801460458 | lr : 0.0001\n",
            "step : 49 | loss : 6.107432842254639 | dt : 121.94ms | tok/sec : 67179.84895758996 | lr : 0.0001\n",
            "step : 49 | loss : 6.254033088684082 | dt : 122.21ms | tok/sec : 67032.143616876 | lr : 0.0001\n",
            "step : 49 | loss : 6.092156410217285 | dt : 121.95ms | tok/sec : 67173.93875316712 | lr : 0.0001\n",
            "step : 49 | loss : 6.118491172790527 | dt : 122.16ms | tok/sec : 67058.70094831406 | lr : 0.0001\n",
            "step : 49 | loss : 5.971887588500977 | dt : 121.94ms | tok/sec : 67179.19221685201 | lr : 0.0001\n",
            "step : 49 | loss : 5.883996963500977 | dt : 122.20ms | tok/sec : 67039.59847735445 | lr : 0.0001\n",
            "step : 49 | loss : 5.954689979553223 | dt : 121.96ms | tok/sec : 67171.1810136357 | lr : 0.0001\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "  print(f\"step : {i} | loss : {loss.item()} | dt : {dt:.2f}ms | tok/sec : {tokens_per_sec} | lr : {lr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a0f565",
      "metadata": {
        "id": "a8a0f565"
      },
      "source": [
        "In original GPT-3, they use batch size = 0.5M (in terms of tokens which is rougly 500 rows [0.5M/1024] ). But we can't do that else our small GPUs would explode\n",
        "\n",
        "Gradient Accumulation to Rescue!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bebbf6f",
      "metadata": {
        "id": "1bebbf6f",
        "outputId": "81815f3e-91af-4397-e0e2-94d81a20104f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total desired batch size : 524288\n",
            "calculated gradient accumulation steps : 32\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = 524288 # 2^19, ~0.5M in number of tokens\n",
        "B = 16 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T) == 0\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"total desired batch size : {total_batch_size}\")\n",
        "print(f\"calculated gradient accumulation steps : {grad_accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rolv4pqk4bYz",
        "outputId": "d9784375-dbdc-4787-cfa0-836d1c851c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 20 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step : 0 | loss : 10.971720 | tok/sec : 18023.857577590145 | lr : 0.0001 | dt : 29088.56ms\n",
            "step : 1 | loss : 9.763161 | tok/sec : 77665.71249227236 | lr : 0.0001 | dt : 6750.57ms\n",
            "step : 2 | loss : 9.382488 | tok/sec : 77690.94549170464 | lr : 0.0002 | dt : 6748.38ms\n",
            "step : 3 | loss : 9.651525 | tok/sec : 77468.55231203312 | lr : 0.0002 | dt : 6767.75ms\n",
            "step : 4 | loss : 9.089612 | tok/sec : 77395.60917608687 | lr : 0.0003 | dt : 6774.13ms\n",
            "step : 5 | loss : 8.640585 | tok/sec : 77361.57720653008 | lr : 0.0004 | dt : 6777.11ms\n",
            "step : 6 | loss : 8.344876 | tok/sec : 77302.65912190381 | lr : 0.0004 | dt : 6782.28ms\n",
            "step : 7 | loss : 8.052789 | tok/sec : 77265.79840663375 | lr : 0.0005 | dt : 6785.51ms\n",
            "step : 8 | loss : 7.710036 | tok/sec : 76974.8246172897 | lr : 0.0005 | dt : 6811.16ms\n",
            "step : 9 | loss : 7.386728 | tok/sec : 76941.83463684625 | lr : 0.0006 | dt : 6814.08ms\n",
            "step : 10 | loss : 7.075692 | tok/sec : 76944.07993478123 | lr : 0.0006 | dt : 6813.88ms\n",
            "step : 11 | loss : 6.810198 | tok/sec : 76918.09203706791 | lr : 0.0006 | dt : 6816.19ms\n",
            "step : 12 | loss : 6.652964 | tok/sec : 76899.03482248016 | lr : 0.0006 | dt : 6817.87ms\n",
            "step : 13 | loss : 6.511443 | tok/sec : 76887.35770008604 | lr : 0.0006 | dt : 6818.91ms\n",
            "step : 14 | loss : 6.426171 | tok/sec : 76870.40086535791 | lr : 0.0006 | dt : 6820.41ms\n",
            "step : 15 | loss : 6.390539 | tok/sec : 76849.83908366045 | lr : 0.0006 | dt : 6822.24ms\n",
            "step : 16 | loss : 6.344986 | tok/sec : 76836.96603597427 | lr : 0.0006 | dt : 6823.38ms\n",
            "step : 17 | loss : 6.349115 | tok/sec : 76824.27443758168 | lr : 0.0006 | dt : 6824.51ms\n",
            "step : 18 | loss : 6.319997 | tok/sec : 76815.35954035628 | lr : 0.0005 | dt : 6825.30ms\n",
            "step : 19 | loss : 6.296029 | tok/sec : 76799.82647935019 | lr : 0.0005 | dt : 6826.68ms\n",
            "step : 20 | loss : 6.277030 | tok/sec : 76770.25591986149 | lr : 0.0005 | dt : 6829.31ms\n",
            "step : 21 | loss : 6.488234 | tok/sec : 76722.6220295288 | lr : 0.0005 | dt : 6833.55ms\n",
            "step : 22 | loss : 6.298110 | tok/sec : 76779.20592328375 | lr : 0.0005 | dt : 6828.52ms\n",
            "step : 23 | loss : 6.254507 | tok/sec : 76791.15591579155 | lr : 0.0005 | dt : 6827.45ms\n",
            "step : 24 | loss : 6.219633 | tok/sec : 76787.59760902921 | lr : 0.0005 | dt : 6827.77ms\n",
            "step : 25 | loss : 6.202120 | tok/sec : 76752.29787813728 | lr : 0.0004 | dt : 6830.91ms\n",
            "step : 26 | loss : 6.182282 | tok/sec : 76744.61562476629 | lr : 0.0004 | dt : 6831.59ms\n",
            "step : 27 | loss : 6.183593 | tok/sec : 76768.83815631339 | lr : 0.0004 | dt : 6829.44ms\n",
            "step : 28 | loss : 6.160937 | tok/sec : 76761.77423983085 | lr : 0.0004 | dt : 6830.07ms\n",
            "step : 29 | loss : 6.157812 | tok/sec : 76769.68237593754 | lr : 0.0004 | dt : 6829.36ms\n",
            "step : 30 | loss : 6.157344 | tok/sec : 76769.99058772864 | lr : 0.0003 | dt : 6829.34ms\n",
            "step : 31 | loss : 6.139940 | tok/sec : 76742.2882162782 | lr : 0.0003 | dt : 6831.80ms\n",
            "step : 32 | loss : 6.147941 | tok/sec : 76750.0905460699 | lr : 0.0003 | dt : 6831.11ms\n",
            "step : 33 | loss : 6.118473 | tok/sec : 76745.66554913735 | lr : 0.0003 | dt : 6831.50ms\n",
            "step : 34 | loss : 6.105076 | tok/sec : 76746.35390800238 | lr : 0.0002 | dt : 6831.44ms\n",
            "step : 35 | loss : 6.098864 | tok/sec : 76745.17808067819 | lr : 0.0002 | dt : 6831.54ms\n",
            "step : 36 | loss : 6.077351 | tok/sec : 76750.26734226198 | lr : 0.0002 | dt : 6831.09ms\n",
            "step : 37 | loss : 6.082654 | tok/sec : 76742.40873454083 | lr : 0.0002 | dt : 6831.79ms\n",
            "step : 38 | loss : 6.052532 | tok/sec : 76710.2652023382 | lr : 0.0002 | dt : 6834.65ms\n",
            "step : 39 | loss : 6.043375 | tok/sec : 76718.51602742398 | lr : 0.0002 | dt : 6833.92ms\n",
            "step : 40 | loss : 6.041497 | tok/sec : 76712.32841124936 | lr : 0.0001 | dt : 6834.47ms\n",
            "step : 41 | loss : 6.020550 | tok/sec : 76728.2330272157 | lr : 0.0001 | dt : 6833.05ms\n",
            "step : 42 | loss : 6.029556 | tok/sec : 76739.36911176394 | lr : 0.0001 | dt : 6832.06ms\n",
            "step : 43 | loss : 6.003572 | tok/sec : 76744.73615033909 | lr : 0.0001 | dt : 6831.58ms\n",
            "step : 44 | loss : 5.997714 | tok/sec : 76718.61505880827 | lr : 0.0001 | dt : 6833.91ms\n",
            "step : 45 | loss : 5.998662 | tok/sec : 76707.96931043928 | lr : 0.0001 | dt : 6834.86ms\n",
            "step : 46 | loss : 5.981760 | tok/sec : 76706.60200888246 | lr : 0.0001 | dt : 6834.98ms\n",
            "step : 47 | loss : 5.995599 | tok/sec : 76706.36922462785 | lr : 0.0001 | dt : 6835.00ms\n",
            "step : 48 | loss : 5.972747 | tok/sec : 76712.90645076857 | lr : 0.0001 | dt : 6834.42ms\n",
            "step : 49 | loss : 5.968428 | tok/sec : 76685.74800688037 | lr : 0.0001 | dt : 6836.84ms\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=B, T=T)\n",
        "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "for step in range(max_steps):\n",
        "  loss_accum = 0.0\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  for micro_step in range(grad_accum_steps):\n",
        "      x, y = train_loader.next_batch()\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "        logits, loss = model(x, y)\n",
        "      # loss is scaled below to account for gradient accumulation as the gradients just add on each successful backward()\n",
        "      # addition of gradients correspons to SUM in the objective, but instead of SUM, we want MEAN.\n",
        "      # So loss is scaled here\n",
        "      loss = loss / grad_accum_steps\n",
        "      loss_accum += loss.detach() # detach the tensor from computational graph\n",
        "      loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "  # determine and set lr for this iteration\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize() # wait for gpu to finish work\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000\n",
        "  tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "  tokens_per_sec = tokens_processed / (t1 - t0)\n",
        "  print(f\"step : {step} | loss : {loss_accum.item():.6f} | tok/sec : {tokens_per_sec} | lr : {lr:.4f} | dt : {dt:.2f}ms\")\n"
      ],
      "id": "rolv4pqk4bYz"
    },
    {
      "cell_type": "markdown",
      "id": "986378b8",
      "metadata": {
        "id": "986378b8"
      },
      "source": [
        "# Distributed Data Parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5288b6ec",
      "metadata": {
        "id": "5288b6ec"
      },
      "source": [
        "DDP - we have 4 GPUs so we launch 4 processes and each process will be assigned a GPU. each GPU processes slightly different part of data and then we do average of gradients of all 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e6c936",
      "metadata": {
        "id": "a4e6c936"
      },
      "source": [
        "There will be 4 python interpreters running the same script in parallel, only difference is that each has unique ddp rank"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}