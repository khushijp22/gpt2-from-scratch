{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5p7sxxw-kp2"
      },
      "source": [
        "Refer paper Language Models are Unsupervised Multitask Learners\n",
        "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "\n",
        "\n",
        "GPT-2[124M] has 12 layers and $d_{model}$ = 768\n",
        "\n",
        "It is a decoder only model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MogKTW_5oPiT"
      },
      "source": [
        "Variables are named to follow the schema of Hugging Face Transformers code.  \n",
        "The following variables should be exactly the same:\n",
        "\n",
        "`transformer.wte.weight`  \n",
        "`transformer.wpe.weight`  \n",
        "`transformer.h.0.attn.c_attn.weight`  \n",
        "`transformer.h.0.attn.c_proj.weight`  \n",
        "`transformer.h.0.mlp.c_fc.weight`  \n",
        "`transformer.h.0.mlp.c_proj.weight`  \n",
        "`transformer.ln_f.weight`  \n",
        "`lm_head.weight`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AivZQshf94C1"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import tiktoken\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F70TvxkCHisq",
        "outputId": "92b0e308-5f38-4466-9309-ca6cbce7ecd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuz6GS5yn8Ta",
        "outputId": "aec42fa9-5d8f-4ec2-f5cd-9b442338bdae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_2.weight torch.Size([768])\n",
            "transformer.h.1.ln_2.bias torch.Size([768])\n",
            "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_1.weight torch.Size([768])\n",
            "transformer.h.2.ln_1.bias torch.Size([768])\n",
            "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_2.weight torch.Size([768])\n",
            "transformer.h.2.ln_2.bias torch.Size([768])\n",
            "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_1.weight torch.Size([768])\n",
            "transformer.h.3.ln_1.bias torch.Size([768])\n",
            "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_2.weight torch.Size([768])\n",
            "transformer.h.3.ln_2.bias torch.Size([768])\n",
            "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_1.weight torch.Size([768])\n",
            "transformer.h.4.ln_1.bias torch.Size([768])\n",
            "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_2.weight torch.Size([768])\n",
            "transformer.h.4.ln_2.bias torch.Size([768])\n",
            "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_1.weight torch.Size([768])\n",
            "transformer.h.5.ln_1.bias torch.Size([768])\n",
            "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_2.weight torch.Size([768])\n",
            "transformer.h.5.ln_2.bias torch.Size([768])\n",
            "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_1.weight torch.Size([768])\n",
            "transformer.h.6.ln_1.bias torch.Size([768])\n",
            "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_2.weight torch.Size([768])\n",
            "transformer.h.6.ln_2.bias torch.Size([768])\n",
            "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_1.weight torch.Size([768])\n",
            "transformer.h.7.ln_1.bias torch.Size([768])\n",
            "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_2.weight torch.Size([768])\n",
            "transformer.h.7.ln_2.bias torch.Size([768])\n",
            "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_1.weight torch.Size([768])\n",
            "transformer.h.8.ln_1.bias torch.Size([768])\n",
            "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_2.weight torch.Size([768])\n",
            "transformer.h.8.ln_2.bias torch.Size([768])\n",
            "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_1.weight torch.Size([768])\n",
            "transformer.h.9.ln_1.bias torch.Size([768])\n",
            "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_2.weight torch.Size([768])\n",
            "transformer.h.9.ln_2.bias torch.Size([768])\n",
            "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_1.weight torch.Size([768])\n",
            "transformer.h.10.ln_1.bias torch.Size([768])\n",
            "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_2.weight torch.Size([768])\n",
            "transformer.h.10.ln_2.bias torch.Size([768])\n",
            "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_1.weight torch.Size([768])\n",
            "transformer.h.11.ln_1.bias torch.Size([768])\n",
            "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_2.weight torch.Size([768])\n",
            "transformer.h.11.ln_2.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\") #124M\n",
        "sd = model_gpt2.state_dict()\n",
        "for k,v in sd.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faOSvk3OphNX",
        "outputId": "e58ff269-8e86-4385-8a80-b486ab21ef09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, and my goal is to make it easier for people to learn.\\n\\nWhy not just start using it?\\n\\nWell, it is possible to\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a programming language. I'm a programming language that you write for your own purposes. And what happened is that I decided, because I was a\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a compiler. I'll be playing and I'll be running on Java. I'll be writing programs on Java that are written in a way that\"},\n",
              " {'generated_text': \"Hello, I'm a language model, I can't even describe it.\\n\\nBut I can say that the way we deal with the language is that a language is a universal language,\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and this is where I'm going to make your code.\\n\\nI'm going to make sure that you run into some errors, since this is\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model = 'gpt2')\n",
        "generator(\"Hello, I'm a language model,\", max_new_tokens=30, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqbFyFaogMXJ"
      },
      "source": [
        "#GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HL9C-49Ml4Ps"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # make sure hidden dim is a multiple of no. of heads\n",
        "    assert config.n_embed % config.n_head == 0\n",
        "\n",
        "    # a single linear layer to compute Q, K, V simultaneously\n",
        "    self.c_attn=nn.Linear(config.n_embed, 3 * config.n_embed)\n",
        "\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1 # flag for weight initialization of c_proj, use std = 0.02/sqroot(num layers)\n",
        "\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embed = config.n_embed\n",
        "\n",
        "    # not really a bias, more of a mask, but following OpenAI naming convention\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                         .view(1, 1,config.block_size, config.block_size ))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()  # Batch size, sequence length, n_embed\n",
        "    qkv= self.c_attn(x)\n",
        "    q,k,v = qkv.split(self.n_embed, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # explanation : C = n_head * head_size\n",
        "    # k.shape = (B, T, n_head, head_size)\n",
        "    # k = k.transpose(1, 2)\n",
        "    # Before transpose: (B, T, n_head, head_size)\n",
        "    # After transpose:  (B, n_head, T, head_size)\n",
        "\n",
        "    # similarly for q and v\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # Attention\n",
        "    # att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size[-1]))  # q : (B, n_head, T, head_size) @ K.T : (B, n_head, head_size, T) = (B, n_head, T, T)\n",
        "    # att = att.masked_fill(self.bias[:, :, :T, :T] == 0.0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, hs), basically a weighted sum of values\n",
        "\n",
        "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.c_proj(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aOrOMwsGHEl6"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)         # ffn. increasing hidden dim size increases capacity of model to learn, 4*embed dim is just design choice\n",
        "    self.gelu = nn.GELU(approximate='tanh')                           # activation\n",
        "    self.c_proj = nn.Linear( 4 * config.n_embed, config.n_embed)      # projection\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FKP1mRRXFbTU"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embed)  # layer norm 1\n",
        "    self.attn = CausalSelfAttention(config)   # causal attention\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embed)  # layer norm 2\n",
        "    self.mlp = MLP(config)                    # fnn\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nzLYZmAICaTG"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size : int = 1024    # max sequence length\n",
        "  vocab_size : int = 50257   # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "  n_layer : int = 12\n",
        "  n_head : int = 12\n",
        "  n_embed : int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config=config\n",
        "\n",
        "    self.transformer=nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embed),  # weights for token embeddings\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embed),  # weights for positional embeddings\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # block for each layer\n",
        "        ln_f = nn.LayerNorm(config.n_embed),  # final layer normalisation\n",
        "        ))\n",
        "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size,bias=False) # last second, linear layer\n",
        "\n",
        "    # weight-sharing scheme\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    # initialize parameters\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):      # will be true only for output projection, `c_proj` layer\n",
        "        std *= (2 * self.config.n_layer) ** -0.05    # scale std by 1/sqrt(no_of_layers) acc to GPT paper\n",
        "        # we are doing 2 * no of layers bcoz every layer has 2 blocks that add to residual stream - attention and then mlp\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std = std) # inititalise weights according to gpt2 official code, i.e., mean 0, std 0.02 for weights\n",
        "        # sqroot n thing is done to control the growth of activations in residual stream in forward pass as each residual stream adds its data so we scale down every contribution to residual stream\n",
        "        torch.nn.init.zeros_(module.bias) # and normal initialisation for bias\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx (B, T) Batch size, B sequences, each of length T stacked up, T<=block_size\n",
        "    B, T = idx.size()\n",
        "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "    # forward the token and posisition embeddings\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T), arange iterate from 0 to T\n",
        "    pos_emb = self.transformer.wpe(pos) # shape (T, n_embd) # identical for every single row (batch)\n",
        "    tok_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
        "    x = tok_emb + pos_emb # internal broadcasting\n",
        "    # forward the blocks of transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "    # forward the final layernorm\n",
        "    x = self.transformer.ln_f(x)\n",
        "    # forward the final classifier\n",
        "    logits=self.lm_head(x) # (B, T, vocab_size)\n",
        "    loss=None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # logits - (B*T, vocab_size)\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "      # Loads pretrained GPT-2 model weights from huggingface\n",
        "      assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "      print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "      # n_layer, n_head and n_embed are determined from model_type\n",
        "      config_args = {\n",
        "          'gpt2':         dict(n_layer=12, n_head=12, n_embed=768),  # 124M params\n",
        "          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embed=1024), # 350M params\n",
        "          'gpt2-large':   dict(n_layer=36, n_head=20, n_embed=1280), # 774M params\n",
        "          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params\n",
        "      }[model_type]\n",
        "      config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "      config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "      # create a from-scratch initialized minGPT model\n",
        "      config = GPTConfig(**config_args)\n",
        "      model = GPT(config)\n",
        "      sd = model.state_dict()\n",
        "      sd_keys = sd.keys()\n",
        "      sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "      # init a huggingface/transformers model\n",
        "      model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "      sd_hf = model_hf.state_dict()\n",
        "\n",
        "      # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "      sd_keys_hf = sd_hf.keys()\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "      transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "      # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "      # this means that we have to transpose these weights when we import them\n",
        "      assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "      for k in sd_keys_hf:\n",
        "          if any(k.endswith(w) for w in transposed):\n",
        "              # special treatment for the Conv1D weights we need to transpose\n",
        "              assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k].t())\n",
        "          else:\n",
        "              # vanilla copy over the other parameters\n",
        "              assert sd_hf[k].shape == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k])\n",
        "\n",
        "      return model\n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "      # start with all of the candidate parameters (that require grad)\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "      # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "      # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "      num_decay_params = sum(p.numel() for p in decay_params)\n",
        "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "      if master_process:\n",
        "          print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "          print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "      # Create AdamW optimizer and use the fused version if it is available\n",
        "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "      use_fused = fused_available and device_type == \"cuda\"\n",
        "      if master_process:\n",
        "          print(f\"using fused AdamW: {use_fused}\")\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "      return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FaffDrldGX9z"
      },
      "outputs": [],
      "source": [
        "# model=GPT.from_pretrained(\"gpt2\")\n",
        "model = GPT(GPTConfig()) # random initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdfSBmI0qwZh",
        "outputId": "11df5fb4-86fe-417b-ac35-ab64f12920da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.eval() #put model into eval mode when not training anything and just using the model\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x22uhgiQhQEA"
      },
      "outputs": [],
      "source": [
        "num_return_sequences=5\n",
        "max_length=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ApxaK6ailQNh"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\") # (8, )\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
        "x = tokens.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co7vykMnloJD",
        "outputId": "6a60cc26-5ab3-4390-8dbe-caaaf131f5f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokens.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD3NVdO2wmzR"
      },
      "source": [
        "##Generate before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe-yLAxVl00o",
        "outputId": "7825900a-05c7-4eb6-ed16-37691b67b3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, gl Robot Cherokeeemptionotechfulness fingerprintsprev PRESIDENT balloons lick histories Side159 altercationVO publishersfulness blink wait048 start\n",
            "> Hello, I'm a language model, Lay jam creeps actresses actresses402 1840 elev freelance creeps leveraging Ether Round markupintensity Cherokeeogether Nasa reactiveintensity start Nope\n",
            "> Hello, I'm a language model, Cherokeewindows Nasafemin wait Publicoptfulness tile Robot PRESIDENT Ratsintensity creeps β Baird start Rats balloonsSpec billionaires histories\n",
            "> Hello, I'm a language model, tenets ® actresses lobby Robot Rats Cherokee Tinaevaluate shore start225387 Nasa grap Nasaicidesintensity fingerprints CherokeefulnessJuly\n",
            "> Hello, I'm a language model, Publicclose comprehension start actressesviron cloakilon creeps billionaires tra congreg creeps thinking maneuvers histories Kang creepsdem pass indoorCop\n"
          ]
        }
      ],
      "source": [
        "# generate.  right now x is (B, T) where B=5 and T=8\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length: # add a column of new indices, i.e. add new token for each of the 5 sequences\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(x) # (B, T, vocab_size)\n",
        "\n",
        "        # get the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # ensures no sampling of very rare tokens\n",
        "\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "  tokens=x[i, :max_length].tolist()\n",
        "  decoded=enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16m5rFx4Qo6n"
      },
      "source": [
        "##Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvwpavxnEz8j"
      },
      "source": [
        "We want to feed the token sequences to a transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DotEemCt_MV",
        "outputId": "9ca02689-21ac-421f-c6fc-a5eb14f7b1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n",
            "tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
            "          3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
            "           461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
            "          1639,   389],\n",
            "        [  477, 12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,\n",
            "           198,   198,  3237,    25,   198,  4965,  5634,    13, 12939,    13,\n",
            "           198,   198,  5962, 22307,    25,   198,  5962,    11,   345,   760,\n",
            "           327,  1872],\n",
            "        [  385,  1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,\n",
            "           198,   198,  3237,    25,   198,  1135,   760,   470,    11,   356,\n",
            "           760,   470,    13,   198,   198,  5962, 22307,    25,   198,  5756,\n",
            "           514,  1494],\n",
            "        [  683,    11,   290,   356,  1183,   423, 11676,   379,   674,   898,\n",
            "          2756,    13,   198,  3792,   470,   257, 15593,    30,   198,   198,\n",
            "          3237,    25,   198,  2949,   517,  3375,   319,   470,    26,  1309,\n",
            "           340,   307]], device='cuda:0')\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,  3285,\n",
            "           502,  2740,    13,   198,   198,  3237,    25,   198,  5248,   461,\n",
            "            11,  2740,    13,   198,   198,  5962, 22307,    25,   198,  1639,\n",
            "           389,   477],\n",
            "        [12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,   198,\n",
            "           198,  3237,    25,   198,  4965,  5634,    13, 12939,    13,   198,\n",
            "           198,  5962, 22307,    25,   198,  5962,    11,   345,   760,   327,\n",
            "          1872,   385],\n",
            "        [ 1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,   198,\n",
            "           198,  3237,    25,   198,  1135,   760,   470,    11,   356,   760,\n",
            "           470,    13,   198,   198,  5962, 22307,    25,   198,  5756,   514,\n",
            "          1494,   683],\n",
            "        [   11,   290,   356,  1183,   423, 11676,   379,   674,   898,  2756,\n",
            "            13,   198,  3792,   470,   257, 15593,    30,   198,   198,  3237,\n",
            "            25,   198,  2949,   517,  3375,   319,   470,    26,  1309,   340,\n",
            "           307,  1760]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "with open(\"input.txt\", 'r') as f:\n",
        "  text = f.read()\n",
        "data = text[:1000]\n",
        "print(data)\n",
        "tokens = enc.encode(data) # encode data\n",
        "B, T = 4, 32\n",
        "buf = torch.tensor(tokens[:B*T+1]) # take only B*T tokens to manage size and an additional token which will be used in output y as target to nth token\n",
        "buf = buf.to(device) # it doesn't just move the data to gpu, it creates a new memory on gpu\n",
        "x = buf[:-1].view(B, T) # all tokens except last\n",
        "y = buf[1:].view(B, T) # targets will be from 1st token\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWn98U7NG9gO"
      },
      "source": [
        "Its like for 0th token, target is 1st token, for 0 and 1st token, target is 2nd token and so on (masked prediction) and so our x goes from 0 to B*T and y goes from 1 to B*T+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nXg3TlzFFa5",
        "outputId": "6f0f5abf-7924-4d9c-c805-0c232c314437"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model.to(device)\n",
        "logits, loss = model(x, y)\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXc02G3tIXLj",
        "outputId": "d6c9121a-8ec0-44fe-90e1-b2b51f5f9592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7661, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSEjRSFfKyUT"
      },
      "source": [
        "At inititalization, we expect loss to be roughly around -ln(1/vocab_size)[NLL]1\n",
        " = 10.8 here,  since at initialization, probability of any word is same i.e., 1/50257."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF294-9NJyWQ",
        "outputId": "836dc00e-6252-4863-b108-9f48d61f25b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 0, loss : 10.766117095947266\n",
            "step : 1, loss : 8.459126472473145\n",
            "step : 2, loss : 7.78156852722168\n",
            "step : 3, loss : 7.601080894470215\n",
            "step : 4, loss : 7.055751800537109\n",
            "step : 5, loss : 6.523852348327637\n",
            "step : 6, loss : 6.2293901443481445\n",
            "step : 7, loss : 5.854854583740234\n",
            "step : 8, loss : 5.603376388549805\n",
            "step : 9, loss : 5.260631561279297\n",
            "step : 10, loss : 4.928299903869629\n",
            "step : 11, loss : 4.6097612380981445\n",
            "step : 12, loss : 4.324857234954834\n",
            "step : 13, loss : 4.081106662750244\n",
            "step : 14, loss : 3.7766263484954834\n",
            "step : 15, loss : 3.46846866607666\n",
            "step : 16, loss : 3.1800835132598877\n",
            "step : 17, loss : 2.9007139205932617\n",
            "step : 18, loss : 2.6462717056274414\n",
            "step : 19, loss : 2.406736135482788\n",
            "step : 20, loss : 2.197084426879883\n",
            "step : 21, loss : 1.9773025512695312\n",
            "step : 22, loss : 1.7539914846420288\n",
            "step : 23, loss : 1.547154188156128\n",
            "step : 24, loss : 1.3641879558563232\n",
            "step : 25, loss : 1.186747670173645\n",
            "step : 26, loss : 1.0240534543991089\n",
            "step : 27, loss : 0.8775370121002197\n",
            "step : 28, loss : 0.7375009655952454\n",
            "step : 29, loss : 0.6299567222595215\n",
            "step : 30, loss : 0.5211267471313477\n",
            "step : 31, loss : 0.4179726243019104\n",
            "step : 32, loss : 0.33982086181640625\n",
            "step : 33, loss : 0.27395427227020264\n",
            "step : 34, loss : 0.22059324383735657\n",
            "step : 35, loss : 0.1782337725162506\n",
            "step : 36, loss : 0.144699364900589\n",
            "step : 37, loss : 0.1159713864326477\n",
            "step : 38, loss : 0.09323643147945404\n",
            "step : 39, loss : 0.07538419961929321\n",
            "step : 40, loss : 0.060536324977874756\n",
            "step : 41, loss : 0.04840730130672455\n",
            "step : 42, loss : 0.03902889043092728\n",
            "step : 43, loss : 0.03189966082572937\n",
            "step : 44, loss : 0.026623181998729706\n",
            "step : 45, loss : 0.02266707643866539\n",
            "step : 46, loss : 0.0194612517952919\n",
            "step : 47, loss : 0.01680280640721321\n",
            "step : 48, loss : 0.014635801315307617\n",
            "step : 49, loss : 0.012877365574240685\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step : {i}, loss : {loss.item()}\")\n",
        "# Here we overfit a single batch, now lets move to training on all batches!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AljICr1YKR0s"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B=B\n",
        "    self.T=T\n",
        "\n",
        "    # at init, load tokens from disk and store them in memory\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"Loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "    self.current_size = 0 # state\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_size:self.current_size + B*T + 1] # +1 coz we need it in 'y'\n",
        "    # buf = buf.to(device) dont do this here to save space on gpu\n",
        "    x = buf[:-1].view(B, T) # inputs\n",
        "    y = buf[1:].view(B, T) # targets\n",
        "    self.current_size += B*T # advance position in tensor\n",
        "    # if loading next batch would be out of bounds, reset\n",
        "    if self.current_size + B*T + 1 > len(self.tokens):\n",
        "      self.current_size = 0\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtoGootE9Kjp",
        "outputId": "2cbbd451-2e53-45c5-d88b-ecd1f6ca363e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 2640 batches\n",
            "step : 0, loss : 10.940428733825684\n",
            "step : 1, loss : 9.843118667602539\n",
            "step : 2, loss : 8.90627670288086\n",
            "step : 3, loss : 9.092300415039062\n",
            "step : 4, loss : 8.670042037963867\n",
            "step : 5, loss : 8.367634773254395\n",
            "step : 6, loss : 9.062261581420898\n",
            "step : 7, loss : 8.763618469238281\n",
            "step : 8, loss : 8.239639282226562\n",
            "step : 9, loss : 8.00062370300293\n",
            "step : 10, loss : 8.376836776733398\n",
            "step : 11, loss : 7.392353057861328\n",
            "step : 12, loss : 7.815987586975098\n",
            "step : 13, loss : 7.484529972076416\n",
            "step : 14, loss : 7.460954666137695\n",
            "step : 15, loss : 7.309003829956055\n",
            "step : 16, loss : 7.421919822692871\n",
            "step : 17, loss : 8.24163818359375\n",
            "step : 18, loss : 7.139786720275879\n",
            "step : 19, loss : 7.7653961181640625\n",
            "step : 20, loss : 7.4729461669921875\n",
            "step : 21, loss : 7.796375274658203\n",
            "step : 22, loss : 6.474678039550781\n",
            "step : 23, loss : 6.83241081237793\n",
            "step : 24, loss : 6.8760786056518555\n",
            "step : 25, loss : 6.69979190826416\n",
            "step : 26, loss : 6.780343055725098\n",
            "step : 27, loss : 7.570000648498535\n",
            "step : 28, loss : 7.17165470123291\n",
            "step : 29, loss : 6.940304279327393\n",
            "step : 30, loss : 6.933614730834961\n",
            "step : 31, loss : 7.2847981452941895\n",
            "step : 32, loss : 7.116950988769531\n",
            "step : 33, loss : 7.043407917022705\n",
            "step : 34, loss : 7.936230659484863\n",
            "step : 35, loss : 7.758362770080566\n",
            "step : 36, loss : 7.691401958465576\n",
            "step : 37, loss : 7.644436359405518\n",
            "step : 38, loss : 8.028663635253906\n",
            "step : 39, loss : 7.48277473449707\n",
            "step : 40, loss : 7.429481506347656\n",
            "step : 41, loss : 6.942080497741699\n",
            "step : 42, loss : 7.131695747375488\n",
            "step : 43, loss : 7.081212997436523\n",
            "step : 44, loss : 7.002164840698242\n",
            "step : 45, loss : 7.077507495880127\n",
            "step : 46, loss : 6.14487361907959\n",
            "step : 47, loss : 6.350444316864014\n",
            "step : 48, loss : 6.980921745300293\n",
            "step : 49, loss : 6.831765174865723\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoaderLite(B=4, T=32)\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step : {i}, loss : {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXYsxX-JFKxl"
      },
      "source": [
        "# Optimization"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}