{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed34ffa",
   "metadata": {},
   "source": [
    "# Optimization!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1447ccba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import tiktoken\n",
    "import math\n",
    "import inspect\n",
    "import os\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3269586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "  def __init__(self, B, T):\n",
    "    self.B=B\n",
    "    self.T=T\n",
    "\n",
    "    # at init, load tokens from disk and store them in memory\n",
    "    with open('input.txt', 'r') as f:\n",
    "      text = f.read()\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    tokens = enc.encode(text)\n",
    "    self.tokens = torch.tensor(tokens)\n",
    "    print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
    "    self.current_size = 0 # state\n",
    "\n",
    "  def next_batch(self):\n",
    "    B, T = self.B, self.T\n",
    "    buf = self.tokens[self.current_size:self.current_size + B*T + 1] # +1 coz we need it in 'y'\n",
    "    # buf = buf.to(device) dont do this here to save space on gpu\n",
    "    x = buf[:-1].view(B, T) # inputs\n",
    "    y = buf[1:].view(B, T) # targets\n",
    "    self.current_size += B*T # advance position in tensor\n",
    "    # if loading next batch would be out of bounds, reset\n",
    "    if self.current_size + B*T + 1 > len(self.tokens):\n",
    "      self.current_size = 0\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c6e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # make sure hidden dim is a multiple of no. of heads\n",
    "    assert config.n_embed % config.n_head == 0\n",
    "\n",
    "    # a single linear layer to compute Q, K, V simultaneously\n",
    "    self.c_attn=nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1 # flag for weight initialization of c_proj, use std = 0.02/sqroot(num layers)\n",
    "\n",
    "    # regularization\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embed = config.n_embed\n",
    "\n",
    "    # not really a bias, more of a mask, but following OpenAI naming convention\n",
    "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                         .view(1, 1,config.block_size, config.block_size ))\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.size()  # Batch size, sequence length, n_embed\n",
    "    qkv= self.c_attn(x)\n",
    "    q,k,v = qkv.split(self.n_embed, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # explanation : C = n_head * head_size\n",
    "    # k.shape = (B, T, n_head, head_size)\n",
    "    # k = k.transpose(1, 2)\n",
    "    # Before transpose: (B, T, n_head, head_size)\n",
    "    # After transpose:  (B, n_head, T, head_size)\n",
    "\n",
    "    # similar for q and v\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # Attention\n",
    "    # att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size[-1]))\n",
    "    # att = att.masked_fill(self.bias[:, :, :T, :T] == 0.0, float('-inf'))\n",
    "    # att = F.softmax(att, dim=-1)\n",
    "    # y = att @ v # (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, hs), basically a weighted sum of values\n",
    "                                                                \n",
    "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "\n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "    # output projection\n",
    "    y = self.c_proj(y)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281721e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)         # ffn. increasing hidden dim size increases capacity of model to learn, 4*embed dim is just design choice\n",
    "    self.gelu = nn.GELU(approximate='tanh')                            # activation\n",
    "    self.c_proj = nn.Linear( 4 * config.n_embed, config.n_embed) # projection\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(config.n_embed)  # layer norm 1\n",
    "    self.attn = CausalSelfAttention(config) # causal attention\n",
    "    self.ln_2 = nn.LayerNorm(config.n_embed) # layer norm 2\n",
    "    self.mlp = MLP(config) # fnn\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.attn(self.ln_1(x))\n",
    "    x = x + self.mlp(self.ln_2(x))\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "  block_size : int = 1024    # max sequence length\n",
    "  vocab_size : int = 50257   # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "  n_layer : int = 12\n",
    "  n_head : int = 12\n",
    "  n_embed : int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config=config\n",
    "\n",
    "    self.transformer=nn.ModuleDict(dict(\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embed),  # weights for token embeddings\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embed),  # weights for positional embeddings\n",
    "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # block for each layer\n",
    "        ln_f = nn.LayerNorm(config.n_embed),  # final layer normalisation\n",
    "        ))\n",
    "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size,bias=False) # last second, linear layer\n",
    "\n",
    "    # weight-sharing scheme\n",
    "    self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    # initialize parameters\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      std = 0.02\n",
    "      if hasattr(module, 'NANOGPT_SCALE_INIT'): # will be true only for output projection, `c_proj` layer\n",
    "        std *= (2 * self.config.n_layer) ** -0.05 # scale std by 1/sqrt(no_of_layers) acc to GPT paper\n",
    "        # we are doing 2 * no of layers bcoz every layer has 2 blocks that add to residual stream - attention and then mlp\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std = std) # inititalise weights according to gpt2 official code, i.e., mean 0, std 0.02 for weights\n",
    "      # sqroot n thing is done to control the growth of activations in residual stream in forward pass as each residual stream adds its data so we scale down every contribution to residual stream\n",
    "        torch.nn.init.zeros_(module.bias) # and normal initialisation for bias\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx (B, T) Batch size, B sequences, each of length T stacked up, T<=block_size\n",
    "    B, T = idx.size()\n",
    "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "    # forward the token and posisition embeddings\n",
    "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T), arange iterate from 0 to T\n",
    "    pos_emb = self.transformer.wpe(pos) # shape (T, n_embd) # identical for every single row (batch)\n",
    "    tok_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
    "    x = tok_emb + pos_emb # internal broadcasting\n",
    "    # forward the blocks of transformer\n",
    "    for block in self.transformer.h:\n",
    "      x = block(x)\n",
    "    # forward the final layernorm\n",
    "    x = self.transformer.ln_f(x)\n",
    "    # forward the final classifier\n",
    "    logits=self.lm_head(x) # (B, T, vocab_size)\n",
    "    loss=None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # logits - (B*T, vocab_size)\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model_type):\n",
    "      # Loads pretrained GPT-2 model weights from huggingface\n",
    "      assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "      print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "      # n_layer, n_head and n_embed are determined from model_type\n",
    "      config_args = {\n",
    "          'gpt2':         dict(n_layer=12, n_head=12, n_embed=768),  # 124M params\n",
    "          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embed=1024), # 350M params\n",
    "          'gpt2-large':   dict(n_layer=36, n_head=20, n_embed=1280), # 774M params\n",
    "          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params\n",
    "      }[model_type]\n",
    "      config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "      config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "      # create a from-scratch initialized minGPT model\n",
    "      config = GPTConfig(**config_args)\n",
    "      model = GPT(config)\n",
    "      sd = model.state_dict()\n",
    "      sd_keys = sd.keys()\n",
    "      sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "      # init a huggingface/transformers model\n",
    "      model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "      sd_hf = model_hf.state_dict()\n",
    "\n",
    "      # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "      sd_keys_hf = sd_hf.keys()\n",
    "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "      transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "      # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "      # this means that we have to transpose these weights when we import them\n",
    "      assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "      for k in sd_keys_hf:\n",
    "          if any(k.endswith(w) for w in transposed):\n",
    "              # special treatment for the Conv1D weights we need to transpose\n",
    "              assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "              with torch.no_grad():\n",
    "                  sd[k].copy_(sd_hf[k].t())\n",
    "          else:\n",
    "              # vanilla copy over the other parameters\n",
    "              assert sd_hf[k].shape == sd[k].shape\n",
    "              with torch.no_grad():\n",
    "                  sd[k].copy_(sd_hf[k])\n",
    "\n",
    "      return model\n",
    "\n",
    "  def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "      # start with all of the candidate parameters (that require grad)\n",
    "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "      # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "      # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "      optim_groups = [\n",
    "          {'params': decay_params, 'weight_decay': weight_decay},\n",
    "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "      ]\n",
    "      num_decay_params = sum(p.numel() for p in decay_params)\n",
    "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "      # if master_process:\n",
    "      print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "      print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "      # Create AdamW optimizer and use the fused version if it is available\n",
    "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "      use_fused = fused_available and device_type == \"cuda\"\n",
    "      # if master_process:\n",
    "      print(f\"using fused AdamW: {use_fused}\")\n",
    "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "      return optimizer\n",
    "      # fused means to fuse all weight updates into single kernel \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2b34b",
   "metadata": {},
   "source": [
    "By default, aa numbers, i.e., weights, bias, logits etc is in `float32`, i.e. float in 32 bits.\n",
    "Deep learning computational workflows can tolerate much lower precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01186ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step : 0, loss : 10.88249397277832, dt : 301.96ms, tok/sec : 27129.22909924115\n",
      "step : 1, loss : 9.567190170288086, dt : 282.20ms, tok/sec : 29029.488784782214\n",
      "step : 2, loss : 8.912208557128906, dt : 281.72ms, tok/sec : 29078.427187013116\n",
      "step : 3, loss : 8.75865650177002, dt : 281.69ms, tok/sec : 29081.282098201704\n",
      "step : 4, loss : 8.458645820617676, dt : 282.02ms, tok/sec : 29047.45301775408\n",
      "step : 5, loss : 8.458444595336914, dt : 281.62ms, tok/sec : 29088.741968364484\n",
      "step : 6, loss : 8.408073425292969, dt : 281.84ms, tok/sec : 29066.49677356045\n",
      "step : 7, loss : 8.067669868469238, dt : 282.12ms, tok/sec : 29037.4374249445\n",
      "step : 8, loss : 7.748229026794434, dt : 282.02ms, tok/sec : 29047.232011037355\n",
      "step : 9, loss : 7.719016075134277, dt : 281.68ms, tok/sec : 29082.31591117807\n",
      "step : 10, loss : 7.7164177894592285, dt : 281.57ms, tok/sec : 29093.86515110974\n",
      "step : 11, loss : 7.545544147491455, dt : 282.24ms, tok/sec : 29025.17282864965\n",
      "step : 12, loss : 7.47494649887085, dt : 282.99ms, tok/sec : 28948.338590299536\n",
      "step : 13, loss : 7.258368492126465, dt : 283.02ms, tok/sec : 28945.02203999936\n",
      "step : 14, loss : 7.168115615844727, dt : 283.10ms, tok/sec : 28936.417186139384\n",
      "step : 15, loss : 6.9864501953125, dt : 283.53ms, tok/sec : 28893.08080117524\n",
      "step : 16, loss : 6.925849914550781, dt : 283.18ms, tok/sec : 28928.572460058735\n",
      "step : 17, loss : 6.890615463256836, dt : 282.96ms, tok/sec : 28951.02165023765\n",
      "step : 18, loss : 6.754840850830078, dt : 283.12ms, tok/sec : 28934.419057114756\n",
      "step : 19, loss : 6.613775730133057, dt : 283.08ms, tok/sec : 28939.049289404487\n",
      "step : 20, loss : 6.655529975891113, dt : 283.18ms, tok/sec : 28928.596815977868\n",
      "step : 21, loss : 6.5382795333862305, dt : 283.19ms, tok/sec : 28927.598256922134\n",
      "step : 22, loss : 6.625465393066406, dt : 283.28ms, tok/sec : 28917.95718493831\n",
      "step : 23, loss : 6.481101989746094, dt : 283.24ms, tok/sec : 28922.48476675808\n",
      "step : 24, loss : 6.458992004394531, dt : 282.97ms, tok/sec : 28949.655666668634\n",
      "step : 25, loss : 6.505716323852539, dt : 282.87ms, tok/sec : 28960.294230738433\n",
      "step : 26, loss : 6.631634712219238, dt : 283.15ms, tok/sec : 28931.49546321058\n",
      "step : 27, loss : 6.518127918243408, dt : 283.15ms, tok/sec : 28931.909603092605\n",
      "step : 28, loss : 6.747167587280273, dt : 283.18ms, tok/sec : 28928.889090206452\n",
      "step : 29, loss : 6.553497791290283, dt : 283.74ms, tok/sec : 28871.327820626688\n",
      "step : 30, loss : 6.5414934158325195, dt : 283.00ms, tok/sec : 28946.99724681294\n",
      "step : 31, loss : 6.5320611000061035, dt : 283.13ms, tok/sec : 28933.39573203901\n",
      "step : 32, loss : 6.369753837585449, dt : 283.89ms, tok/sec : 28856.585514320483\n",
      "step : 33, loss : 6.7029924392700195, dt : 283.16ms, tok/sec : 28930.64285960869\n",
      "step : 34, loss : 6.56422233581543, dt : 283.04ms, tok/sec : 28942.657023533357\n",
      "step : 35, loss : 6.4737701416015625, dt : 282.92ms, tok/sec : 28954.80316988408\n",
      "step : 36, loss : 6.519139289855957, dt : 283.20ms, tok/sec : 28926.185778916726\n",
      "step : 37, loss : 6.508028984069824, dt : 283.21ms, tok/sec : 28925.06563581752\n",
      "step : 38, loss : 6.271595001220703, dt : 283.22ms, tok/sec : 28924.16471410232\n",
      "step : 39, loss : 6.282309532165527, dt : 283.16ms, tok/sec : 28930.35054994186\n",
      "step : 40, loss : 6.557649612426758, dt : 283.08ms, tok/sec : 28938.48870876592\n",
      "step : 41, loss : 6.368338584899902, dt : 283.13ms, tok/sec : 28933.785561627916\n",
      "step : 42, loss : 6.394419193267822, dt : 283.25ms, tok/sec : 28921.803104656312\n",
      "step : 43, loss : 6.134553909301758, dt : 282.98ms, tok/sec : 28949.411754595418\n",
      "step : 44, loss : 6.072843074798584, dt : 283.30ms, tok/sec : 28916.83767902739\n",
      "step : 45, loss : 6.147922515869141, dt : 283.16ms, tok/sec : 28930.813376308884\n",
      "step : 46, loss : 6.261806488037109, dt : 283.18ms, tok/sec : 28928.134060527278\n",
      "step : 47, loss : 6.238415241241455, dt : 283.06ms, tok/sec : 28941.26745186035\n",
      "step : 48, loss : 6.131135940551758, dt : 283.14ms, tok/sec : 28932.518653869796\n",
      "step : 49, loss : 5.9885454177856445, dt : 282.99ms, tok/sec : 28947.558157148695\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n",
    "\n",
    "  # first iteration is usually slower since all initializations and everything is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e6ffb",
   "metadata": {},
   "source": [
    "## TF32 (TensorFloat-32)\n",
    "FP32 - sign : 1, exponent : 8, mantissa : 23\n",
    "TF32 - sign : 1, exponent : 8, mantissa : 10\n",
    "13 mantissa bits get cropped which makes matrix multiplication faster. Speedup is achieved at the cost of precision.\n",
    "inputs are fp32, outputs are fp32, but internally, numbers get truncated to perform operations faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "torch.set_float32_matmul_precision('high') # utilises tf-32 precision for all matmul in linear layers\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n",
    "\n",
    "  # first iteration is usually slower since all initializations and everything is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c5175",
   "metadata": {},
   "source": [
    "## BF16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214fcecf",
   "metadata": {},
   "source": [
    "FP16 (that comes after FP32) is even more aggressive in cropping off bits\n",
    "sign : 1, exponent : 5, mantissa : 7\n",
    "(exponent is how much range you have for numbers and presicion is how much precision we have)\n",
    "Range of numbers in same in all (8 bits) but we have fewer possibilities of numbers in the range as precision decreases.\n",
    "\n",
    "FP32 : exponent = 8 bits -> range = -126 to +127 \n",
    "max value = (1.11111...binary)* 2^127 = 3.4 * 10^38\n",
    "min value approx = 2^-126 ~ 1.18 * 10^-38\n",
    "\n",
    "FP16 : exponent = 5 bits -> range = -14 to +15\n",
    "max value = (1.111111.... binary) * 2^15  ~ 6.55 *10^4\n",
    "min value = 2^-14 = 6.1*10^-5\n",
    "\n",
    "Gradients are often extremely small. During backpropagation, gradients commonly look like: 1e−6, 1e−7, 1e−8. \n",
    "But FP16 cannot represent many numbers below ~1e−5, they underflow to zero.\n",
    "\n",
    "\n",
    "So why GRADIENT SCALING?\n",
    "\n",
    "Gradient scaling multiplies the loss (and therefore gradients) by a large constant, such as:\n",
    "scaled_loss = loss * 2^10\n",
    "\n",
    "This makes tiny gradients large enough to fit into FP16 without being rounded to zero.\n",
    "\n",
    "Then after backpropagation:\n",
    "scaled_gradients / 2^10\n",
    "\n",
    "HOW BF16 AVOIDS THE PROBLEM ?\n",
    "BF16 keeps FP32’s exponent size (8 bits), so it has the same numeric range, just lower precision.\n",
    "\n",
    "BF16 - sign : 1, exponent : 8, mantissa : 7\n",
    "\n",
    "With `torch.autocast` to `BF16`, activations change to BF16 dtype but weights remain in FP32 only - **Mixed Precision Training**\n",
    "Only some operations like matrix multiplication change to BF16 but some operations like layer norm remain in FP32 (as they are more sensitive to precision changes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbf7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step : 0, loss : 10.838104248046875, dt : 153.73ms, tok/sec : 53289.418911373876\n",
      "step : 1, loss : 9.494848251342773, dt : 147.25ms, tok/sec : 55634.56164456758\n",
      "step : 2, loss : 8.750768661499023, dt : 146.89ms, tok/sec : 55768.92784880597\n",
      "step : 3, loss : 8.591974258422852, dt : 147.48ms, tok/sec : 55548.30664968087\n",
      "step : 4, loss : 8.436563491821289, dt : 146.85ms, tok/sec : 55784.04844611993\n",
      "step : 5, loss : 8.402196884155273, dt : 147.09ms, tok/sec : 55695.43373068232\n",
      "step : 6, loss : 8.312629699707031, dt : 146.86ms, tok/sec : 55781.24100894033\n",
      "step : 7, loss : 7.993788719177246, dt : 147.10ms, tok/sec : 55688.12204802894\n",
      "step : 8, loss : 7.721209526062012, dt : 146.88ms, tok/sec : 55774.81213679542\n",
      "step : 9, loss : 7.682507514953613, dt : 147.11ms, tok/sec : 55687.03899905189\n",
      "step : 10, loss : 7.654758453369141, dt : 147.02ms, tok/sec : 55719.45844684881\n",
      "step : 11, loss : 7.482460021972656, dt : 147.32ms, tok/sec : 55605.840224237596\n",
      "step : 12, loss : 7.447639465332031, dt : 146.93ms, tok/sec : 55754.539206968395\n",
      "step : 13, loss : 7.24798583984375, dt : 147.24ms, tok/sec : 55638.525548414465\n",
      "step : 14, loss : 7.147510528564453, dt : 146.92ms, tok/sec : 55757.61539934506\n",
      "step : 15, loss : 6.975330352783203, dt : 147.21ms, tok/sec : 55649.96998506704\n",
      "step : 16, loss : 6.85297966003418, dt : 146.86ms, tok/sec : 55782.5994109988\n",
      "step : 17, loss : 6.856358528137207, dt : 147.20ms, tok/sec : 55650.330516243346\n",
      "step : 18, loss : 6.725128173828125, dt : 146.97ms, tok/sec : 55739.79629188019\n",
      "step : 19, loss : 6.584721565246582, dt : 147.23ms, tok/sec : 55640.68791587116\n",
      "step : 20, loss : 6.626708984375, dt : 147.16ms, tok/sec : 55668.99438283256\n",
      "step : 21, loss : 6.511322021484375, dt : 147.16ms, tok/sec : 55668.363033074\n",
      "step : 22, loss : 6.588613510131836, dt : 146.98ms, tok/sec : 55737.26455125158\n",
      "step : 23, loss : 6.454343795776367, dt : 147.21ms, tok/sec : 55649.879853002865\n",
      "step : 24, loss : 6.422605514526367, dt : 146.87ms, tok/sec : 55778.43385432562\n",
      "step : 25, loss : 6.465125560760498, dt : 147.16ms, tok/sec : 55665.74759376656\n",
      "step : 26, loss : 6.611123561859131, dt : 146.92ms, tok/sec : 55756.34869078683\n",
      "step : 27, loss : 6.516225337982178, dt : 147.18ms, tok/sec : 55657.90275019519\n",
      "step : 28, loss : 6.764455318450928, dt : 146.87ms, tok/sec : 55777.34728968079\n",
      "step : 29, loss : 6.554035186767578, dt : 147.14ms, tok/sec : 55674.85760025925\n",
      "step : 30, loss : 6.551987648010254, dt : 146.94ms, tok/sec : 55752.006126926026\n",
      "step : 31, loss : 6.524900913238525, dt : 147.30ms, tok/sec : 55613.04030195715\n",
      "step : 32, loss : 6.354310989379883, dt : 146.92ms, tok/sec : 55759.60605765548\n",
      "step : 33, loss : 6.691020965576172, dt : 147.30ms, tok/sec : 55616.10089964099\n",
      "step : 34, loss : 6.546926498413086, dt : 146.91ms, tok/sec : 55760.14898913675\n",
      "step : 35, loss : 6.427603721618652, dt : 147.19ms, tok/sec : 55656.09965044593\n",
      "step : 36, loss : 9.302313804626465, dt : 146.95ms, tok/sec : 55745.8553192594\n",
      "step : 37, loss : 6.511934280395508, dt : 147.11ms, tok/sec : 55687.58051827445\n",
      "step : 38, loss : 6.298160552978516, dt : 146.87ms, tok/sec : 55775.89860266999\n",
      "step : 39, loss : 6.350425720214844, dt : 147.17ms, tok/sec : 55664.12434125155\n",
      "step : 40, loss : 6.589813232421875, dt : 146.98ms, tok/sec : 55736.63165202947\n",
      "step : 41, loss : 6.3705902099609375, dt : 147.29ms, tok/sec : 55618.98176991437\n",
      "step : 42, loss : 6.4264984130859375, dt : 146.91ms, tok/sec : 55760.87291422562\n",
      "step : 43, loss : 6.154778957366943, dt : 147.24ms, tok/sec : 55636.543525887544\n",
      "step : 44, loss : 6.10206937789917, dt : 146.91ms, tok/sec : 55761.77784702071\n",
      "step : 45, loss : 6.186244010925293, dt : 147.28ms, tok/sec : 55620.06217321346\n",
      "step : 46, loss : 6.299129486083984, dt : 146.98ms, tok/sec : 55736.450826320266\n",
      "step : 47, loss : 6.25861120223999, dt : 147.18ms, tok/sec : 55658.35354338744\n",
      "step : 48, loss : 6.155384540557861, dt : 146.95ms, tok/sec : 55748.56874357085\n",
      "step : 49, loss : 6.02863883972168, dt : 147.24ms, tok/sec : 55638.88593131881\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "torch.set_float32_matmul_precision('high') # utilises tf-32 precision for all matmul in linear layers\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n",
    "\n",
    "# Throughput - rougly 2x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76485c7",
   "metadata": {},
   "source": [
    "## torch.compile() - TC\n",
    "\n",
    "It costs compilation time but execution will be much faster. Speedup comes from python overhead and GPU read-writes. TC will know what kind of operations will come and will try to optimize them. It takes out python interpreter from forward pass entirely and compiles entire neural net as a single object without the interpreter.\n",
    "\n",
    "Read/writes : between GPU and HBM. \n",
    "GPU has HBM (High Bandwidth Memory), CPU has RAM\n",
    "without `torch.compile`, python doesn't know some variable is accessed multiple times, continuously and so variable does round trips between gpu and hbm. Because of TC, python knows the entire operation in advance and prevents these continous round trips\n",
    "\n",
    " TC optimizes round-trips to memory like if some operation causes frequent switch between GPU-HBM, it will keep the object on gpu until completion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ea165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step : 0, loss : 10.931659698486328, dt : 3108.30ms, tok/sec : 2635.5246707620904\n",
      "step : 1, loss : 9.535611152648926, dt : 127.04ms, tok/sec : 64481.13193397952\n",
      "step : 2, loss : 8.900646209716797, dt : 126.20ms, tok/sec : 64912.85697714619\n",
      "step : 3, loss : 8.814277648925781, dt : 126.38ms, tok/sec : 64821.74519728713\n",
      "step : 4, loss : 8.590795516967773, dt : 125.99ms, tok/sec : 65021.44696151102\n",
      "step : 5, loss : 8.411693572998047, dt : 126.38ms, tok/sec : 64820.033179959966\n",
      "step : 6, loss : 8.372682571411133, dt : 125.89ms, tok/sec : 65072.79718192493\n",
      "step : 7, loss : 8.086053848266602, dt : 126.24ms, tok/sec : 64891.525638580155\n",
      "step : 8, loss : 7.807655334472656, dt : 125.90ms, tok/sec : 65067.49832501046\n",
      "step : 9, loss : 7.746466159820557, dt : 126.27ms, tok/sec : 64878.047541091946\n",
      "step : 10, loss : 7.729903697967529, dt : 125.87ms, tok/sec : 65080.56218214741\n",
      "step : 11, loss : 7.5294671058654785, dt : 126.63ms, tok/sec : 64692.13270649016\n",
      "step : 12, loss : 7.490123271942139, dt : 126.18ms, tok/sec : 64920.58385135416\n",
      "step : 13, loss : 7.297810077667236, dt : 125.98ms, tok/sec : 65025.63075531365\n",
      "step : 14, loss : 7.20110559463501, dt : 125.61ms, tok/sec : 65219.90696791559\n",
      "step : 15, loss : 7.003090858459473, dt : 126.12ms, tok/sec : 64952.36922565071\n",
      "step : 16, loss : 6.914262771606445, dt : 125.52ms, tok/sec : 65266.98382559374\n",
      "step : 17, loss : 6.881943702697754, dt : 125.79ms, tok/sec : 65124.59887793783\n",
      "step : 18, loss : 6.777994155883789, dt : 125.49ms, tok/sec : 65282.484492441916\n",
      "step : 19, loss : 6.638248443603516, dt : 125.77ms, tok/sec : 65135.95692956315\n",
      "step : 20, loss : 6.678903579711914, dt : 125.52ms, tok/sec : 65265.248209275866\n",
      "step : 21, loss : 6.55802059173584, dt : 125.80ms, tok/sec : 65121.14286580697\n",
      "step : 22, loss : 6.63087797164917, dt : 125.51ms, tok/sec : 65269.3394525388\n",
      "step : 23, loss : 6.516386032104492, dt : 125.81ms, tok/sec : 65113.49157270367\n",
      "step : 24, loss : 6.506683349609375, dt : 125.53ms, tok/sec : 65257.563017900386\n",
      "step : 25, loss : 6.554165363311768, dt : 125.79ms, tok/sec : 65126.080138289726\n",
      "step : 26, loss : 6.660981178283691, dt : 125.52ms, tok/sec : 65266.487925797745\n",
      "step : 27, loss : 6.561960697174072, dt : 125.79ms, tok/sec : 65123.48797689198\n",
      "step : 28, loss : 6.769651889801025, dt : 125.48ms, tok/sec : 65287.81842399783\n",
      "step : 29, loss : 6.584995269775391, dt : 125.80ms, tok/sec : 65118.921337411826\n",
      "step : 30, loss : 6.572522163391113, dt : 125.53ms, tok/sec : 65260.41376481001\n",
      "step : 31, loss : 6.5664215087890625, dt : 125.82ms, tok/sec : 65109.54322162047\n",
      "step : 32, loss : 6.4203691482543945, dt : 125.48ms, tok/sec : 65284.96528242232\n",
      "step : 33, loss : 6.735064506530762, dt : 125.82ms, tok/sec : 65107.19911471726\n",
      "step : 34, loss : 6.600493431091309, dt : 125.54ms, tok/sec : 65255.45610416643\n",
      "step : 35, loss : 6.4838714599609375, dt : 125.75ms, tok/sec : 65147.44246561057\n",
      "step : 36, loss : 6.524599075317383, dt : 125.50ms, tok/sec : 65272.68721492848\n",
      "step : 37, loss : 6.523524761199951, dt : 125.79ms, tok/sec : 65126.45046390629\n",
      "step : 38, loss : 8.4571533203125, dt : 125.51ms, tok/sec : 65269.46343761457\n",
      "step : 39, loss : 6.38041877746582, dt : 125.75ms, tok/sec : 65146.08375013746\n",
      "step : 40, loss : 6.625796318054199, dt : 125.54ms, tok/sec : 65253.47325080998\n",
      "step : 41, loss : 6.371439456939697, dt : 125.81ms, tok/sec : 65115.34252712371\n",
      "step : 42, loss : 6.41518497467041, dt : 125.52ms, tok/sec : 65266.11600589603\n",
      "step : 43, loss : 6.151994705200195, dt : 125.79ms, tok/sec : 65122.62396894711\n",
      "step : 44, loss : 6.113969802856445, dt : 125.50ms, tok/sec : 65273.80321204269\n",
      "step : 45, loss : 6.195273399353027, dt : 125.78ms, tok/sec : 65130.524323668564\n",
      "step : 46, loss : 6.304962158203125, dt : 125.48ms, tok/sec : 65286.45384691388\n",
      "step : 47, loss : 6.251792907714844, dt : 126.04ms, tok/sec : 64995.126024541714\n",
      "step : 48, loss : 6.174210548400879, dt : 125.53ms, tok/sec : 65259.67006769128\n",
      "step : 49, loss : 6.04508638381958, dt : 125.80ms, tok/sec : 65121.513135276\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "model=torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ed14d",
   "metadata": {},
   "source": [
    "## Flask Attention \n",
    "change in code in CausalAttention class, implementation of Attention\n",
    "The 4 lines of code are fused into Flash Attention.\n",
    "Its kernel fusion operation that TC does not find because it requires algorithmic re-write how how attention is implemented. FA is very mindful about how pytorch does that computation such that fewer read/writes to HBM are ensured.\n",
    "FA does not store the Attention matrix in HBM.\n",
    "It does this using 'online softmax' trick shown in one paper, 'Online Normalizer Calculation for Softmax' (2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11012d88",
   "metadata": {},
   "source": [
    "The trick is to maintain two running values for each query row:\n",
    "\n",
    "m = running maximum of logits seen so far\n",
    "s = running sum of exp(logits − m) \n",
    "so if m was updated to m', s = s*(m-m') + sum over all logits(exp(logits - m'))\n",
    "we do (logits-m) because it keeps the exponentials small and avoids overflow\n",
    "At the end, we have:\n",
    "- the correct maximum\n",
    "- the correct sum\n",
    "so we can compute the correct softmax, even though we never saw the whole row at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbfef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khushi/miniconda3/lib/python3.13/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss : 10.88905143737793, dt : 21698.49ms, tok/sec : 377.53776027710165\n",
      "step : 1, loss : 9.530860900878906, dt : 132.12ms, tok/sec : 62003.95263410532\n",
      "step : 2, loss : 8.81982135772705, dt : 126.08ms, tok/sec : 64976.56659392362\n",
      "step : 3, loss : 8.73803424835205, dt : 126.34ms, tok/sec : 64841.43989854766\n",
      "step : 4, loss : 8.494017601013184, dt : 125.90ms, tok/sec : 65067.005451959696\n",
      "step : 5, loss : 8.378604888916016, dt : 125.44ms, tok/sec : 65305.43899615881\n",
      "step : 6, loss : 8.361116409301758, dt : 125.06ms, tok/sec : 65503.26635783052\n",
      "step : 7, loss : 8.03114128112793, dt : 125.35ms, tok/sec : 65353.75818925345\n",
      "step : 8, loss : 7.7094807624816895, dt : 125.00ms, tok/sec : 65533.875068900736\n",
      "step : 9, loss : 7.686307907104492, dt : 125.37ms, tok/sec : 65340.21166890745\n",
      "step : 10, loss : 7.7129364013671875, dt : 125.04ms, tok/sec : 65513.00806906756\n",
      "step : 11, loss : 7.530092716217041, dt : 125.41ms, tok/sec : 65319.46779816967\n",
      "step : 12, loss : 7.474321365356445, dt : 124.99ms, tok/sec : 65542.2505961033\n",
      "step : 13, loss : 7.246155738830566, dt : 125.31ms, tok/sec : 65372.036247890515\n",
      "step : 14, loss : 7.159945964813232, dt : 125.31ms, tok/sec : 65371.41437725453\n",
      "step : 15, loss : 6.976926803588867, dt : 125.29ms, tok/sec : 65383.6051987593\n",
      "step : 16, loss : 6.891236305236816, dt : 124.95ms, tok/sec : 65564.13707512265\n",
      "step : 17, loss : 6.90234375, dt : 125.28ms, tok/sec : 65391.56900613954\n",
      "step : 18, loss : 6.776302814483643, dt : 126.32ms, tok/sec : 64851.842550002926\n",
      "step : 19, loss : 6.629461288452148, dt : 125.33ms, tok/sec : 65365.942425787405\n",
      "step : 20, loss : 6.664961814880371, dt : 125.00ms, tok/sec : 65537.12501931224\n",
      "step : 21, loss : 6.567729949951172, dt : 125.99ms, tok/sec : 65020.462617680176\n",
      "step : 22, loss : 6.637228012084961, dt : 125.02ms, tok/sec : 65523.002578223925\n",
      "step : 23, loss : 6.515974044799805, dt : 125.13ms, tok/sec : 65468.0704835071\n",
      "step : 24, loss : 6.493626117706299, dt : 124.91ms, tok/sec : 65583.53445261382\n",
      "step : 25, loss : 6.551189422607422, dt : 125.15ms, tok/sec : 65457.84331909608\n",
      "step : 26, loss : 6.6523895263671875, dt : 125.97ms, tok/sec : 65030.92279506397\n",
      "step : 27, loss : 6.560812950134277, dt : 125.37ms, tok/sec : 65341.702753842845\n",
      "step : 28, loss : 6.771550178527832, dt : 124.95ms, tok/sec : 65563.51154430342\n",
      "step : 29, loss : 6.580754280090332, dt : 125.19ms, tok/sec : 65434.28311778832\n",
      "step : 30, loss : 6.5668840408325195, dt : 125.27ms, tok/sec : 65393.311347614916\n",
      "step : 31, loss : 6.5547895431518555, dt : 125.29ms, tok/sec : 65385.34711586768\n",
      "step : 32, loss : 6.407623291015625, dt : 124.82ms, tok/sec : 65629.63358673645\n",
      "step : 33, loss : 6.720677375793457, dt : 125.14ms, tok/sec : 65461.45987787801\n",
      "step : 34, loss : 6.5770182609558105, dt : 124.97ms, tok/sec : 65549.87793817738\n",
      "step : 35, loss : 6.466804504394531, dt : 126.30ms, tok/sec : 64863.595441391066\n",
      "step : 36, loss : 6.5375542640686035, dt : 125.07ms, tok/sec : 65499.02087749531\n",
      "step : 37, loss : 6.528676986694336, dt : 125.34ms, tok/sec : 65355.99576970776\n",
      "step : 38, loss : 6.322078227996826, dt : 125.06ms, tok/sec : 65503.26635783052\n",
      "step : 39, loss : 6.314095497131348, dt : 125.49ms, tok/sec : 65281.74029221211\n",
      "step : 40, loss : 6.598089218139648, dt : 125.28ms, tok/sec : 65390.32453335769\n",
      "step : 41, loss : 6.393716812133789, dt : 125.44ms, tok/sec : 65304.81839192312\n",
      "step : 42, loss : 6.44027042388916, dt : 124.95ms, tok/sec : 65562.01031903527\n",
      "step : 43, loss : 6.163751125335693, dt : 125.29ms, tok/sec : 65385.471542069856\n",
      "step : 44, loss : 6.0942254066467285, dt : 124.99ms, tok/sec : 65543.12577470619\n",
      "step : 45, loss : 6.169733047485352, dt : 125.40ms, tok/sec : 65327.04337360851\n",
      "step : 46, loss : 6.272375583648682, dt : 125.88ms, tok/sec : 65077.357354308755\n",
      "step : 47, loss : 6.251258850097656, dt : 125.38ms, tok/sec : 65335.4903488706\n",
      "step : 48, loss : 6.138665199279785, dt : 124.92ms, tok/sec : 65575.39867130558\n",
      "step : 49, loss : 5.997424125671387, dt : 125.26ms, tok/sec : 65399.16131598732\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "model=torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b75a7",
   "metadata": {},
   "source": [
    "## Numbers\n",
    "everything in cuda works in power of 2 and lots of blocks are written in power of 2 \n",
    "Change in code wherever numbers are there to powers of 2\n",
    "like changing vocab size to power of 2\n",
    "\n",
    "we achieve improvement by increasing vocab size, but tokens till 50304 will never be indexed since gpt-2 has fixed token size of 50257, we are wasting a little bit space\n",
    "the model has to learn to push the probabilities of these tokens to neg infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c31e44ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step : 0, loss : 11.029869079589844, dt : 19122.68ms, tok/sec : 428.3917359562744\n",
      "step : 1, loss : 9.663396835327148, dt : 132.58ms, tok/sec : 61790.86683751\n",
      "step : 2, loss : 9.066415786743164, dt : 126.67ms, tok/sec : 64669.60725148312\n",
      "step : 3, loss : 8.837348937988281, dt : 123.76ms, tok/sec : 66190.8537413721\n",
      "step : 4, loss : 8.709108352661133, dt : 123.07ms, tok/sec : 66566.0645478762\n",
      "step : 5, loss : 8.54254150390625, dt : 123.30ms, tok/sec : 66440.56534467755\n",
      "step : 6, loss : 8.422245025634766, dt : 122.94ms, tok/sec : 66634.61296553435\n",
      "step : 7, loss : 8.09824275970459, dt : 123.37ms, tok/sec : 66401.14708661783\n",
      "step : 8, loss : 7.821625232696533, dt : 122.95ms, tok/sec : 66627.89389097453\n",
      "step : 9, loss : 7.791788101196289, dt : 123.35ms, tok/sec : 66411.79952606818\n",
      "step : 10, loss : 7.746125221252441, dt : 123.00ms, tok/sec : 66601.6766227498\n",
      "step : 11, loss : 7.566364288330078, dt : 123.36ms, tok/sec : 66409.23236063813\n",
      "step : 12, loss : 7.526142120361328, dt : 122.99ms, tok/sec : 66605.54979655688\n",
      "step : 13, loss : 7.303412437438965, dt : 123.35ms, tok/sec : 66411.28607710417\n",
      "step : 14, loss : 7.205931186676025, dt : 123.03ms, tok/sec : 66583.47873808233\n",
      "step : 15, loss : 7.0183820724487305, dt : 123.11ms, tok/sec : 66540.41134526525\n",
      "step : 16, loss : 6.930351734161377, dt : 122.96ms, tok/sec : 66622.20957846977\n",
      "step : 17, loss : 6.904794692993164, dt : 123.33ms, tok/sec : 66422.84059943397\n",
      "step : 18, loss : 6.771768093109131, dt : 123.01ms, tok/sec : 66595.99678259033\n",
      "step : 19, loss : 6.636331558227539, dt : 123.29ms, tok/sec : 66445.8332150475\n",
      "step : 20, loss : 6.671727180480957, dt : 123.01ms, tok/sec : 66594.96418845975\n",
      "step : 21, loss : 6.578782081604004, dt : 123.35ms, tok/sec : 66415.13713786464\n",
      "step : 22, loss : 6.659771919250488, dt : 122.91ms, tok/sec : 66651.5458737219\n",
      "step : 23, loss : 6.552412033081055, dt : 124.11ms, tok/sec : 66005.71766012175\n",
      "step : 24, loss : 6.530409812927246, dt : 122.98ms, tok/sec : 66613.42663936969\n",
      "step : 25, loss : 6.5930962562561035, dt : 123.35ms, tok/sec : 66410.3875605207\n",
      "step : 26, loss : 6.673398971557617, dt : 123.21ms, tok/sec : 66488.90688773042\n",
      "step : 27, loss : 6.570662498474121, dt : 123.09ms, tok/sec : 66552.78429063411\n",
      "step : 28, loss : 6.7841339111328125, dt : 122.97ms, tok/sec : 66615.36385332876\n",
      "step : 29, loss : 6.603981971740723, dt : 123.27ms, tok/sec : 66458.29955204134\n",
      "step : 30, loss : 6.595612525939941, dt : 122.93ms, tok/sec : 66639.3946707473\n",
      "step : 31, loss : 6.625951290130615, dt : 123.22ms, tok/sec : 66483.76082691258\n",
      "step : 32, loss : 6.490488052368164, dt : 122.97ms, tok/sec : 66615.49300493221\n",
      "step : 33, loss : 6.771844863891602, dt : 123.23ms, tok/sec : 66475.91461765417\n",
      "step : 34, loss : 6.651806831359863, dt : 122.95ms, tok/sec : 66626.73111177902\n",
      "step : 35, loss : 6.5585503578186035, dt : 123.34ms, tok/sec : 66418.988645282\n",
      "step : 36, loss : 6.604560852050781, dt : 122.83ms, tok/sec : 66691.65041352632\n",
      "step : 37, loss : 6.616907119750977, dt : 123.20ms, tok/sec : 66494.05374525384\n",
      "step : 38, loss : 6.382877349853516, dt : 122.88ms, tok/sec : 66667.32319345894\n",
      "step : 39, loss : 6.412755966186523, dt : 123.16ms, tok/sec : 66514.90666935102\n",
      "step : 40, loss : 6.639836311340332, dt : 123.08ms, tok/sec : 66558.45676185605\n",
      "step : 41, loss : 6.421876907348633, dt : 123.11ms, tok/sec : 66541.18451956744\n",
      "step : 42, loss : 6.46583366394043, dt : 122.96ms, tok/sec : 66623.24301769136\n",
      "step : 43, loss : 6.202525615692139, dt : 123.30ms, tok/sec : 66438.38135070489\n",
      "step : 44, loss : 6.149142742156982, dt : 123.06ms, tok/sec : 66570.70744121254\n",
      "step : 45, loss : 6.223628044128418, dt : 123.47ms, tok/sec : 66346.78307951054\n",
      "step : 46, loss : 6.32037353515625, dt : 123.86ms, tok/sec : 66140.27073828966\n",
      "step : 47, loss : 6.282793998718262, dt : 123.30ms, tok/sec : 66441.33620036198\n",
      "step : 48, loss : 6.1843767166137695, dt : 122.95ms, tok/sec : 66626.8603074632\n",
      "step : 49, loss : 6.071453094482422, dt : 123.28ms, tok/sec : 66451.61598564201\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
    "model.to(device)\n",
    "model=torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i}, loss : {loss.item()}, dt : {dt:.2f}ms, tok/sec : {tokens_per_sec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05523f",
   "metadata": {},
   "source": [
    "# Hyperparameters, AdamW, Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0823d",
   "metadata": {},
   "source": [
    "gpt-2 - gpt3 change : gpt3 context window 2048 from 1024 and was trained for longer time and on larger dataset\n",
    "\n",
    "AdamW beta1 =0.9 , beta2=0.95, change in code Gradient Clipping\n",
    "\n",
    "Norms are high at beginning and then stablise during training bcoz at start model is completely random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65babc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step : 0 | loss : 11.049921035766602 | dt : 239.44ms | tok/sec : 34213.475759129724 | norm : 9.8443\n",
      "step : 1 | loss : 9.629323959350586 | dt : 126.19ms | tok/sec : 64920.461188189525 | norm : 4.6551\n",
      "step : 2 | loss : 9.161086082458496 | dt : 124.90ms | tok/sec : 65587.79089628422 | norm : 8.0600\n",
      "step : 3 | loss : 8.884873390197754 | dt : 125.12ms | tok/sec : 65475.306261362035 | norm : 4.0701\n",
      "step : 4 | loss : 8.677845001220703 | dt : 124.63ms | tok/sec : 65729.57005341047 | norm : 3.6677\n",
      "step : 5 | loss : 8.4899320602417 | dt : 124.87ms | tok/sec : 65605.44851670603 | norm : 2.7417\n",
      "step : 6 | loss : 8.43120002746582 | dt : 124.60ms | tok/sec : 65744.03327790843 | norm : 2.0226\n",
      "step : 7 | loss : 8.095075607299805 | dt : 125.22ms | tok/sec : 65422.0733928534 | norm : 2.3913\n",
      "step : 8 | loss : 7.7871413230896 | dt : 124.67ms | tok/sec : 65708.57826284014 | norm : 1.7439\n",
      "step : 9 | loss : 7.741115093231201 | dt : 124.88ms | tok/sec : 65599.06062040842 | norm : 1.7335\n",
      "step : 10 | loss : 7.723459243774414 | dt : 124.63ms | tok/sec : 65728.68988868505 | norm : 1.9576\n",
      "step : 11 | loss : 7.516016960144043 | dt : 125.18ms | tok/sec : 65443.6303739991 | norm : 1.6709\n",
      "step : 12 | loss : 7.446322917938232 | dt : 124.72ms | tok/sec : 65682.95378262906 | norm : 1.2986\n",
      "step : 13 | loss : 7.227147579193115 | dt : 124.98ms | tok/sec : 65546.87680488973 | norm : 1.2710\n",
      "step : 14 | loss : 7.1014227867126465 | dt : 124.79ms | tok/sec : 65647.81545104558 | norm : 1.1412\n",
      "step : 15 | loss : 6.892168045043945 | dt : 125.58ms | tok/sec : 65232.66056355676 | norm : 1.0919\n",
      "step : 16 | loss : 6.804851055145264 | dt : 124.76ms | tok/sec : 65659.48222632438 | norm : 1.3171\n",
      "step : 17 | loss : 6.775634288787842 | dt : 125.05ms | tok/sec : 65511.38424931123 | norm : 0.9621\n",
      "step : 18 | loss : 6.626482009887695 | dt : 124.70ms | tok/sec : 65691.36746149493 | norm : 0.8762\n",
      "step : 19 | loss : 6.457003593444824 | dt : 124.99ms | tok/sec : 65541.25042060406 | norm : 1.0786\n",
      "step : 20 | loss : 6.505607604980469 | dt : 124.61ms | tok/sec : 65740.8885572482 | norm : 0.9425\n",
      "step : 21 | loss : 6.38307523727417 | dt : 125.03ms | tok/sec : 65521.753097814275 | norm : 0.9966\n",
      "step : 22 | loss : 6.484042167663574 | dt : 125.07ms | tok/sec : 65496.773493048066 | norm : 0.8973\n",
      "step : 23 | loss : 6.390791893005371 | dt : 125.01ms | tok/sec : 65532.625173798195 | norm : 1.9345\n",
      "step : 24 | loss : 6.357727527618408 | dt : 124.84ms | tok/sec : 65618.3533563776 | norm : 1.7476\n",
      "step : 25 | loss : 6.417191028594971 | dt : 124.99ms | tok/sec : 65542.87572129187 | norm : 0.9225\n",
      "step : 26 | loss : 6.627445697784424 | dt : 124.76ms | tok/sec : 65661.74079085162 | norm : 1.2402\n",
      "step : 27 | loss : 6.511113166809082 | dt : 125.11ms | tok/sec : 65479.29909936693 | norm : 1.6470\n",
      "step : 28 | loss : 6.732813835144043 | dt : 124.74ms | tok/sec : 65674.91884804245 | norm : 1.2211\n",
      "step : 29 | loss : 6.496570587158203 | dt : 125.10ms | tok/sec : 65483.16762560724 | norm : 1.0949\n",
      "step : 30 | loss : 6.478662490844727 | dt : 124.76ms | tok/sec : 65663.37207297939 | norm : 0.8210\n",
      "step : 31 | loss : 6.475762844085693 | dt : 125.02ms | tok/sec : 65523.12752888589 | norm : 1.0922\n",
      "step : 32 | loss : 6.330128192901611 | dt : 124.78ms | tok/sec : 65653.08380385705 | norm : 1.6516\n",
      "step : 33 | loss : 6.613536834716797 | dt : 125.12ms | tok/sec : 65471.563417961755 | norm : 1.7417\n",
      "step : 34 | loss : 6.461516857147217 | dt : 124.82ms | tok/sec : 65630.00966120829 | norm : 1.3357\n",
      "step : 35 | loss : 6.380087852478027 | dt : 125.19ms | tok/sec : 65435.903117560796 | norm : 0.7781\n",
      "step : 36 | loss : 6.4179582595825195 | dt : 124.99ms | tok/sec : 65543.75091658942 | norm : 1.2563\n",
      "step : 37 | loss : 6.42179012298584 | dt : 125.28ms | tok/sec : 65390.44897850434 | norm : 1.1040\n",
      "step : 38 | loss : 6.179688930511475 | dt : 124.78ms | tok/sec : 65651.07671260621 | norm : 1.0080\n",
      "step : 39 | loss : 6.213400840759277 | dt : 125.08ms | tok/sec : 65492.02952119549 | norm : 1.0988\n",
      "step : 40 | loss : 6.475924015045166 | dt : 124.77ms | tok/sec : 65658.47846974069 | norm : 1.2268\n",
      "step : 41 | loss : 6.331905364990234 | dt : 125.15ms | tok/sec : 65458.96565664263 | norm : 0.9203\n",
      "step : 42 | loss : 6.353975296020508 | dt : 124.72ms | tok/sec : 65681.94930828887 | norm : 1.0365\n",
      "step : 43 | loss : 6.092036724090576 | dt : 125.27ms | tok/sec : 65394.80485706673 | norm : 1.2806\n",
      "step : 44 | loss : 6.016482353210449 | dt : 124.79ms | tok/sec : 65647.94087819119 | norm : 1.3394\n",
      "step : 45 | loss : 6.084061622619629 | dt : 125.11ms | tok/sec : 65478.176064457235 | norm : 1.4100\n",
      "step : 46 | loss : 6.172419548034668 | dt : 124.77ms | tok/sec : 65656.0946708476 | norm : 0.9803\n",
      "step : 47 | loss : 6.13072395324707 | dt : 124.96ms | tok/sec : 65559.25825125644 | norm : 1.2247\n",
      "step : 48 | loss : 6.007756233215332 | dt : 124.80ms | tok/sec : 65640.41609768958 | norm : 1.2049\n",
      "step : 49 | loss : 5.864972114562988 | dt : 125.14ms | tok/sec : 65462.08346288675 | norm : 1.0786\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
    "model.to(device)\n",
    "model=torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "for i in range(50):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i} | loss : {loss.item()} | dt : {dt:.2f}ms | tok/sec : {tokens_per_sec} | norm : {norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28eebcf",
   "metadata": {},
   "source": [
    "# Learning Rate :  Cosine decay LR schedule with Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3103ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4  # as per GPT-paper\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "def get_lr(it):\n",
    "  #  linear warmup for warm-iter steps\n",
    "  if it < warmup_steps:\n",
    "    return max_lr * (it+1)/ warmup_steps\n",
    "\n",
    "  # if it > lr_decay iters, return min lr\n",
    "  if it > max_steps:\n",
    "    return min_lr\n",
    "  \n",
    "  # in between, use cosine decay down to min lr\n",
    "  decay_ratio = (it-warmup_steps) / (max_steps-warmup_steps)\n",
    "  assert 0<=decay_ratio<=1\n",
    "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "  return min_lr + coeff * (max_lr-min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ef2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 41 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1052936/2447106789.py:14: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 49 | loss : 10.843093872070312 | dt : 133.76ms | tok/sec : 61242.3929773904 | lr : 0.0001\n",
      "step : 49 | loss : 9.732853889465332 | dt : 131.02ms | tok/sec : 62526.25152267868 | lr : 0.0001\n",
      "step : 49 | loss : 9.19817066192627 | dt : 125.39ms | tok/sec : 65329.7759795263 | lr : 0.0002\n",
      "step : 49 | loss : 9.358125686645508 | dt : 125.82ms | tok/sec : 65108.679583649944 | lr : 0.0002\n",
      "step : 49 | loss : 8.91999340057373 | dt : 125.40ms | tok/sec : 65328.28543886834 | lr : 0.0003\n",
      "step : 49 | loss : 8.662559509277344 | dt : 125.63ms | tok/sec : 65208.64337660366 | lr : 0.0004\n",
      "step : 49 | loss : 8.59248161315918 | dt : 125.27ms | tok/sec : 65392.937980911054 | lr : 0.0004\n",
      "step : 49 | loss : 8.153423309326172 | dt : 124.94ms | tok/sec : 65568.26587790775 | lr : 0.0005\n",
      "step : 49 | loss : 7.788590431213379 | dt : 124.41ms | tok/sec : 65846.21145303534 | lr : 0.0005\n",
      "step : 49 | loss : 7.580927848815918 | dt : 125.73ms | tok/sec : 65157.94354199457 | lr : 0.0006\n",
      "step : 49 | loss : 7.40199089050293 | dt : 124.45ms | tok/sec : 65826.78451717716 | lr : 0.0006\n",
      "step : 49 | loss : 7.153200149536133 | dt : 124.70ms | tok/sec : 65691.36746149493 | lr : 0.0006\n",
      "step : 49 | loss : 7.044985771179199 | dt : 124.34ms | tok/sec : 65882.19416221026 | lr : 0.0006\n",
      "step : 49 | loss : 6.829320907592773 | dt : 124.61ms | tok/sec : 65740.63699259168 | lr : 0.0006\n",
      "step : 49 | loss : 6.74041223526001 | dt : 124.39ms | tok/sec : 65858.58001993406 | lr : 0.0006\n",
      "step : 49 | loss : 6.541644096374512 | dt : 124.70ms | tok/sec : 65695.51230461842 | lr : 0.0006\n",
      "step : 49 | loss : 6.676312446594238 | dt : 124.37ms | tok/sec : 65869.18534763705 | lr : 0.0006\n",
      "step : 49 | loss : 6.694443225860596 | dt : 124.62ms | tok/sec : 65735.98339369381 | lr : 0.0006\n",
      "step : 49 | loss : 6.594126224517822 | dt : 124.36ms | tok/sec : 65872.72121593227 | lr : 0.0005\n",
      "step : 49 | loss : 6.41893196105957 | dt : 124.66ms | tok/sec : 65716.74712630488 | lr : 0.0005\n",
      "step : 49 | loss : 6.569978713989258 | dt : 124.39ms | tok/sec : 65854.79323047436 | lr : 0.0005\n",
      "step : 49 | loss : 6.369355201721191 | dt : 124.73ms | tok/sec : 65677.30401406836 | lr : 0.0005\n",
      "step : 49 | loss : 6.498651504516602 | dt : 124.35ms | tok/sec : 65880.04672227016 | lr : 0.0005\n",
      "step : 49 | loss : 6.314252853393555 | dt : 124.76ms | tok/sec : 65664.62696054167 | lr : 0.0005\n",
      "step : 49 | loss : 6.31321907043457 | dt : 124.37ms | tok/sec : 65866.28116834944 | lr : 0.0005\n",
      "step : 49 | loss : 6.35487174987793 | dt : 124.56ms | tok/sec : 65769.95723388901 | lr : 0.0004\n",
      "step : 49 | loss : 6.679948806762695 | dt : 124.49ms | tok/sec : 65806.61282587545 | lr : 0.0004\n",
      "step : 49 | loss : 6.531573295593262 | dt : 124.73ms | tok/sec : 65678.81052398174 | lr : 0.0004\n",
      "step : 49 | loss : 6.753340721130371 | dt : 124.42ms | tok/sec : 65844.06635814003 | lr : 0.0004\n",
      "step : 49 | loss : 6.511752605438232 | dt : 124.74ms | tok/sec : 65674.04014633418 | lr : 0.0004\n",
      "step : 49 | loss : 6.467935562133789 | dt : 124.38ms | tok/sec : 65860.978545182 | lr : 0.0003\n",
      "step : 49 | loss : 6.464847564697266 | dt : 124.63ms | tok/sec : 65729.3185753693 | lr : 0.0003\n",
      "step : 49 | loss : 6.308985233306885 | dt : 124.42ms | tok/sec : 65843.30929919267 | lr : 0.0003\n",
      "step : 49 | loss : 6.603049278259277 | dt : 124.72ms | tok/sec : 65684.33498501254 | lr : 0.0003\n",
      "step : 49 | loss : 6.460018634796143 | dt : 124.48ms | tok/sec : 65812.03275303636 | lr : 0.0002\n",
      "step : 49 | loss : 6.381960868835449 | dt : 124.74ms | tok/sec : 65672.53385525174 | lr : 0.0002\n",
      "step : 49 | loss : 6.416051864624023 | dt : 124.38ms | tok/sec : 65861.35727566523 | lr : 0.0002\n",
      "step : 49 | loss : 6.428523063659668 | dt : 124.69ms | tok/sec : 65696.39158100833 | lr : 0.0002\n",
      "step : 49 | loss : 6.209352016448975 | dt : 124.45ms | tok/sec : 65823.88407551797 | lr : 0.0002\n",
      "step : 49 | loss : 6.260601043701172 | dt : 124.70ms | tok/sec : 65691.9954344098 | lr : 0.0002\n",
      "step : 49 | loss : 6.4734392166137695 | dt : 124.33ms | tok/sec : 65890.91167942561 | lr : 0.0001\n",
      "step : 49 | loss : 6.302643775939941 | dt : 124.76ms | tok/sec : 65660.36053851078 | lr : 0.0001\n",
      "step : 49 | loss : 6.3400349617004395 | dt : 124.41ms | tok/sec : 65847.72572181998 | lr : 0.0001\n",
      "step : 49 | loss : 6.077908039093018 | dt : 124.71ms | tok/sec : 65687.5998761179 | lr : 0.0001\n",
      "step : 49 | loss : 6.009638786315918 | dt : 124.33ms | tok/sec : 65888.63732209358 | lr : 0.0001\n",
      "step : 49 | loss : 6.104546546936035 | dt : 124.49ms | tok/sec : 65806.99093135988 | lr : 0.0001\n",
      "step : 49 | loss : 6.170901298522949 | dt : 124.49ms | tok/sec : 65802.83200998923 | lr : 0.0001\n",
      "step : 49 | loss : 6.055858612060547 | dt : 125.88ms | tok/sec : 65077.60386832102 | lr : 0.0001\n",
      "step : 49 | loss : 5.947863578796387 | dt : 124.34ms | tok/sec : 65885.60508987433 | lr : 0.0001\n",
      "step : 49 | loss : 5.835231781005859 | dt : 124.50ms | tok/sec : 65801.82386579084 | lr : 0.0001\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
    "model.to(device)\n",
    "model=torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "for step in range(max_steps):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "  lr = get_lr(step)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i} | loss : {loss.item()} | dt : {dt:.2f}ms | tok/sec : {tokens_per_sec} | lr : {lr:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cecd0",
   "metadata": {},
   "source": [
    "All models use weight decay of 0.1 to provide a small amount of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "for step in range(max_steps):\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "  lr = get_lr(step)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize()\n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "  print(f\"step : {i} | loss : {loss.item()} | dt : {dt:.2f}ms | tok/sec : {tokens_per_sec} | lr : {lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0f565",
   "metadata": {},
   "source": [
    "In original GPT-3, they use batch size = 0.5M (in terms of tokens which is rougly 500 rows [0.5M/1024] ). But we can't do that else our small GPUs would explode\n",
    "\n",
    "Gradient Accumulation to Rescue!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bebbf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size : 524288\n",
      "calculated gradient accumulation steps : 32\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524288 # 2^19, ~0.5M in number of tokens\n",
    "B = 32 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T) == 0\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size : {total_batch_size}\")\n",
    "print(f\"calculated gradient accumulation steps : {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch = 20 batches\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step : 0 | loss : 10.986649 | tok/sec : 78220.95958608709 | lr : 0.0001 | dt : 6702.65ms\n",
      "step : 1 | loss : 9.722128 | tok/sec : 78162.1675988861 | lr : 0.0001 | dt : 6707.70ms\n",
      "step : 2 | loss : 9.355636 | tok/sec : 78092.89908146107 | lr : 0.0002 | dt : 6713.64ms\n",
      "step : 3 | loss : 9.638528 | tok/sec : 78070.50851701543 | lr : 0.0002 | dt : 6715.57ms\n",
      "step : 4 | loss : 9.039129 | tok/sec : 77913.67233554977 | lr : 0.0003 | dt : 6729.09ms\n",
      "step : 5 | loss : 8.604715 | tok/sec : 77905.50744847069 | lr : 0.0004 | dt : 6729.79ms\n",
      "step : 6 | loss : 8.368487 | tok/sec : 77867.39141303494 | lr : 0.0004 | dt : 6733.09ms\n",
      "step : 7 | loss : 8.068794 | tok/sec : 77657.16618514319 | lr : 0.0005 | dt : 6751.32ms\n",
      "step : 8 | loss : 7.720360 | tok/sec : 77443.80718999157 | lr : 0.0005 | dt : 6769.92ms\n",
      "step : 9 | loss : 7.386857 | tok/sec : 77441.3335472133 | lr : 0.0006 | dt : 6770.13ms\n",
      "step : 10 | loss : 7.083947 | tok/sec : 77399.42020218246 | lr : 0.0006 | dt : 6773.80ms\n",
      "step : 11 | loss : 6.806040 | tok/sec : 77369.79724584731 | lr : 0.0006 | dt : 6776.39ms\n",
      "step : 12 | loss : 6.638362 | tok/sec : 77309.57832876881 | lr : 0.0006 | dt : 6781.67ms\n",
      "step : 13 | loss : 6.498683 | tok/sec : 77105.96483023402 | lr : 0.0006 | dt : 6799.58ms\n",
      "step : 14 | loss : 6.402255 | tok/sec : 76856.95413876262 | lr : 0.0006 | dt : 6821.61ms\n",
      "step : 15 | loss : 6.363053 | tok/sec : 76837.73389406932 | lr : 0.0006 | dt : 6823.31ms\n",
      "step : 16 | loss : 6.308476 | tok/sec : 76829.18092895595 | lr : 0.0006 | dt : 6824.07ms\n",
      "step : 17 | loss : 6.300042 | tok/sec : 76820.93312139504 | lr : 0.0006 | dt : 6824.81ms\n",
      "step : 18 | loss : 6.265434 | tok/sec : 76794.42223292001 | lr : 0.0005 | dt : 6827.16ms\n",
      "step : 19 | loss : 6.257458 | tok/sec : 76759.38416489151 | lr : 0.0005 | dt : 6830.28ms\n",
      "step : 20 | loss : 6.229150 | tok/sec : 76751.59333966559 | lr : 0.0005 | dt : 6830.97ms\n",
      "step : 21 | loss : 6.189003 | tok/sec : 76756.77186533493 | lr : 0.0005 | dt : 6830.51ms\n",
      "step : 22 | loss : 6.826686 | tok/sec : 76731.36279991212 | lr : 0.0005 | dt : 6832.77ms\n",
      "step : 23 | loss : 6.144308 | tok/sec : 76772.80749015379 | lr : 0.0005 | dt : 6829.08ms\n",
      "step : 24 | loss : 6.135485 | tok/sec : 76751.33081532994 | lr : 0.0005 | dt : 6831.00ms\n",
      "step : 25 | loss : 6.125299 | tok/sec : 76728.75240723883 | lr : 0.0004 | dt : 6833.01ms\n",
      "step : 26 | loss : 6.097565 | tok/sec : 76748.15119889012 | lr : 0.0004 | dt : 6831.28ms\n",
      "step : 27 | loss : 6.102455 | tok/sec : 76753.76861039312 | lr : 0.0004 | dt : 6830.78ms\n",
      "step : 28 | loss : 6.066394 | tok/sec : 76728.53555183807 | lr : 0.0004 | dt : 6833.02ms\n",
      "step : 29 | loss : 6.050563 | tok/sec : 76751.54244194998 | lr : 0.0004 | dt : 6830.98ms\n",
      "step : 30 | loss : 6.040265 | tok/sec : 76711.76376068554 | lr : 0.0003 | dt : 6834.52ms\n",
      "step : 31 | loss : 6.016330 | tok/sec : 76692.65885167786 | lr : 0.0003 | dt : 6836.22ms\n",
      "step : 32 | loss : 6.017941 | tok/sec : 76701.35266591585 | lr : 0.0003 | dt : 6835.45ms\n",
      "step : 33 | loss : 5.988286 | tok/sec : 76700.83633205203 | lr : 0.0003 | dt : 6835.49ms\n",
      "step : 34 | loss : 5.974520 | tok/sec : 76690.27575327433 | lr : 0.0002 | dt : 6836.43ms\n",
      "step : 35 | loss : 5.969413 | tok/sec : 76699.038581336 | lr : 0.0002 | dt : 6835.65ms\n",
      "step : 36 | loss : 5.950045 | tok/sec : 76666.9126519638 | lr : 0.0002 | dt : 6838.52ms\n",
      "step : 37 | loss : 5.955992 | tok/sec : 76738.40505263487 | lr : 0.0002 | dt : 6832.15ms\n",
      "step : 38 | loss : 5.932929 | tok/sec : 76750.49771427987 | lr : 0.0002 | dt : 6831.07ms\n",
      "step : 39 | loss : 5.925669 | tok/sec : 76743.95140173491 | lr : 0.0002 | dt : 6831.65ms\n",
      "step : 40 | loss : 5.925241 | tok/sec : 76709.58551838806 | lr : 0.0001 | dt : 6834.71ms\n",
      "step : 41 | loss : 5.911896 | tok/sec : 76672.86036262497 | lr : 0.0001 | dt : 6837.99ms\n",
      "step : 42 | loss : 5.922660 | tok/sec : 76677.91062651153 | lr : 0.0001 | dt : 6837.54ms\n",
      "step : 43 | loss : 5.902142 | tok/sec : 76664.72893768667 | lr : 0.0001 | dt : 6838.71ms\n",
      "step : 44 | loss : 5.899735 | tok/sec : 76640.4892459614 | lr : 0.0001 | dt : 6840.87ms\n",
      "step : 45 | loss : 5.902416 | tok/sec : 76658.111733437 | lr : 0.0001 | dt : 6839.30ms\n",
      "step : 46 | loss : 5.891271 | tok/sec : 76656.67138681782 | lr : 0.0001 | dt : 6839.43ms\n",
      "step : 47 | loss : 5.905720 | tok/sec : 76647.21828443799 | lr : 0.0001 | dt : 6840.27ms\n",
      "step : 48 | loss : 5.886256 | tok/sec : 76675.55517978693 | lr : 0.0001 | dt : 6837.75ms\n",
      "step : 49 | loss : 5.885692 | tok/sec : 76675.77440935609 | lr : 0.0001 | dt : 6837.73ms\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=B, T=T)\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # changed to nearest power of 2\n",
    "model.to(device)\n",
    "model=torch.compile(model)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "for step in range(max_steps):\n",
    "  loss_accum = 0.0\n",
    "  t0 = time.time()\n",
    "  optimizer.zero_grad()\n",
    "  for micro_step in range(grad_accum_steps): \n",
    "      x, y = train_loader.next_batch()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      \n",
    "      with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "      # loss is scaled below to account for gradient accumulation as the gradients just add on each successful backward()\n",
    "      # addition of gradients correspons to SUM in the objective, but instead of SUM, we want MEAN.\n",
    "      # So loss is scaled here\n",
    "      loss = loss / grad_accum_steps \n",
    "      loss_accum += loss.detach() # detach the tensor from computational graph\n",
    "      loss.backward()\n",
    "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "  # determine and set lr for this iteration\n",
    "  lr = get_lr(step)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "  optimizer.step()\n",
    "  torch.cuda.synchronize() # wait for gpu to finish work \n",
    "  t1 = time.time()\n",
    "  dt = (t1-t0)*1000\n",
    "  tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "  tokens_per_sec = tokens_processed / (t1 - t0)\n",
    "  print(f\"step : {step} | loss : {loss_accum.item():.6f} | tok/sec : {tokens_per_sec} | lr : {lr:.4f} | dt : {dt:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986378b8",
   "metadata": {},
   "source": [
    "# Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288b6ec",
   "metadata": {},
   "source": [
    "DDP - we have 4 GPUs so we launch 4 processes and each process will be assigned a GPU. each GPU processes slightly different part of data and then we do average of gradients of all 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6c936",
   "metadata": {},
   "source": [
    "There will be 4 python interpreters running the same script in parallel, only difference is that each has unique ddp rank"
   ]
  }
 
