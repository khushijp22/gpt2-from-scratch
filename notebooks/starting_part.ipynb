{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5p7sxxw-kp2"
   },
   "source": [
    "Refer paper Language Models are Unsupervised Multitask Learners\n",
    "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "\n",
    "GPT-2[124M] has 12 layers and $d_{model}$ = 768\n",
    "\n",
    "It is decoder only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MogKTW_5oPiT"
   },
   "source": [
    "Variables are named to follow the schema of Hugging Face Transformers code.  \n",
    "The following variables should be exactly the same:\n",
    "\n",
    "`transformer.wte.weight`  \n",
    "`transformer.wpe.weight`  \n",
    "`transformer.h.0.attn.c_attn.weight`  \n",
    "`transformer.h.0.attn.c_proj.weight`  \n",
    "`transformer.h.0.mlp.c_fc.weight`  \n",
    "`transformer.h.0.mlp.c_proj.weight`  \n",
    "`transformer.ln_f.weight`  \n",
    "`lm_head.weight`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AivZQshf94C1"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import tiktoken\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "F70TvxkCHisq",
    "outputId": "f8b37f17-3fc4-4149-eab2-595b612ed439"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a3db92a766724868af87dda287644497",
      "0e161cb47ea242a8925d2e84f93df828",
      "1539651aad78451497b634d53e80d30a",
      "8e4a2ae409da4efbab84c91ac6d40634",
      "e41ce93f424549e5a7913053ae80d23c",
      "637fa0d7052843139fa6807319736d35",
      "b12fb73568594124b789287534c141e1",
      "c6879c6929144b5e8279733429127edb",
      "c768bc36988749d2a7f2f8be8fadd856",
      "7b3c3a63e8144b7e8be9cb8d43bf0a5f",
      "b5bb0ce37bec4baeaa617c5fa0a3898b",
      "f3f942728f5c4faba6e6565c945378e3",
      "7aa66e058f9e4d59b46d2c05347af6cb",
      "72a7b94fbc3140e9bc2cdfb895a335e8",
      "dd3a0712de2c47a7acd49247c7769b32",
      "eea28c6cc6f84a2d9abb7908d31ff5aa",
      "21582e9fb97c4e86abf031a1e5a2bd9f",
      "3d2e3b74e4fa422cb30ffacbc43b9298",
      "c55cc2672cbc4183b92c625c84587a95",
      "5cc14673359140278cf351c92bac0c60",
      "9b8d1220847e4b5f9aa79d80357fef4a",
      "adb0305197e64fe98b2e75086ca91480",
      "86045121c2dc4f8aaf194376615539ec",
      "136a118457b7475899b81830e3ab35e9",
      "a4edf4e48ab8403bab2cb64a1843bd92",
      "256254aaa9ac4402b7d2ab1bbc57b46c",
      "9f5ece2a7a354dbbae1f51337e3ead4a",
      "08bba939aced448e88c0047b8157735d",
      "695ee9b569164daeb45c886f02b66094",
      "c0bd72480f8f45c6a076098ea5b9c38d",
      "d2fcfe13d23b473784dec513ed588f4f",
      "dacdc85236ce4ce08d95fe80d6169ea1",
      "9d61f24f3090425995beb0af5eff3ae4"
     ]
    },
    "id": "nuz6GS5yn8Ta",
    "outputId": "19ffccb2-c18f-4bf2-f014-0484f5c6ac09"
   },
   "outputs": [],
   "source": [
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\") #124M\n",
    "sd = model_gpt2.state_dict()\n",
    "for k,v in sd.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "0ba48ebfa6bc4869b6fa00d93ea6ccad",
      "44a2ac6af3b74f9490a3a64808eaf161",
      "c940e54136424f7b88a85ec8ddbf50e9",
      "bf93d3d4a8ba4920be7a4fd701a0924c",
      "c7716421d1dc4570bec2e37255c2cae0",
      "86e6da46b6f040909da6e637492dda19",
      "437d4724c39242f2ab882659c86ee344",
      "505778c070d447ea8ace4f5f5823a274",
      "1c3cc9df8b9e463ebad3dbf2cc3996a4",
      "954fa61ac13b4ea6b0556661f3494a79",
      "6a1e63811f464d96ba48643d9269827a",
      "3d517f41e30b4b3abb30f993459cf50d",
      "98a3ed2950554a4cb470a23e2ffae74d",
      "219a2e93248443aa920ff8a42f5bbc4a",
      "8eaaa4032d994218b32e55a39e325a5a",
      "c0f06bc4991b492785b10b8e7ac7a9a6",
      "0833f9f6a79d484d9d7faea7590781f9",
      "1ecda8fe2ade402caeaa66739cd76241",
      "617725f2528e4033b55797da71f68311",
      "a3b03ebd19af4b40ad508f260175512a",
      "df64a4396f934071a300f346f12c7a9a",
      "5b46a497f5a14037931470ea30f96817",
      "6c7a835f21dd4251b2a4bc24c648ac8e",
      "95fb392a65834496a8e755ca496b1f3e",
      "090e32ea46ed49b1ac601487d6561e40",
      "84a18459d2ad4dc0a4ead47dfdf74e2d",
      "24306b105ee74e44b8555724cfb96ab3",
      "c04ea420680e44f9bdd3f31f566cb2ad",
      "99cdbb703794448ea7d50e81ffa9e8dc",
      "7e5139617f7c405ea8cf6ce5d995b451",
      "c93eb1a5c39e4fc381776f12c3aefe84",
      "3f45583485ff4cd0a02a07638f1a30b4",
      "32380c81a3a647a69af9205a3b2c6d81",
      "8217f7fbaf10433ba0ca300584157854",
      "d369f24aa136469ea411587252d38498",
      "4008c825745e450090c3adc9ccb59722",
      "bf9130591e83402cb3a4b698795921df",
      "c7249fb1599a47f0876e2f69bc2d1c8e",
      "161903d80ce646c3b734c82906000fdb",
      "9a8413f9dc3c42d8b66f2c3179084cf7",
      "c5fa9a0a892b4255b7b4ce857cb06fff",
      "2b82ff4f41e4467bad27bbdc0b12db25",
      "0b51c0c846644338be5a40998333b97a",
      "eb37aa2b953a41de87753893445afaf4"
     ]
    },
    "id": "faOSvk3OphNX",
    "outputId": "0796497b-79ee-4655-ab16-e372719ef166"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'gpt2')\n",
    "generator(\"Hello, I'm a language model,\", max_new_tokens=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqbFyFaogMXJ"
   },
   "source": [
    "#GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL9C-49Ml4Ps"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # make sure hidden dim is a multiple of no. of heads\n",
    "    assert config.n_embed % config.n_head == 0\n",
    "\n",
    "    # a single linear layer to compute Q, K, V simultaneously\n",
    "    self.c_attn=nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1 # flag for weight initialization of c_proj, use std = 0.02/sqroot(num layers)\n",
    "\n",
    "    # regularization\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embed = config.n_embed\n",
    "\n",
    "    # not really a bias, more of a mask, but following OpenAI naming convention\n",
    "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                         .view(1, 1,config.block_size, config.block_size ))\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.size()  # Batch size, sequence length, n_embed\n",
    "    qkv= self.c_attn(x)\n",
    "    q,k,v = qkv.split(self.n_embed, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # explanation : C = n_head * head_size\n",
    "    # k.shape = (B, T, n_head, head_size)\n",
    "    # k = k.transpose(1, 2)\n",
    "    # Before transpose: (B, T, n_head, head_size)\n",
    "    # After transpose:  (B, n_head, T, head_size)\n",
    "\n",
    "    # similar for q and v\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # Attention\n",
    "    # att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size[-1]))\n",
    "    # att = att.masked_fill(self.bias[:, :, :T, :T] == 0.0, float('-inf'))\n",
    "    # att = F.softmax(att, dim=-1)\n",
    "    # y = att @ v # (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, hs), basically a weighted sum of values\n",
    "\n",
    "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "\n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "    # output projection\n",
    "    y = self.c_proj(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOrOMwsGHEl6"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)         # ffn. increasing hidden dim size increases capacity of model to learn, 4*embed dim is just design choice\n",
    "    self.gelu = nn.GELU(approximate='tanh')                            # activation\n",
    "    self.c_proj = nn.Linear( 4 * config.n_embed, config.n_embed) # projection\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKP1mRRXFbTU"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(config.n_embed)  # layer norm 1\n",
    "    self.attn = CausalSelfAttention(config) # causal attention\n",
    "    self.ln_2 = nn.LayerNorm(config.n_embed) # layer norm 2\n",
    "    self.mlp = MLP(config) # fnn\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.attn(self.ln_1(x))\n",
    "    x = x + self.mlp(self.ln_2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzLYZmAICaTG"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "  block_size : int = 1024    # max sequence length\n",
    "  vocab_size : int = 50257   # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "  n_layer : int = 12\n",
    "  n_head : int = 12\n",
    "  n_embed : int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config=config\n",
    "\n",
    "    self.transformer=nn.ModuleDict(dict(\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embed),  # weights for token embeddings\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embed),  # weights for positional embeddings\n",
    "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # block for each layer\n",
    "        ln_f = nn.LayerNorm(config.n_embed),  # final layer normalisation\n",
    "        ))\n",
    "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size,bias=False) # last second, linear layer\n",
    "\n",
    "    # weight-sharing scheme\n",
    "    self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    # initialize parameters\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      std = 0.02\n",
    "      if hasattr(module, 'NANOGPT_SCALE_INIT'): # will be true only for output projection, `c_proj` layer\n",
    "        std *= (2 * self.config.n_layer) ** -0.05 # scale std by 1/sqrt(no_of_layers) acc to GPT paper\n",
    "        # we are doing 2 * no of layers bcoz every layer has 2 blocks that add to residual stream - attention and then mlp\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std = std) # inititalise weights according to gpt2 official code, i.e., mean 0, std 0.02 for weights\n",
    "      # sqroot n thing is done to control the growth of activations in residual stream in forward pass as each residual stream adds its data so we scale down every contribution to residual stream\n",
    "        torch.nn.init.zeros_(module.bias) # and normal initialisation for bias\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx (B, T) Batch size, B sequences, each of length T stacked up, T<=block_size\n",
    "    B, T = idx.size()\n",
    "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "    # forward the token and posisition embeddings\n",
    "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T), arange iterate from 0 to T\n",
    "    pos_emb = self.transformer.wpe(pos) # shape (T, n_embd) # identical for every single row (batch)\n",
    "    tok_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
    "    x = tok_emb + pos_emb # internal broadcasting\n",
    "    # forward the blocks of transformer\n",
    "    for block in self.transformer.h:\n",
    "      x = block(x)\n",
    "    # forward the final layernorm\n",
    "    x = self.transformer.ln_f(x)\n",
    "    # forward the final classifier\n",
    "    logits=self.lm_head(x) # (B, T, vocab_size)\n",
    "    loss=None\n",
    "    if targets is not None:\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # logits - (B*T, vocab_size)\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model_type):\n",
    "      # Loads pretrained GPT-2 model weights from huggingface\n",
    "      assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "      print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "      # n_layer, n_head and n_embed are determined from model_type\n",
    "      config_args = {\n",
    "          'gpt2':         dict(n_layer=12, n_head=12, n_embed=768),  # 124M params\n",
    "          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embed=1024), # 350M params\n",
    "          'gpt2-large':   dict(n_layer=36, n_head=20, n_embed=1280), # 774M params\n",
    "          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params\n",
    "      }[model_type]\n",
    "      config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "      config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "      # create a from-scratch initialized minGPT model\n",
    "      config = GPTConfig(**config_args)\n",
    "      model = GPT(config)\n",
    "      sd = model.state_dict()\n",
    "      sd_keys = sd.keys()\n",
    "      sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "      # init a huggingface/transformers model\n",
    "      model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "      sd_hf = model_hf.state_dict()\n",
    "\n",
    "      # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "      sd_keys_hf = sd_hf.keys()\n",
    "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "      transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "      # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "      # this means that we have to transpose these weights when we import them\n",
    "      assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "      for k in sd_keys_hf:\n",
    "          if any(k.endswith(w) for w in transposed):\n",
    "              # special treatment for the Conv1D weights we need to transpose\n",
    "              assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "              with torch.no_grad():\n",
    "                  sd[k].copy_(sd_hf[k].t())\n",
    "          else:\n",
    "              # vanilla copy over the other parameters\n",
    "              assert sd_hf[k].shape == sd[k].shape\n",
    "              with torch.no_grad():\n",
    "                  sd[k].copy_(sd_hf[k])\n",
    "\n",
    "      return model\n",
    "\n",
    "  def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "      # start with all of the candidate parameters (that require grad)\n",
    "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "      # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "      # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "      optim_groups = [\n",
    "          {'params': decay_params, 'weight_decay': weight_decay},\n",
    "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "      ]\n",
    "      num_decay_params = sum(p.numel() for p in decay_params)\n",
    "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "      if master_process:\n",
    "          print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "          print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "      # Create AdamW optimizer and use the fused version if it is available\n",
    "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "      use_fused = fused_available and device_type == \"cuda\"\n",
    "      if master_process:\n",
    "          print(f\"using fused AdamW: {use_fused}\")\n",
    "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "      return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaffDrldGX9z"
   },
   "outputs": [],
   "source": [
    "# model=GPT.from_pretrained(\"gpt2\")\n",
    "model = GPT(GPTConfig()) # random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdfSBmI0qwZh",
    "outputId": "6fd1367e-13b0-4cc9-9bdf-7344dea1332f"
   },
   "outputs": [],
   "source": [
    "model.eval() #put model into eval mode when not training anything and just using the model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x22uhgiQhQEA"
   },
   "outputs": [],
   "source": [
    "num_return_sequences=5\n",
    "max_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApxaK6ailQNh"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\") # (8, )\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
    "x = tokens.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "co7vykMnloJD",
    "outputId": "a730777f-977c-4b21-898c-f6e2d7c8d403"
   },
   "outputs": [],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD3NVdO2wmzR"
   },
   "source": [
    "##Generate before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fe-yLAxVl00o",
    "outputId": "0d27649b-c134-48e5-bc31-ebbfa097d9b8"
   },
   "outputs": [],
   "source": [
    "# generate.  right now x is (B, T) where B=5 and T=8\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1) < max_length: # add a column of new indices, i.e. add new token for each of the 5 sequences\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(x) # (B, T, vocab_size)\n",
    "\n",
    "        # get the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # ensures no sampling of very rare tokens\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "  tokens=x[i, :max_length].tolist()\n",
    "  decoded=enc.decode(tokens)\n",
    "  print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16m5rFx4Qo6n"
   },
   "source": [
    "##Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwpavxnEz8j"
   },
   "source": [
    "We want to feed the token sequences to a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DotEemCt_MV",
    "outputId": "6f3e2e5c-d4bc-43ea-90a8-c063e0da1035"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"input.txt\", 'r') as f:\n",
    "  text = f.read()\n",
    "data = text[:1000]\n",
    "print(data)\n",
    "tokens = enc.encode(data) # encode data\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[:B*T+1]) # take only B*T tokens to manage size and an additional token which will be used in output y as target to nth token\n",
    "buf = buf.to(device) # it doesn't just move the data to gpu, it creates a new memory on gpu\n",
    "x = buf[:-1].view(B, T) # all tokens except last\n",
    "y = buf[1:].view(B, T) # targets will be from 1st token\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWn98U7NG9gO"
   },
   "source": [
    "Its like for 0th token, target is 1st token, for 0 and 1st token, target is 2nd token and so on (masked prediction) and so our x goes from 0 to B*T and y goes from 1 to B*T+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nXg3TlzFFa5",
    "outputId": "da4fb4f5-c026-42bc-b7fd-3b943e80002a"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "logits, loss = model(x, y)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXc02G3tIXLj",
    "outputId": "8a7ebd61-a022-438c-d650-ef62526d5a5b"
   },
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSEjRSFfKyUT"
   },
   "source": [
    "At inititalization, we expect loss to be roughly around -ln(1/vocab_size)[NLL]1\n",
    " = 10.8 here,  since at initialization, probability of any word is same i.e., 1/50257."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF294-9NJyWQ",
    "outputId": "7cf93579-734d-4813-dcf4-0947099eceb0"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  optimizer.zero_grad()\n",
    "  logits, loss = model(x,y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(f\"step : {i}, loss : {loss.item()}\")\n",
    "# Here we overfit a single batch, now lets move to training on all batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AljICr1YKR0s"
   },
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "  def __init__(self, B, T):\n",
    "    self.B=B\n",
    "    self.T=T\n",
    "\n",
    "    # at init, load tokens from disk and store them in memory\n",
    "    with open('input.txt', 'r') as f:\n",
    "      text = f.read()\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    tokens = enc.encode(text)\n",
    "    self.tokens = torch.tensor(tokens)\n",
    "    print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
    "    self.current_size = 0 # state\n",
    "\n",
    "  def next_batch(self):\n",
    "    B, T = self.B, self.T\n",
    "    buf = self.tokens[self.current_size:self.current_size + B*T + 1] # +1 coz we need it in 'y'\n",
    "    # buf = buf.to(device) dont do this here to save space on gpu\n",
    "    x = buf[:-1].view(B, T) # inputs\n",
    "    y = buf[1:].view(B, T) # targets\n",
    "    self.current_size += B*T # advance position in tensor\n",
    "    # if loading next batch would be out of bounds, reset\n",
    "    if self.current_size + B*T + 1 > len(self.tokens):\n",
    "      self.current_size = 0\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmTvwbr2MMSM",
    "outputId": "b1b5858c-6405-41b9-cc71-806d0e8114a8"
   },
   "outputs": [],
   "source": [
    "for name, m in model.named_modules():\n",
    "    if hasattr(m, \"bias\") and m.bias is None:\n",
    "        print(\"no-bias module:\", name, type(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtoGootE9Kjp",
    "outputId": "2f3c0e28-78ab-49f0-a82f-e789a9868368"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=4, T=32)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad()\n",
    "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(f\"step : {i}, loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXYsxX-JFKxl"
   },
   "source": [
    "# Optimization"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
