{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Refer paper Language Models are Unsupervised Multitask Learners\n",
        "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "\n",
        "\n",
        "GPT-2[124M] has 12 layers and $d_{model}$ = 768\n",
        "\n",
        "It is decoder only model."
      ],
      "metadata": {
        "id": "G5p7sxxw-kp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables are named to follow the schema of Hugging Face Transformers code.  \n",
        "The following variables should be exactly the same:\n",
        "\n",
        "`transformer.wte.weight`  \n",
        "`transformer.wpe.weight`  \n",
        "`transformer.h.0.attn.c_attn.weight`  \n",
        "`transformer.h.0.attn.c_proj.weight`  \n",
        "`transformer.h.0.mlp.c_fc.weight`  \n",
        "`transformer.h.0.mlp.c_proj.weight`  \n",
        "`transformer.ln_f.weight`  \n",
        "`lm_head.weight`\n"
      ],
      "metadata": {
        "id": "MogKTW_5oPiT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AivZQshf94C1"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import tiktoken\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "F70TvxkCHisq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f8b37f17-3fc4-4149-eab2-595b612ed439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\") #124M\n",
        "sd = model_gpt2.state_dict()\n",
        "for k,v in sd.items():\n",
        "  print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a3db92a766724868af87dda287644497",
            "0e161cb47ea242a8925d2e84f93df828",
            "1539651aad78451497b634d53e80d30a",
            "8e4a2ae409da4efbab84c91ac6d40634",
            "e41ce93f424549e5a7913053ae80d23c",
            "637fa0d7052843139fa6807319736d35",
            "b12fb73568594124b789287534c141e1",
            "c6879c6929144b5e8279733429127edb",
            "c768bc36988749d2a7f2f8be8fadd856",
            "7b3c3a63e8144b7e8be9cb8d43bf0a5f",
            "b5bb0ce37bec4baeaa617c5fa0a3898b",
            "f3f942728f5c4faba6e6565c945378e3",
            "7aa66e058f9e4d59b46d2c05347af6cb",
            "72a7b94fbc3140e9bc2cdfb895a335e8",
            "dd3a0712de2c47a7acd49247c7769b32",
            "eea28c6cc6f84a2d9abb7908d31ff5aa",
            "21582e9fb97c4e86abf031a1e5a2bd9f",
            "3d2e3b74e4fa422cb30ffacbc43b9298",
            "c55cc2672cbc4183b92c625c84587a95",
            "5cc14673359140278cf351c92bac0c60",
            "9b8d1220847e4b5f9aa79d80357fef4a",
            "adb0305197e64fe98b2e75086ca91480",
            "86045121c2dc4f8aaf194376615539ec",
            "136a118457b7475899b81830e3ab35e9",
            "a4edf4e48ab8403bab2cb64a1843bd92",
            "256254aaa9ac4402b7d2ab1bbc57b46c",
            "9f5ece2a7a354dbbae1f51337e3ead4a",
            "08bba939aced448e88c0047b8157735d",
            "695ee9b569164daeb45c886f02b66094",
            "c0bd72480f8f45c6a076098ea5b9c38d",
            "d2fcfe13d23b473784dec513ed588f4f",
            "dacdc85236ce4ce08d95fe80d6169ea1",
            "9d61f24f3090425995beb0af5eff3ae4"
          ]
        },
        "id": "nuz6GS5yn8Ta",
        "outputId": "19ffccb2-c18f-4bf2-f014-0484f5c6ac09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3db92a766724868af87dda287644497"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3f942728f5c4faba6e6565c945378e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86045121c2dc4f8aaf194376615539ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_2.weight torch.Size([768])\n",
            "transformer.h.1.ln_2.bias torch.Size([768])\n",
            "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_1.weight torch.Size([768])\n",
            "transformer.h.2.ln_1.bias torch.Size([768])\n",
            "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_2.weight torch.Size([768])\n",
            "transformer.h.2.ln_2.bias torch.Size([768])\n",
            "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_1.weight torch.Size([768])\n",
            "transformer.h.3.ln_1.bias torch.Size([768])\n",
            "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_2.weight torch.Size([768])\n",
            "transformer.h.3.ln_2.bias torch.Size([768])\n",
            "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_1.weight torch.Size([768])\n",
            "transformer.h.4.ln_1.bias torch.Size([768])\n",
            "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_2.weight torch.Size([768])\n",
            "transformer.h.4.ln_2.bias torch.Size([768])\n",
            "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_1.weight torch.Size([768])\n",
            "transformer.h.5.ln_1.bias torch.Size([768])\n",
            "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_2.weight torch.Size([768])\n",
            "transformer.h.5.ln_2.bias torch.Size([768])\n",
            "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_1.weight torch.Size([768])\n",
            "transformer.h.6.ln_1.bias torch.Size([768])\n",
            "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_2.weight torch.Size([768])\n",
            "transformer.h.6.ln_2.bias torch.Size([768])\n",
            "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_1.weight torch.Size([768])\n",
            "transformer.h.7.ln_1.bias torch.Size([768])\n",
            "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_2.weight torch.Size([768])\n",
            "transformer.h.7.ln_2.bias torch.Size([768])\n",
            "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_1.weight torch.Size([768])\n",
            "transformer.h.8.ln_1.bias torch.Size([768])\n",
            "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_2.weight torch.Size([768])\n",
            "transformer.h.8.ln_2.bias torch.Size([768])\n",
            "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_1.weight torch.Size([768])\n",
            "transformer.h.9.ln_1.bias torch.Size([768])\n",
            "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_2.weight torch.Size([768])\n",
            "transformer.h.9.ln_2.bias torch.Size([768])\n",
            "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_1.weight torch.Size([768])\n",
            "transformer.h.10.ln_1.bias torch.Size([768])\n",
            "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_2.weight torch.Size([768])\n",
            "transformer.h.10.ln_2.bias torch.Size([768])\n",
            "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_1.weight torch.Size([768])\n",
            "transformer.h.11.ln_1.bias torch.Size([768])\n",
            "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_2.weight torch.Size([768])\n",
            "transformer.h.11.ln_2.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model = 'gpt2')\n",
        "generator(\"Hello, I'm a language model,\", max_new_tokens=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "faOSvk3OphNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "0ba48ebfa6bc4869b6fa00d93ea6ccad",
            "44a2ac6af3b74f9490a3a64808eaf161",
            "c940e54136424f7b88a85ec8ddbf50e9",
            "bf93d3d4a8ba4920be7a4fd701a0924c",
            "c7716421d1dc4570bec2e37255c2cae0",
            "86e6da46b6f040909da6e637492dda19",
            "437d4724c39242f2ab882659c86ee344",
            "505778c070d447ea8ace4f5f5823a274",
            "1c3cc9df8b9e463ebad3dbf2cc3996a4",
            "954fa61ac13b4ea6b0556661f3494a79",
            "6a1e63811f464d96ba48643d9269827a",
            "3d517f41e30b4b3abb30f993459cf50d",
            "98a3ed2950554a4cb470a23e2ffae74d",
            "219a2e93248443aa920ff8a42f5bbc4a",
            "8eaaa4032d994218b32e55a39e325a5a",
            "c0f06bc4991b492785b10b8e7ac7a9a6",
            "0833f9f6a79d484d9d7faea7590781f9",
            "1ecda8fe2ade402caeaa66739cd76241",
            "617725f2528e4033b55797da71f68311",
            "a3b03ebd19af4b40ad508f260175512a",
            "df64a4396f934071a300f346f12c7a9a",
            "5b46a497f5a14037931470ea30f96817",
            "6c7a835f21dd4251b2a4bc24c648ac8e",
            "95fb392a65834496a8e755ca496b1f3e",
            "090e32ea46ed49b1ac601487d6561e40",
            "84a18459d2ad4dc0a4ead47dfdf74e2d",
            "24306b105ee74e44b8555724cfb96ab3",
            "c04ea420680e44f9bdd3f31f566cb2ad",
            "99cdbb703794448ea7d50e81ffa9e8dc",
            "7e5139617f7c405ea8cf6ce5d995b451",
            "c93eb1a5c39e4fc381776f12c3aefe84",
            "3f45583485ff4cd0a02a07638f1a30b4",
            "32380c81a3a647a69af9205a3b2c6d81",
            "8217f7fbaf10433ba0ca300584157854",
            "d369f24aa136469ea411587252d38498",
            "4008c825745e450090c3adc9ccb59722",
            "bf9130591e83402cb3a4b698795921df",
            "c7249fb1599a47f0876e2f69bc2d1c8e",
            "161903d80ce646c3b734c82906000fdb",
            "9a8413f9dc3c42d8b66f2c3179084cf7",
            "c5fa9a0a892b4255b7b4ce857cb06fff",
            "2b82ff4f41e4467bad27bbdc0b12db25",
            "0b51c0c846644338be5a40998333b97a",
            "eb37aa2b953a41de87753893445afaf4"
          ]
        },
        "outputId": "0796497b-79ee-4655-ab16-e372719ef166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ba48ebfa6bc4869b6fa00d93ea6ccad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d517f41e30b4b3abb30f993459cf50d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c7a835f21dd4251b2a4bc24c648ac8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8217f7fbaf10433ba0ca300584157854"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, and I'm a man. I'm a man. I'm a man. I'm a man. I'm a man. I'm a man\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I know that it's not easy to do the right thing.\\n\\nBut I want to share with you two things.\\n\\nThe first\"},\n",
              " {'generated_text': \"Hello, I'm a language model, I'm a designer.\\n\\nAnd I'm a programmer too.\\n\\nMy first book, The Language of Programming, was written in the late\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I'm pretty sure I understand this.\\n\\nAnd, as I said, I'm a language model, and I'm pretty sure I understand\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a language.\\n\\nThe question is not why I'm different from a language model, but why I'm different from a language model.\\n\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT-2"
      ],
      "metadata": {
        "id": "KqbFyFaogMXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # make sure hidden dim is a multiple of no. of heads\n",
        "    assert config.n_embed % config.n_head == 0\n",
        "\n",
        "    # a single linear layer to compute Q, K, V simultaneously\n",
        "    self.c_attn=nn.Linear(config.n_embed, 3 * config.n_embed)\n",
        "\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1 # flag for weight initialization of c_proj, use std = 0.02/sqroot(num layers)\n",
        "\n",
        "    # regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embed = config.n_embed\n",
        "\n",
        "    # not really a bias, more of a mask, but following OpenAI naming convention\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                         .view(1, 1,config.block_size, config.block_size ))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()  # Batch size, sequence length, n_embed\n",
        "    qkv= self.c_attn(x)\n",
        "    q,k,v = qkv.split(self.n_embed, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # explanation : C = n_head * head_size\n",
        "    # k.shape = (B, T, n_head, head_size)\n",
        "    # k = k.transpose(1, 2)\n",
        "    # Before transpose: (B, T, n_head, head_size)\n",
        "    # After transpose:  (B, n_head, T, head_size)\n",
        "\n",
        "    # similar for q and v\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # Attention\n",
        "    # att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size[-1]))\n",
        "    # att = att.masked_fill(self.bias[:, :, :T, :T] == 0.0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, hs), basically a weighted sum of values\n",
        "\n",
        "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.c_proj(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HL9C-49Ml4Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)         # ffn. increasing hidden dim size increases capacity of model to learn, 4*embed dim is just design choice\n",
        "    self.gelu = nn.GELU(approximate='tanh')                            # activation\n",
        "    self.c_proj = nn.Linear( 4 * config.n_embed, config.n_embed) # projection\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "aOrOMwsGHEl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embed)  # layer norm 1\n",
        "    self.attn = CausalSelfAttention(config) # causal attention\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embed) # layer norm 2\n",
        "    self.mlp = MLP(config) # fnn\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "FKP1mRRXFbTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size : int = 1024    # max sequence length\n",
        "  vocab_size : int = 50257   # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "  n_layer : int = 12\n",
        "  n_head : int = 12\n",
        "  n_embed : int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config=config\n",
        "\n",
        "    self.transformer=nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embed),  # weights for token embeddings\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embed),  # weights for positional embeddings\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # block for each layer\n",
        "        ln_f = nn.LayerNorm(config.n_embed),  # final layer normalisation\n",
        "        ))\n",
        "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size,bias=False) # last second, linear layer\n",
        "\n",
        "    # weight-sharing scheme\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    # initialize parameters\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'): # will be true only for output projection, `c_proj` layer\n",
        "        std *= (2 * self.config.n_layer) ** -0.05 # scale std by 1/sqrt(no_of_layers) acc to GPT paper\n",
        "        # we are doing 2 * no of layers bcoz every layer has 2 blocks that add to residual stream - attention and then mlp\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std = std) # inititalise weights according to gpt2 official code, i.e., mean 0, std 0.02 for weights\n",
        "      # sqroot n thing is done to control the growth of activations in residual stream in forward pass as each residual stream adds its data so we scale down every contribution to residual stream\n",
        "        torch.nn.init.zeros_(module.bias) # and normal initialisation for bias\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx (B, T) Batch size, B sequences, each of length T stacked up, T<=block_size\n",
        "    B, T = idx.size()\n",
        "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "    # forward the token and posisition embeddings\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T), arange iterate from 0 to T\n",
        "    pos_emb = self.transformer.wpe(pos) # shape (T, n_embd) # identical for every single row (batch)\n",
        "    tok_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
        "    x = tok_emb + pos_emb # internal broadcasting\n",
        "    # forward the blocks of transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "    # forward the final layernorm\n",
        "    x = self.transformer.ln_f(x)\n",
        "    # forward the final classifier\n",
        "    logits=self.lm_head(x) # (B, T, vocab_size)\n",
        "    loss=None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # logits - (B*T, vocab_size)\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "      # Loads pretrained GPT-2 model weights from huggingface\n",
        "      assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "      print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "      # n_layer, n_head and n_embed are determined from model_type\n",
        "      config_args = {\n",
        "          'gpt2':         dict(n_layer=12, n_head=12, n_embed=768),  # 124M params\n",
        "          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embed=1024), # 350M params\n",
        "          'gpt2-large':   dict(n_layer=36, n_head=20, n_embed=1280), # 774M params\n",
        "          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params\n",
        "      }[model_type]\n",
        "      config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "      config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "      # create a from-scratch initialized minGPT model\n",
        "      config = GPTConfig(**config_args)\n",
        "      model = GPT(config)\n",
        "      sd = model.state_dict()\n",
        "      sd_keys = sd.keys()\n",
        "      sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "      # init a huggingface/transformers model\n",
        "      model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "      sd_hf = model_hf.state_dict()\n",
        "\n",
        "      # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "      sd_keys_hf = sd_hf.keys()\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "      transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "      # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "      # this means that we have to transpose these weights when we import them\n",
        "      assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "      for k in sd_keys_hf:\n",
        "          if any(k.endswith(w) for w in transposed):\n",
        "              # special treatment for the Conv1D weights we need to transpose\n",
        "              assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k].t())\n",
        "          else:\n",
        "              # vanilla copy over the other parameters\n",
        "              assert sd_hf[k].shape == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k])\n",
        "\n",
        "      return model\n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "      # start with all of the candidate parameters (that require grad)\n",
        "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "      # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "      # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "      num_decay_params = sum(p.numel() for p in decay_params)\n",
        "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "      if master_process:\n",
        "          print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "          print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "      # Create AdamW optimizer and use the fused version if it is available\n",
        "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "      use_fused = fused_available and device_type == \"cuda\"\n",
        "      if master_process:\n",
        "          print(f\"using fused AdamW: {use_fused}\")\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "      return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "nzLYZmAICaTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model=GPT.from_pretrained(\"gpt2\")\n",
        "model = GPT(GPTConfig()) # random initialization"
      ],
      "metadata": {
        "id": "FaffDrldGX9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #put model into eval mode when not training anything and just using the model\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "hdfSBmI0qwZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd1367e-13b0-4cc9-9bdf-7344dea1332f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences=5\n",
        "max_length=30"
      ],
      "metadata": {
        "id": "x22uhgiQhQEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\") # (8, )\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
        "x = tokens.to('cuda')"
      ],
      "metadata": {
        "id": "ApxaK6ailQNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co7vykMnloJD",
        "outputId": "a730777f-977c-4b21-898c-f6e2d7c8d403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate before Training"
      ],
      "metadata": {
        "id": "bD3NVdO2wmzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate.  right now x is (B, T) where B=5 and T=8\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length: # add a column of new indices, i.e. add new token for each of the 5 sequences\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(x) # (B, T, vocab_size)\n",
        "\n",
        "        # get the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # ensures no sampling of very rare tokens\n",
        "\n",
        "        # select a token from the top-k probabilities\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "  tokens=x[i, :max_length].tolist()\n",
        "  decoded=enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe-yLAxVl00o",
        "outputId": "0d27649b-c134-48e5-bc31-ebbfa097d9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, PTS headset rivaldealDN carried Furthermore headset universe makes\u062fdatDNangs192 strained Tough depositedierrez convoluted\u0002 Designs\n",
            "> Hello, I'm a language model, criticisms Cullen reviewer universe headsetGet modifications herdpowers Sul Bj immigrant \u00a0 repeatedly \u2212 Pop masse masse \u2212 quantGet brew\n",
            "> Hello, I'm a language model, accessory unbeliev unbelievshaped accessory carried\"), reducediefrizBradesternIsariz Shy recommendremote representing ger improperlyMagazine Turtle\n",
            "> Hello, I'm a language model,Ham criticisms aggressively rivalDN islandsDN Dragonbound Russo ad encompasses ost RFCAdding some comboAyylum Travels beams comprehensiveserial\n",
            "> Hello, I'm a language model,Bradannie697ryptBrad summerBornEat taxp willingly summerophone some protestsanc improperly Cullen Mar79 someKn reiter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Processing"
      ],
      "metadata": {
        "id": "16m5rFx4Qo6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to feed the token sequences to a transformer"
      ],
      "metadata": {
        "id": "vvwpavxnEz8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "with open(\"input.txt\", 'r') as f:\n",
        "  text = f.read()\n",
        "data = text[:1000]\n",
        "print(data)\n",
        "tokens = enc.encode(data) # encode data\n",
        "B, T = 4, 32\n",
        "buf = torch.tensor(tokens[:B*T+1]) # take only B*T tokens to manage size and an additional token which will be used in output y as target to nth token\n",
        "buf = buf.to(device) # it doesn't just move the data to gpu, it creates a new memory on gpu\n",
        "x = buf[:-1].view(B, T) # all tokens except last\n",
        "y = buf[1:].view(B, T) # targets will be from 1st token\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "4DotEemCt_MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3e2e5c-d4bc-43ea-90a8-c063e0da1035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n",
            "tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
            "          3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
            "           461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
            "          1639,   389],\n",
            "        [  477, 12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,\n",
            "           198,   198,  3237,    25,   198,  4965,  5634,    13, 12939,    13,\n",
            "           198,   198,  5962, 22307,    25,   198,  5962,    11,   345,   760,\n",
            "           327,  1872],\n",
            "        [  385,  1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,\n",
            "           198,   198,  3237,    25,   198,  1135,   760,   470,    11,   356,\n",
            "           760,   470,    13,   198,   198,  5962, 22307,    25,   198,  5756,\n",
            "           514,  1494],\n",
            "        [  683,    11,   290,   356,  1183,   423, 11676,   379,   674,   898,\n",
            "          2756,    13,   198,  3792,   470,   257, 15593,    30,   198,   198,\n",
            "          3237,    25,   198,  2949,   517,  3375,   319,   470,    26,  1309,\n",
            "           340,   307]], device='cuda:0')\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,  3285,\n",
            "           502,  2740,    13,   198,   198,  3237,    25,   198,  5248,   461,\n",
            "            11,  2740,    13,   198,   198,  5962, 22307,    25,   198,  1639,\n",
            "           389,   477],\n",
            "        [12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,   198,\n",
            "           198,  3237,    25,   198,  4965,  5634,    13, 12939,    13,   198,\n",
            "           198,  5962, 22307,    25,   198,  5962,    11,   345,   760,   327,\n",
            "          1872,   385],\n",
            "        [ 1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,   198,\n",
            "           198,  3237,    25,   198,  1135,   760,   470,    11,   356,   760,\n",
            "           470,    13,   198,   198,  5962, 22307,    25,   198,  5756,   514,\n",
            "          1494,   683],\n",
            "        [   11,   290,   356,  1183,   423, 11676,   379,   674,   898,  2756,\n",
            "            13,   198,  3792,   470,   257, 15593,    30,   198,   198,  3237,\n",
            "            25,   198,  2949,   517,  3375,   319,   470,    26,  1309,   340,\n",
            "           307,  1760]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its like for 0th token, target is 1st token, for 0 and 1st token, target is 2nd token and so on (masked prediction) and so our x goes from 0 to B*T and y goes from 1 to B*T+1"
      ],
      "metadata": {
        "id": "cWn98U7NG9gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "logits, loss = model(x, y)\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nXg3TlzFFa5",
        "outputId": "da4fb4f5-c026-42bc-b7fd-3b943e80002a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXc02G3tIXLj",
        "outputId": "8a7ebd61-a022-438c-d650-ef62526d5a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.0363, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At inititalization, we expect loss to be roughly around -ln(1/vocab_size)[NLL]1\n",
        " = 10.8 here,  since at initialization, probability of any word is same i.e., 1/50257."
      ],
      "metadata": {
        "id": "NSEjRSFfKyUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step : {i}, loss : {loss.item()}\")\n",
        "# Here we overfit a single batch, now lets move to training on all batches!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF294-9NJyWQ",
        "outputId": "7cf93579-734d-4813-dcf4-0947099eceb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 0, loss : 11.036296844482422\n",
            "step : 1, loss : 8.401263236999512\n",
            "step : 2, loss : 7.556160926818848\n",
            "step : 3, loss : 7.823549270629883\n",
            "step : 4, loss : 6.896722793579102\n",
            "step : 5, loss : 6.51249361038208\n",
            "step : 6, loss : 6.098946571350098\n",
            "step : 7, loss : 5.710570335388184\n",
            "step : 8, loss : 5.27562141418457\n",
            "step : 9, loss : 4.860488414764404\n",
            "step : 10, loss : 4.485625267028809\n",
            "step : 11, loss : 4.047159194946289\n",
            "step : 12, loss : 3.5754752159118652\n",
            "step : 13, loss : 3.153848171234131\n",
            "step : 14, loss : 2.7754809856414795\n",
            "step : 15, loss : 2.3487753868103027\n",
            "step : 16, loss : 2.044778347015381\n",
            "step : 17, loss : 1.7443827390670776\n",
            "step : 18, loss : 1.439927577972412\n",
            "step : 19, loss : 1.1365606784820557\n",
            "step : 20, loss : 0.8545939922332764\n",
            "step : 21, loss : 0.6557171940803528\n",
            "step : 22, loss : 0.49248164892196655\n",
            "step : 23, loss : 0.35574260354042053\n",
            "step : 24, loss : 0.25000423192977905\n",
            "step : 25, loss : 0.18138493597507477\n",
            "step : 26, loss : 0.13208100199699402\n",
            "step : 27, loss : 0.09720337390899658\n",
            "step : 28, loss : 0.07480422407388687\n",
            "step : 29, loss : 0.058957431465387344\n",
            "step : 30, loss : 0.04726431518793106\n",
            "step : 31, loss : 0.038995616137981415\n",
            "step : 32, loss : 0.033052101731300354\n",
            "step : 33, loss : 0.028286181390285492\n",
            "step : 34, loss : 0.024161001667380333\n",
            "step : 35, loss : 0.020603492856025696\n",
            "step : 36, loss : 0.017626751214265823\n",
            "step : 37, loss : 0.015202797949314117\n",
            "step : 38, loss : 0.01326095312833786\n",
            "step : 39, loss : 0.011709336191415787\n",
            "step : 40, loss : 0.010459909215569496\n",
            "step : 41, loss : 0.009440543130040169\n",
            "step : 42, loss : 0.008595634251832962\n",
            "step : 43, loss : 0.007883011363446712\n",
            "step : 44, loss : 0.007271425798535347\n",
            "step : 45, loss : 0.006739228032529354\n",
            "step : 46, loss : 0.0062720151618123055\n",
            "step : 47, loss : 0.005859816446900368\n",
            "step : 48, loss : 0.005495084449648857\n",
            "step : 49, loss : 0.0051715923473238945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B=B\n",
        "    self.T=T\n",
        "\n",
        "    # at init, load tokens from disk and store them in memory\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"Loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "    self.current_size = 0 # state\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_size:self.current_size + B*T + 1] # +1 coz we need it in 'y'\n",
        "    # buf = buf.to(device) dont do this here to save space on gpu\n",
        "    x = buf[:-1].view(B, T) # inputs\n",
        "    y = buf[1:].view(B, T) # targets\n",
        "    self.current_size += B*T # advance position in tensor\n",
        "    # if loading next batch would be out of bounds, reset\n",
        "    if self.current_size + B*T + 1 > len(self.tokens):\n",
        "      self.current_size = 0\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "AljICr1YKR0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, m in model.named_modules():\n",
        "    if hasattr(m, \"bias\") and m.bias is None:\n",
        "        print(\"no-bias module:\", name, type(m))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmTvwbr2MMSM",
        "outputId": "b1b5858c-6405-41b9-cc71-806d0e8114a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no-bias module: lm_head <class 'torch.nn.modules.linear.Linear'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoaderLite(B=4, T=32)\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step : {i}, loss : {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtoGootE9Kjp",
        "outputId": "2f3c0e28-78ab-49f0-a82f-e789a9868368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 2640 batches\n",
            "step : 0, loss : 10.81787109375\n",
            "step : 1, loss : 9.78515625\n",
            "step : 2, loss : 9.002197265625\n",
            "step : 3, loss : 9.053466796875\n",
            "step : 4, loss : 8.6226806640625\n",
            "step : 5, loss : 8.3692626953125\n",
            "step : 6, loss : 9.005859375\n",
            "step : 7, loss : 8.7735595703125\n",
            "step : 8, loss : 8.06494140625\n",
            "step : 9, loss : 7.9627685546875\n",
            "step : 10, loss : 8.40625\n",
            "step : 11, loss : 7.395263671875\n",
            "step : 12, loss : 7.79168701171875\n",
            "step : 13, loss : 7.4029541015625\n",
            "step : 14, loss : 7.56097412109375\n",
            "step : 15, loss : 7.36614990234375\n",
            "step : 16, loss : 7.43896484375\n",
            "step : 17, loss : 8.29638671875\n",
            "step : 18, loss : 7.3306884765625\n",
            "step : 19, loss : 7.87353515625\n",
            "step : 20, loss : 7.483642578125\n",
            "step : 21, loss : 7.7362060546875\n",
            "step : 22, loss : 6.419921875\n",
            "step : 23, loss : 6.866455078125\n",
            "step : 24, loss : 6.86273193359375\n",
            "step : 25, loss : 6.73077392578125\n",
            "step : 26, loss : 6.82000732421875\n",
            "step : 27, loss : 7.6192626953125\n",
            "step : 28, loss : 7.19854736328125\n",
            "step : 29, loss : 6.965576171875\n",
            "step : 30, loss : 7.0177001953125\n",
            "step : 31, loss : 7.1590576171875\n",
            "step : 32, loss : 7.1702880859375\n",
            "step : 33, loss : 7.0030517578125\n",
            "step : 34, loss : 7.9168701171875\n",
            "step : 35, loss : 7.7799072265625\n",
            "step : 36, loss : 7.75634765625\n",
            "step : 37, loss : 7.7099609375\n",
            "step : 38, loss : 8.061279296875\n",
            "step : 39, loss : 7.4923095703125\n",
            "step : 40, loss : 7.4549560546875\n",
            "step : 41, loss : 6.9793701171875\n",
            "step : 42, loss : 7.1624755859375\n",
            "step : 43, loss : 7.1212158203125\n",
            "step : 44, loss : 7.01947021484375\n",
            "step : 45, loss : 7.0743408203125\n",
            "step : 46, loss : 6.22052001953125\n",
            "step : 47, loss : 6.40740966796875\n",
            "step : 48, loss : 6.9620361328125\n",
            "step : 49, loss : 6.82391357421875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization"
      ],
      "metadata": {
        "id": "vXYsxX-JFKxl"
      }
    }
  ]
}
